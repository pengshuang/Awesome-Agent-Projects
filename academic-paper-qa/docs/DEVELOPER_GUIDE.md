# ğŸ‘¨â€ğŸ’» å¼€å‘è€…æŒ‡å—

> æœ¬æ–‡æ¡£é¢å‘éœ€è¦è¿›è¡ŒäºŒæ¬¡å¼€å‘ã€æ‰©å±•åŠŸèƒ½æˆ–æ·±å…¥äº†è§£é¡¹ç›®æ¶æ„çš„å¼€å‘è€…

## ç›®å½•
- [é¡¹ç›®ç»“æ„](#é¡¹ç›®ç»“æ„)
- [æ ¸å¿ƒæ¶æ„](#æ ¸å¿ƒæ¶æ„)
- [æ ¸å¿ƒæ¨¡å—](#æ ¸å¿ƒæ¨¡å—)
- [API å‚è€ƒ](#api-å‚è€ƒ)
- [æ‰©å±•å¼€å‘](#æ‰©å±•å¼€å‘)
- [æµ‹è¯•æŒ‡å—](#æµ‹è¯•æŒ‡å—)
- [éƒ¨ç½²æŒ‡å—](#éƒ¨ç½²æŒ‡å—)
- [ä»£ç è§„èŒƒ](#ä»£ç è§„èŒƒ)

---

## é¡¹ç›®ç»“æ„

### ğŸ“‚ å®Œæ•´ç›®å½•ç»“æ„

```
academic-paper-qa/
â”œâ”€â”€ ğŸš€ å¯åŠ¨æ–‡ä»¶
â”‚   â”œâ”€â”€ web_ui_multi_turn.py        # Web UI å¤šè½®å¯¹è¯ â­
â”‚   â”œâ”€â”€ web_ui_single_turn.py       # Web UI å•è½®é—®ç­”
â”‚   â”œâ”€â”€ cli_multi_turn.py           # å‘½ä»¤è¡Œå¤šè½®å¯¹è¯ â­
â”‚   â”œâ”€â”€ cli_single_turn.py          # å‘½ä»¤è¡Œå•è½®é—®ç­”
â”‚   â””â”€â”€ init_system.py              # ç³»ç»Ÿåˆå§‹åŒ–
â”‚
â”œâ”€â”€ ğŸ› ï¸ å¯åŠ¨è„šæœ¬
â”‚   â”œâ”€â”€ start_web_multi.sh          # å¯åŠ¨ Web å¤šè½®
â”‚   â”œâ”€â”€ start_web_single.sh         # å¯åŠ¨ Web å•è½®
â”‚   â”œâ”€â”€ start_cli_multi.sh          # å¯åŠ¨å‘½ä»¤è¡Œå¤šè½®
â”‚   â””â”€â”€ start_cli_single.sh         # å¯åŠ¨å‘½ä»¤è¡Œå•è½®
â”‚
â”œâ”€â”€ ğŸ”§ æ ¸å¿ƒæ¨¡å— (src/)
â”‚   â”œâ”€â”€ agent.py                    # AcademicAgent æ ¸å¿ƒç±»
â”‚   â”œâ”€â”€ constants.py                # å¸¸é‡å®šä¹‰
â”‚   â”‚
â”‚   â”œâ”€â”€ loaders/                    # æ–‡æ¡£åŠ è½½å™¨
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ document_loader.py      # æ–‡æ¡£åŠ è½½ä¸»ç±»
â”‚   â”‚
â”‚   â”œâ”€â”€ indexing/                   # ç´¢å¼•æ„å»º
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ indexer.py              # ç´¢å¼•æ„å»ºå™¨
â”‚   â”‚   â””â”€â”€ vector_store.py         # å‘é‡å­˜å‚¨
â”‚   â”‚
â”‚   â”œâ”€â”€ query/                      # æŸ¥è¯¢å¼•æ“
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ qa_engine.py            # é—®ç­”å¼•æ“
â”‚   â”‚   â””â”€â”€ rag_pipeline.py         # RAG æµç¨‹
â”‚   â”‚
â”‚   â”œâ”€â”€ tools/                      # å·¥å…·é›†
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ web_search.py           # ç½‘ç»œæœç´¢
â”‚   â”‚
â”‚   â””â”€â”€ utils/                      # å·¥å…·å‡½æ•°
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ logger.py               # æ—¥å¿—å·¥å…·
â”‚       â””â”€â”€ helpers.py              # è¾…åŠ©å‡½æ•°
â”‚
â”œâ”€â”€ âš™ï¸ é…ç½® (config/)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ llm_config.py               # LLM é…ç½®
â”‚   â””â”€â”€ settings.py                 # å…¨å±€è®¾ç½®
â”‚
â”œâ”€â”€ ğŸ“š æ–‡æ¡£ (docs/)
â”‚   â”œâ”€â”€ USER_GUIDE.md               # ç”¨æˆ·ä½¿ç”¨æŒ‡å—
â”‚   â”œâ”€â”€ FEATURES.md                 # åŠŸèƒ½è¯¦ç»†è¯´æ˜
â”‚   â””â”€â”€ DEVELOPER_GUIDE.md          # å¼€å‘è€…æ–‡æ¡£ï¼ˆæœ¬æ–‡ä»¶ï¼‰
â”‚
â”œâ”€â”€ ğŸ“„ ç¤ºä¾‹ (examples/)
â”‚   â”œâ”€â”€ quick_start.py              # å¿«é€Ÿå…¥é—¨
â”‚   â”œâ”€â”€ advanced_query.py           # é«˜çº§æŸ¥è¯¢
â”‚   â”œâ”€â”€ agent_demo.py               # Agent æ¼”ç¤º
â”‚   â””â”€â”€ README.md                   # ç¤ºä¾‹è¯´æ˜
â”‚
â”œâ”€â”€ ğŸ§ª æµ‹è¯• (tests/)
â”‚   â”œâ”€â”€ test_agent_core.py          # Agent æ ¸å¿ƒæµ‹è¯•
â”‚   â”œâ”€â”€ test_multi_turn_chat.py     # å¤šè½®å¯¹è¯æµ‹è¯•
â”‚   â”œâ”€â”€ test_loader_documents.py    # æ–‡æ¡£åŠ è½½æµ‹è¯•
â”‚   â”œâ”€â”€ test_web_search.py          # Web æœç´¢æµ‹è¯•
â”‚   â”œâ”€â”€ test_ui_features.py         # UI åŠŸèƒ½æµ‹è¯•
â”‚   â””â”€â”€ README.md                   # æµ‹è¯•è¯´æ˜
â”‚
â”œâ”€â”€ ğŸ“¦ æ•°æ® (data/)
â”‚   â”œâ”€â”€ documents/                  # æ–‡æ¡£å­˜å‚¨ï¼ˆæ”¾ç½® PDF ç­‰ï¼‰
â”‚   â”œâ”€â”€ vector_store/               # å‘é‡ç´¢å¼•
â”‚   â”œâ”€â”€ cache/                      # ç¼“å­˜æ–‡ä»¶
â”‚   â””â”€â”€ processed/                  # å¤„ç†åçš„æ–‡æ¡£
â”‚
â”œâ”€â”€ ğŸ“‹ é¡¹ç›®æ–‡æ¡£
â”‚   â”œâ”€â”€ README.md                   # é¡¹ç›®ä¸»æ–‡æ¡£ â­
â”‚   â””â”€â”€ CODE_OPTIMIZATION_SUMMARY.md # ä»£ç ä¼˜åŒ–è®°å½•
â”‚
â””â”€â”€ âš™ï¸ é…ç½®æ–‡ä»¶
    â”œâ”€â”€ .env                        # ç¯å¢ƒå˜é‡ï¼ˆä¸æäº¤ï¼‰
    â”œâ”€â”€ .env.example                # é…ç½®æ¨¡æ¿
    â””â”€â”€ requirements.txt            # Python ä¾èµ–
```

### ğŸ“Š æ ¸å¿ƒåŠŸèƒ½æ¨¡å—

| åŠŸèƒ½æ¨¡å— | æ–‡ä»¶è·¯å¾„ | è¯´æ˜ |
|---------|---------|------|
| **Agent æ ¸å¿ƒ** | `src/agent.py` | AcademicAgent ä¸»ç±»ï¼ŒRAG é—®ç­”é€»è¾‘ |
| **æ–‡æ¡£åŠ è½½** | `src/loaders/document_loader.py` | æ”¯æŒ PDFã€DOCXã€MDã€TXT |
| **ç´¢å¼•æ„å»º** | `src/indexing/indexer.py` | å‘é‡ç´¢å¼•æ„å»ºå’Œç®¡ç† |
| **æŸ¥è¯¢å¼•æ“** | `src/query/qa_engine.py` | é—®ç­”å¼•æ“ï¼Œæ£€ç´¢å’Œç”Ÿæˆ |
| **ç½‘ç»œæœç´¢** | `src/tools/web_search.py` | DuckDuckGo æœç´¢é›†æˆ |
| **å¤šè½®å¯¹è¯** | `src/agent.py` | å¯¹è¯å†å²ç®¡ç† |

---

## æ ¸å¿ƒæ¶æ„

### æ•´ä½“æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    User Layer                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚   Web UI     â”‚              â”‚   CLI        â”‚    â”‚
â”‚  â”‚  (Gradio)    â”‚              â”‚   (Typer)    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Application Layer                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              Agent (main.py)                  â”‚  â”‚
â”‚  â”‚  â€¢ Query Processing                          â”‚  â”‚
â”‚  â”‚  â€¢ Context Management                        â”‚  â”‚
â”‚  â”‚  â€¢ Response Generation                       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼               â–¼               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Document   â”‚ â”‚   Retrieval â”‚ â”‚    LLM     â”‚
â”‚   Loader     â”‚ â”‚   Engine    â”‚ â”‚  Service   â”‚
â”‚              â”‚ â”‚             â”‚ â”‚            â”‚
â”‚ â€¢ PDF        â”‚ â”‚ â€¢ Vector    â”‚ â”‚ â€¢ OpenAI   â”‚
â”‚ â€¢ DOCX       â”‚ â”‚   Store     â”‚ â”‚ â€¢ Moonshot â”‚
â”‚ â€¢ Markdown   â”‚ â”‚ â€¢ Semantic  â”‚ â”‚ â€¢ DeepSeek â”‚
â”‚ â€¢ TXT        â”‚ â”‚   Search    â”‚ â”‚            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚               â”‚               â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Storage Layer                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Chroma     â”‚  â”‚   Config     â”‚  â”‚   Logs   â”‚ â”‚
â”‚  â”‚ Vector Store â”‚  â”‚    (.env)    â”‚  â”‚          â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## æ ¸å¿ƒæ¨¡å—

### 1. AcademicAgent - æ ¸å¿ƒ Agent ç±»

**ä½ç½®**: `src/agent.py`

**èŒè´£**:
- å‘é‡ç´¢å¼•ç®¡ç†ï¼ˆæ„å»ºã€åŠ è½½ã€æŒä¹…åŒ–ï¼‰
- RAG é—®ç­”ï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰
- ç›´æ¥ LLM å¯¹è¯ï¼ˆæ”¯æŒæ–‡æ¡£é™„ä»¶ï¼‰
- å¤šè½®å¯¹è¯å†å²ç®¡ç†
- æ–‡æ¡£ç»Ÿè®¡å’Œç®¡ç†

#### æ ¸å¿ƒæ–¹æ³•

##### 1.1 ç´¢å¼•ç®¡ç†

```python
from src.agent import AcademicAgent

# åˆ›å»º Agent å®ä¾‹ï¼ˆè‡ªåŠ¨åŠ è½½ç´¢å¼•ï¼‰
agent = AcademicAgent()

# åˆ›å»º Agent ä½†ä¸è‡ªåŠ¨åŠ è½½ç´¢å¼•
agent = AcademicAgent(auto_load=False)

# æ‰‹åŠ¨æ„å»ºæˆ–åŠ è½½ç´¢å¼•
agent.load_or_build_index(force_rebuild=False)

# å¼ºåˆ¶é‡å»ºç´¢å¼•
agent.rebuild_index()

# æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
if agent._index_exists():
    print("ç´¢å¼•å·²å­˜åœ¨")
```

**æ–¹æ³•è¯¦è§£**:

```python
def load_or_build_index(self, force_rebuild: bool = False) -> VectorStoreIndex:
    """
    åŠ è½½æˆ–æ„å»ºå‘é‡ç´¢å¼•
    
    Args:
        force_rebuild: æ˜¯å¦å¼ºåˆ¶é‡å»ºç´¢å¼•
        
    Returns:
        VectorStoreIndex: å‘é‡ç´¢å¼•å®ä¾‹
        
    æµç¨‹:
        1. å¦‚æœ force_rebuild=Trueï¼Œç›´æ¥é‡å»º
        2. å¦åˆ™æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
        3. å­˜åœ¨åˆ™åŠ è½½ï¼Œä¸å­˜åœ¨åˆ™æ„å»º
    """
```

##### 1.2 RAG é—®ç­”

```python
# åŸºç¡€ RAG æŸ¥è¯¢
result = agent.query(
    question="Transformer çš„æ ¸å¿ƒåˆ›æ–°æ˜¯ä»€ä¹ˆï¼Ÿ",
    mode="rag"
)

# é«˜çº§ RAG æŸ¥è¯¢ï¼ˆå¯ç”¨ç½‘ç»œæœç´¢ï¼‰
result = agent.query(
    question="æœ€æ–°çš„å¤§è¯­è¨€æ¨¡å‹æœ‰å“ªäº›ï¼Ÿ",
    mode="rag",
    enable_web_search=True,
    top_k=5,
    similarity_threshold=0.7
)

# å¤„ç†ç»“æœ
print(f"ç­”æ¡ˆ: {result['answer']}")
print(f"æ£€ç´¢åˆ°çš„æ–‡æ¡£: {len(result.get('source_nodes', []))}")

# æŸ¥çœ‹æ¥æºæ–‡æ¡£
for i, node in enumerate(result.get('source_nodes', []), 1):
    print(f"\næ¥æº {i}:")
    print(f"  æ–‡ä»¶: {node.node.metadata.get('file_name')}")
    print(f"  ç›¸ä¼¼åº¦: {node.score:.2f}")
    print(f"  å†…å®¹ç‰‡æ®µ: {node.node.text[:100]}...")

# æŸ¥çœ‹ç½‘ç»œæœç´¢ç»“æœï¼ˆå¦‚æœå¯ç”¨ï¼‰
if result.get('web_sources'):
    print("\nç½‘ç»œæœç´¢ç»“æœ:")
    for source in result['web_sources']:
        print(f"  - {source['title']}: {source['url']}")
```

**æ–¹æ³•è¯¦è§£**:

```python
def query(
    self,
    question: str,
    mode: str = "rag",
    enable_web_search: bool = False,
    top_k: int = 5,
    similarity_threshold: float = 0.7,
    response_mode: str = "compact"
) -> Dict[str, Any]:
    """
    æ‰§è¡ŒæŸ¥è¯¢
    
    Args:
        question: ç”¨æˆ·é—®é¢˜
        mode: æŸ¥è¯¢æ¨¡å¼ ('rag' æˆ– 'llm')
        enable_web_search: æ˜¯å¦å¯ç”¨ç½‘ç»œæœç´¢
        top_k: æ£€ç´¢æ–‡æ¡£æ•°é‡
        similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
        response_mode: å“åº”æ¨¡å¼ (compact/tree_summarize/refine)
        
    Returns:
        {
            'answer': str,              # ç”Ÿæˆçš„ç­”æ¡ˆ
            'source_nodes': List,       # æ£€ç´¢åˆ°çš„æ–‡æ¡£èŠ‚ç‚¹
            'web_sources': List[Dict],  # ç½‘ç»œæœç´¢ç»“æœï¼ˆå¦‚æœå¯ç”¨ï¼‰
            'query_time': float         # æŸ¥è¯¢è€—æ—¶
        }
    """
```

##### 1.3 ç›´æ¥ LLM å¯¹è¯ï¼ˆæ”¯æŒæ–‡æ¡£é™„ä»¶ï¼‰

```python
# åŸºç¡€ LLM å¯¹è¯
result = agent.query_direct(
    question="è§£é‡Šä¸€ä¸‹æœºå™¨å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µ"
)

# å¸¦æ–‡æ¡£é™„ä»¶çš„å¯¹è¯ï¼ˆMoonshot APIï¼‰
result = agent.query_direct(
    question="è¯·æ€»ç»“è¿™ç¯‡è®ºæ–‡çš„ä¸»è¦å†…å®¹",
    document_paths=[
        "data/documents/paper1.pdf",
        "data/documents/paper2.pdf"
    ]
)

# å¸¦ç½‘ç»œæœç´¢çš„å¯¹è¯
result = agent.query_direct(
    question="2024å¹´AIé¢†åŸŸæœ‰å“ªäº›é‡å¤§çªç ´ï¼Ÿ",
    enable_web_search=True
)

# ç»„åˆä½¿ç”¨ï¼šæ–‡æ¡£é™„ä»¶ + ç½‘ç»œæœç´¢
result = agent.query_direct(
    question="å¯¹æ¯”è¿™ç¯‡è®ºæ–‡å’Œæœ€æ–°çš„ç ”ç©¶è¿›å±•",
    document_paths=["data/documents/paper.pdf"],
    enable_web_search=True
)

# å¤„ç†ç»“æœ
print(f"ç­”æ¡ˆ: {result['answer']}")
print(f"ä½¿ç”¨çš„æ–‡æ¡£: {result.get('attached_documents', [])}")
print(f"ç½‘ç»œæ¥æº: {len(result.get('web_sources', []))}")
```

**æ–¹æ³•è¯¦è§£**:

```python
def query_direct(
    self,
    question: str,
    document_paths: Optional[List[str]] = None,
    enable_web_search: bool = False,
    temperature: float = 0.1
) -> Dict[str, Any]:
    """
    ç›´æ¥è°ƒç”¨ LLMï¼ˆä¸ä½¿ç”¨ RAGï¼‰
    
    Args:
        question: ç”¨æˆ·é—®é¢˜
        document_paths: æ–‡æ¡£è·¯å¾„åˆ—è¡¨ï¼ˆMoonshot æ”¯æŒæ–‡ä»¶ä¸Šä¼ ï¼‰
        enable_web_search: æ˜¯å¦å¯ç”¨ç½‘ç»œæœç´¢
        temperature: LLM æ¸©åº¦å‚æ•°
        
    Returns:
        {
            'answer': str,
            'attached_documents': List[str],  # ä½¿ç”¨çš„æ–‡æ¡£åˆ—è¡¨
            'web_sources': List[Dict],        # ç½‘ç»œæœç´¢ç»“æœ
            'mode': 'direct'
        }
        
    æ³¨æ„:
        - æ–‡æ¡£ä¸Šä¼ åŠŸèƒ½éœ€è¦ Moonshot API
        - æ”¯æŒ PDFã€DOCXã€MDã€TXT æ ¼å¼
        - æ–‡ä»¶ä¼šè¢«ç¼“å­˜ï¼Œé¿å…é‡å¤ä¸Šä¼ 
    """
```

##### 1.4 å¤šè½®å¯¹è¯ç®¡ç†

```python
# å¤šè½®å¯¹è¯ç¤ºä¾‹
agent = AcademicAgent()

# ç¬¬ä¸€è½®å¯¹è¯
result1 = agent.query_direct("ä»€ä¹ˆæ˜¯ Transformerï¼Ÿ")
print(result1['answer'])

# ç¬¬äºŒè½®ï¼ˆè‡ªåŠ¨è®°å¿†ä¸Šä¸‹æ–‡ï¼‰
result2 = agent.query_direct("å®ƒæœ‰å“ªäº›åº”ç”¨ï¼Ÿ")  # "å®ƒ"ä¼šè‡ªåŠ¨ç†è§£ä¸º Transformer
print(result2['answer'])

# ç¬¬ä¸‰è½®ï¼ˆç»§ç»­æ·±å…¥ï¼‰
result3 = agent.query_direct("åœ¨ NLP é¢†åŸŸçš„å…·ä½“åº”ç”¨æœ‰å“ªäº›ï¼Ÿ")
print(result3['answer'])

# æŸ¥çœ‹å¯¹è¯å†å²
history = agent.get_chat_history()
for i, msg in enumerate(history, 1):
    print(f"\n[{i}] {msg['role']}: {msg['content'][:50]}...")

# æ¸…ç©ºå¯¹è¯å†å²
agent.clear_chat_history()

# è®¾ç½®å†å²ä¿ç•™è½®æ•°ï¼ˆé»˜è®¤10è½®ï¼‰
agent.set_max_history_turns(20)

# æ„å»ºä¸Šä¸‹æ–‡ Promptï¼ˆå†…éƒ¨ä½¿ç”¨ï¼‰
context_prompt = agent._build_context_prompt("å½“å‰é—®é¢˜")
```

**å¯¹è¯å†å²ç®¡ç†æ–¹æ³•**:

```python
def _update_chat_history(self, user_message: str, assistant_message: str):
    """æ›´æ–°å¯¹è¯å†å²ï¼Œè‡ªåŠ¨ç®¡ç†å†å²é•¿åº¦"""
    
def clear_chat_history(self):
    """æ¸…ç©ºå¯¹è¯å†å²"""
    
def get_chat_history(self) -> List[Dict[str, str]]:
    """è·å–å¯¹è¯å†å²"""
    
def set_max_history_turns(self, max_turns: int):
    """è®¾ç½®æœ€å¤§ä¿ç•™å†å²è½®æ•°"""
    
def _build_context_prompt(self, question: str) -> str:
    """
    æ„å»ºåŒ…å«å†å²ä¸Šä¸‹æ–‡çš„ Prompt
    
    æ ¼å¼:
        å†å²å¯¹è¯:
        ç”¨æˆ·: é—®é¢˜1
        åŠ©æ‰‹: å›ç­”1
        ç”¨æˆ·: é—®é¢˜2
        åŠ©æ‰‹: å›ç­”2
        
        å½“å‰é—®é¢˜: {question}
    """
```

##### 1.5 æ–‡æ¡£å’Œç»Ÿè®¡ç®¡ç†

```python
# åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ–‡æ¡£
documents = agent.list_available_documents()
print(f"æ‰¾åˆ° {len(documents)} ä¸ªæ–‡æ¡£:")
for doc in documents:
    print(f"  - {doc}")

# åˆ—å‡ºå·²ç´¢å¼•çš„è®ºæ–‡ï¼ˆç®€ç•¥ï¼‰
papers = agent.list_papers(detailed=False)
for paper in papers:
    print(f"{paper['file_name']}: {paper['char_count']} å­—ç¬¦")

# åˆ—å‡ºå·²ç´¢å¼•çš„è®ºæ–‡ï¼ˆè¯¦ç»†ï¼‰
papers = agent.list_papers(detailed=True)
for paper in papers:
    print(f"\næ–‡ä»¶: {paper['file_name']}")
    print(f"  æ ¼å¼: {paper['format']}")
    print(f"  å¤§å°: {paper['size_mb']:.2f} MB")
    print(f"  å­—ç¬¦æ•°: {paper['char_count']}")
    print(f"  é¢„è§ˆ: {paper['preview']}")

# è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯
stats = agent.get_statistics()
print(f"æ–‡æ¡£æ€»æ•°: {stats['total_documents']}")
print(f"æ€»å­—ç¬¦æ•°: {stats['total_chars']:,}")
print(f"æ€»å¤§å°: {stats['total_size_mb']:.2f} MB")
print(f"ç´¢å¼•çŠ¶æ€: {stats['index_built']}")

# æ¸…ç©ºæ–‡ä»¶ä¸Šä¼ ç¼“å­˜
agent.clear_file_cache()
```

#### å®Œæ•´ä½¿ç”¨ç¤ºä¾‹

```python
from src.agent import AcademicAgent
from pathlib import Path

def main():
    """å®Œæ•´çš„ Agent ä½¿ç”¨ç¤ºä¾‹"""
    
    # 1. åˆå§‹åŒ– Agent
    print("=" * 70)
    print("åˆå§‹åŒ– Academic Agent")
    print("=" * 70)
    
    agent = AcademicAgent(
        documents_dir="./data/documents",
        index_dir="./data/vector_store",
        auto_load=True  # è‡ªåŠ¨åŠ è½½æˆ–æ„å»ºç´¢å¼•
    )
    
    # 2. æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯
    stats = agent.get_statistics()
    print(f"\næ–‡æ¡£ç»Ÿè®¡:")
    print(f"  æ€»æ–‡æ¡£æ•°: {stats['total_documents']}")
    print(f"  æ€»å­—ç¬¦æ•°: {stats['total_chars']:,}")
    print(f"  ç´¢å¼•çŠ¶æ€: {'å·²æ„å»º' if stats['index_built'] else 'æœªæ„å»º'}")
    
    # 3. RAG é—®ç­”ç¤ºä¾‹
    print("\n" + "=" * 70)
    print("RAG é—®ç­”ç¤ºä¾‹")
    print("=" * 70)
    
    result = agent.query(
        question="ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æœºåˆ¶ï¼Ÿ",
        mode="rag",
        top_k=3
    )
    
    print(f"\né—®é¢˜: ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æœºåˆ¶ï¼Ÿ")
    print(f"\nç­”æ¡ˆ:\n{result['answer']}")
    print(f"\næ¥æºæ–‡æ¡£: {len(result.get('source_nodes', []))} ä¸ª")
    
    # 4. å¸¦ç½‘ç»œæœç´¢çš„ RAG
    print("\n" + "=" * 70)
    print("RAG + ç½‘ç»œæœç´¢ç¤ºä¾‹")
    print("=" * 70)
    
    result = agent.query(
        question="2024å¹´å¤§è¯­è¨€æ¨¡å‹çš„æœ€æ–°è¿›å±•",
        mode="rag",
        enable_web_search=True,
        top_k=3
    )
    
    print(f"\né—®é¢˜: 2024å¹´å¤§è¯­è¨€æ¨¡å‹çš„æœ€æ–°è¿›å±•")
    print(f"\nç­”æ¡ˆ:\n{result['answer']}")
    if result.get('web_sources'):
        print(f"\nç½‘ç»œæ¥æº: {len(result['web_sources'])} ä¸ª")
    
    # 5. ç›´æ¥ LLM å¯¹è¯
    print("\n" + "=" * 70)
    print("ç›´æ¥ LLM å¯¹è¯ç¤ºä¾‹")
    print("=" * 70)
    
    result = agent.query_direct(
        question="è§£é‡Šä¸€ä¸‹æ·±åº¦å­¦ä¹ çš„åŸºæœ¬åŸç†"
    )
    
    print(f"\né—®é¢˜: è§£é‡Šä¸€ä¸‹æ·±åº¦å­¦ä¹ çš„åŸºæœ¬åŸç†")
    print(f"\nç­”æ¡ˆ:\n{result['answer']}")
    
    # 6. å¤šè½®å¯¹è¯ç¤ºä¾‹
    print("\n" + "=" * 70)
    print("å¤šè½®å¯¹è¯ç¤ºä¾‹")
    print("=" * 70)
    
    # ç¬¬ä¸€è½®
    result1 = agent.query_direct("ä»€ä¹ˆæ˜¯å·ç§¯ç¥ç»ç½‘ç»œï¼Ÿ")
    print(f"\n[ç”¨æˆ·] ä»€ä¹ˆæ˜¯å·ç§¯ç¥ç»ç½‘ç»œï¼Ÿ")
    print(f"[åŠ©æ‰‹] {result1['answer'][:200]}...")
    
    # ç¬¬äºŒè½®ï¼ˆæœ‰ä¸Šä¸‹æ–‡è®°å¿†ï¼‰
    result2 = agent.query_direct("å®ƒä¸»è¦ç”¨åœ¨å“ªäº›é¢†åŸŸï¼Ÿ")
    print(f"\n[ç”¨æˆ·] å®ƒä¸»è¦ç”¨åœ¨å“ªäº›é¢†åŸŸï¼Ÿ")
    print(f"[åŠ©æ‰‹] {result2['answer'][:200]}...")
    
    # æŸ¥çœ‹å†å²
    history = agent.get_chat_history()
    print(f"\nå¯¹è¯å†å²: {len(history)} æ¡æ¶ˆæ¯")
    
    # 7. æ–‡æ¡£é™„ä»¶ç¤ºä¾‹ï¼ˆMoonshot APIï¼‰
    print("\n" + "=" * 70)
    print("æ–‡æ¡£é™„ä»¶ç¤ºä¾‹")
    print("=" * 70)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ–‡æ¡£
    docs = agent.list_available_documents()
    if docs:
        result = agent.query_direct(
            question="è¯·æ€»ç»“è¿™ç¯‡æ–‡æ¡£çš„ä¸»è¦å†…å®¹",
            document_paths=[docs[0]]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ–‡æ¡£
        )
        print(f"\né—®é¢˜: è¯·æ€»ç»“è¿™ç¯‡æ–‡æ¡£çš„ä¸»è¦å†…å®¹")
        print(f"ä½¿ç”¨æ–‡æ¡£: {docs[0]}")
        print(f"\nç­”æ¡ˆ:\n{result['answer'][:300]}...")
    else:
        print("\næ²¡æœ‰å¯ç”¨æ–‡æ¡£ï¼Œè·³è¿‡æ–‡æ¡£é™„ä»¶ç¤ºä¾‹")
    
    print("\n" + "=" * 70)
    print("ç¤ºä¾‹å®Œæˆ")
    print("=" * 70)

if __name__ == "__main__":
    main()
```

---

### 2. DocumentLoader - æ–‡æ¡£åŠ è½½å™¨

**ä½ç½®**: `src/loaders/document_loader.py`

**èŒè´£**:
- åŠ è½½å¤šç§æ ¼å¼æ–‡æ¡£ï¼ˆPDFã€DOCXã€MDã€TXTï¼‰
- æ–‡æœ¬æ¸…æ´—å’Œè§„èŒƒåŒ–
- å…ƒæ•°æ®æå–
- ç»Ÿè®¡ä¿¡æ¯è®¡ç®—

#### æ ¸å¿ƒæ–¹æ³•

```python
from src.loaders.document_loader import DocumentLoader

# åŸºç¡€ä½¿ç”¨
loader = DocumentLoader(input_dir="./data/documents")
documents = loader.load_documents()

# é«˜çº§é…ç½®
loader = DocumentLoader(
    input_dir="./data/documents",
    recursive=True,           # é€’å½’æ‰«æå­ç›®å½•
    clean_text=True,          # æ¸…æ´—æ–‡æœ¬
    preserve_formatting=False # ä¸ä¿ç•™æ ¼å¼
)
documents = loader.load_documents()

# è·å–ç»Ÿè®¡ä¿¡æ¯
stats = loader.get_statistics()
print(f"æ€»æ–‡æ¡£æ•°: {stats['total_documents']}")
print(f"æ€»æ–‡ä»¶æ•°: {stats['total_files']}")
print(f"æ–‡ä»¶ç±»å‹: {stats['file_types']}")
print(f"æ€»å¤§å°: {stats['total_size_mb']:.2f} MB")
print(f"æ€»å­—ç¬¦æ•°: {stats['total_chars']:,}")
```

#### æ”¯æŒçš„æ–‡æ¡£æ ¼å¼

```python
# PDF æ–‡ä»¶
# - ä¼˜å…ˆä½¿ç”¨ PyMuPDFï¼ˆæ›´å¿«æ›´å‡†ç¡®ï¼‰
# - å¤‡é€‰ pypdf
documents_pdf = loader._load_pdf("path/to/paper.pdf")

# DOCX æ–‡ä»¶
documents_docx = loader._load_docx("path/to/document.docx")

# Markdown æ–‡ä»¶
documents_md = loader._load_markdown("path/to/readme.md")

# æ–‡æœ¬æ–‡ä»¶
documents_txt = loader._load_text("path/to/notes.txt")
```

#### æ–‡æœ¬æ¸…æ´—åŠŸèƒ½

```python
# æ¸…æ´—æ–‡æœ¬ï¼ˆå»é™¤æ§åˆ¶å­—ç¬¦ã€è§„èŒƒç©ºç™½ï¼‰
cleaned_text = loader._clean_text(raw_text)

# æ¸…æ´—æ­¥éª¤:
# 1. ç§»é™¤æ§åˆ¶å­—ç¬¦
# 2. è§„èŒƒå¤šä½™çš„æ¢è¡Œç¬¦
# 3. è§„èŒƒç©ºæ ¼
# 4. ä¿®æ­£ä¸­æ–‡æ ‡ç‚¹åçš„ç©ºæ ¼
# 5. ä¿®æ­£è‹±æ–‡æ ‡ç‚¹åçš„å¤šä½™ç©ºæ ¼
```

#### å®Œæ•´ä½¿ç”¨ç¤ºä¾‹

```python
from src.loaders.document_loader import DocumentLoader
from pathlib import Path

def load_and_analyze_documents():
    """åŠ è½½å¹¶åˆ†ææ–‡æ¡£"""
    
    # 1. åˆ›å»ºåŠ è½½å™¨
    loader = DocumentLoader(
        input_dir="./data/documents",
        recursive=True,
        clean_text=True
    )
    
    # 2. åŠ è½½æ–‡æ¡£
    print("æ­£åœ¨åŠ è½½æ–‡æ¡£...")
    documents = loader.load_documents()
    
    # 3. æŸ¥çœ‹ç»Ÿè®¡
    stats = loader.get_statistics()
    print(f"\næ–‡æ¡£ç»Ÿè®¡:")
    print(f"  æ€»æ–‡æ¡£å—æ•°: {stats['total_documents']}")
    print(f"  æ€»æ–‡ä»¶æ•°: {stats['total_files']}")
    print(f"  æ–‡ä»¶ç±»å‹åˆ†å¸ƒ: {stats['file_types']}")
    print(f"  æ€»å¤§å°: {stats['total_size_mb']:.2f} MB")
    print(f"  æ€»å­—ç¬¦æ•°: {stats['total_chars']:,}")
    print(f"  æ€»å•è¯æ•°: {stats['total_words']:,}")
    
    # 4. æŸ¥çœ‹æ–‡æ¡£è¯¦æƒ…
    print(f"\næ–‡æ¡£è¯¦æƒ…:")
    for i, doc in enumerate(documents[:3], 1):  # åªæ˜¾ç¤ºå‰3ä¸ª
        print(f"\næ–‡æ¡£ {i}:")
        print(f"  æ–‡ä»¶å: {doc.metadata.get('file_name')}")
        print(f"  æ ¼å¼: {doc.metadata.get('format')}")
        print(f"  å¤§å°: {doc.metadata.get('file_size_mb', 0):.2f} MB")
        print(f"  å­—ç¬¦æ•°: {doc.metadata.get('char_count')}")
        print(f"  é¢„è§ˆ: {doc.text[:100]}...")
    
    return documents

if __name__ == "__main__":
    documents = load_and_analyze_documents()
```

---

### 3. WebSearchTool - ç½‘ç»œæœç´¢å·¥å…·

**ä½ç½®**: `src/tools/web_search.py`

**èŒè´£**:
- DuckDuckGo ç½‘ç»œæœç´¢
- å¤šæœç´¢å¼•æ“æ”¯æŒï¼ˆDuckDuckGoã€SearXNGã€SerpAPIï¼‰
- è‡ªåŠ¨æ•…éšœè½¬ç§»
- ç»“æœæ ¼å¼åŒ–

#### æ ¸å¿ƒæ–¹æ³•

```python
from src.tools.web_search import WebSearchTool

# åŸºç¡€æœç´¢
tool = WebSearchTool(max_results=5)
results = tool.search("æœºå™¨å­¦ä¹ æœ€æ–°è¿›å±•")

# ä½¿ç”¨ç‰¹å®šæœç´¢å¼•æ“
tool = WebSearchTool(
    max_results=5,
    engine="duckduckgo"  # æˆ– "searxng", "serpapi"
)
results = tool.search("æ·±åº¦å­¦ä¹ ")

# å¤„ç†ç»“æœ
for i, result in enumerate(results, 1):
    print(f"\n{i}. {result['title']}")
    print(f"   URL: {result['url']}")
    print(f"   æ‘˜è¦: {result['snippet'][:100]}...")
```

#### å¤šå¼•æ“æ”¯æŒ

```python
# DuckDuckGoï¼ˆé»˜è®¤ï¼Œå…è´¹ï¼‰
tool = WebSearchTool(engine="duckduckgo")

# SearXNGï¼ˆéœ€è¦è‡ªå»ºå®ä¾‹ï¼‰
tool = WebSearchTool(
    engine="searxng",
    searxng_base_url="http://localhost:8888"
)

# SerpAPIï¼ˆéœ€è¦ API Keyï¼‰
tool = WebSearchTool(
    engine="serpapi",
    serpapi_api_key="your-api-key"
)
```

#### å®Œæ•´ä½¿ç”¨ç¤ºä¾‹

```python
from src.tools.web_search import WebSearchTool
import os

def search_with_fallback(query: str):
    """å¸¦æ•…éšœè½¬ç§»çš„æœç´¢"""
    
    # å°è¯•å¤šä¸ªå¼•æ“
    engines = ["duckduckgo", "searxng"]
    
    for engine in engines:
        try:
            print(f"\nå°è¯•ä½¿ç”¨ {engine}...")
            tool = WebSearchTool(max_results=3, engine=engine)
            results = tool.search(query)
            
            if results:
                print(f"âœ“ ä½¿ç”¨ {engine} æ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
                return results
        except Exception as e:
            print(f"âœ— {engine} å¤±è´¥: {e}")
            continue
    
    print("æ‰€æœ‰æœç´¢å¼•æ“éƒ½å¤±è´¥äº†")
    return []

# ä½¿ç”¨
results = search_with_fallback("2024å¹´AIé¢†åŸŸçªç ´")
for result in results:
    print(f"- {result['title']}: {result['url']}")
```

---

### 4. é…ç½®æ¨¡å—

#### 4.1 SystemConfig - ç³»ç»Ÿé…ç½®

**ä½ç½®**: `config/settings.py`

```python
from config import SystemConfig

# è®¿é—®é…ç½®
print(f"æ–‡æ¡£ç›®å½•: {SystemConfig.DOCUMENTS_DIR}")
print(f"å‘é‡åº“ç›®å½•: {SystemConfig.VECTOR_STORE_DIR}")
print(f"Chunk å¤§å°: {SystemConfig.CHUNK_SIZE}")
print(f"Chunk é‡å : {SystemConfig.CHUNK_OVERLAP}")
print(f"æ£€ç´¢ Top-K: {SystemConfig.RETRIEVAL_TOP_K}")

# ç¡®ä¿ç›®å½•å­˜åœ¨
SystemConfig.ensure_directories()
```

#### 4.2 LLM é…ç½®

**ä½ç½®**: `config/llm_config.py`

```python
from config.llm_config import get_llm, get_embedding_model

# è·å– LLM å®ä¾‹
llm = get_llm()

# è·å– Embedding æ¨¡å‹
embed_model = get_embedding_model(provider="huggingface")

# è‡ªå®šä¹‰é…ç½®
llm = get_llm(
    api_key="your-api-key",
    api_base="https://api.moonshot.cn/v1",
    model="moonshot-v1-8k"
)
```

---

## API å‚è€ƒ

### create_agent() - å¿«é€Ÿåˆ›å»º Agent

```python
from src.agent import create_agent

def create_agent(
    documents_dir: str = "./data/documents",
    index_dir: str = "./data/vector_store",
    force_rebuild: bool = False,
    auto_load: bool = True
) -> AcademicAgent:
    """
    å¿«é€Ÿåˆ›å»º Academic Agent å®ä¾‹
    
    Args:
        documents_dir: æ–‡æ¡£ç›®å½•è·¯å¾„
        index_dir: ç´¢å¼•å­˜å‚¨ç›®å½•
        force_rebuild: æ˜¯å¦å¼ºåˆ¶é‡å»ºç´¢å¼•
        auto_load: æ˜¯å¦è‡ªåŠ¨åŠ è½½ç´¢å¼•
        
    Returns:
        AcademicAgent å®ä¾‹
        
    ç¤ºä¾‹:
        # é»˜è®¤é…ç½®
        agent = create_agent()
        
        # è‡ªå®šä¹‰ç›®å½•
        agent = create_agent(
            documents_dir="./my_papers",
            force_rebuild=True
        )
    """
```

### AcademicAgent ç±»å®Œæ•´ API

```python
class AcademicAgent:
    """å­¦æœ¯è®ºæ–‡é—®ç­” Agent"""
    
    # åˆå§‹åŒ–
    def __init__(
        documents_dir: Optional[str] = None,
        index_dir: Optional[str] = None,
        auto_load: bool = True
    )
    
    # ç´¢å¼•ç®¡ç†
    def load_or_build_index(force_rebuild: bool = False) -> VectorStoreIndex
    def rebuild_index() -> VectorStoreIndex
    def _index_exists() -> bool
    
    # æŸ¥è¯¢æ–¹æ³•
    def query(
        question: str,
        mode: str = "rag",
        enable_web_search: bool = False,
        top_k: int = 5,
        similarity_threshold: float = 0.7,
        response_mode: str = "compact"
    ) -> Dict[str, Any]
    
    def query_direct(
        question: str,
        document_paths: Optional[List[str]] = None,
        enable_web_search: bool = False,
        temperature: float = 0.1
    ) -> Dict[str, Any]
    
    # å¯¹è¯å†å²
    def get_chat_history() -> List[Dict[str, str]]
    def clear_chat_history()
    def set_max_history_turns(max_turns: int)
    
    # æ–‡æ¡£ç®¡ç†
    def list_papers(detailed: bool = False) -> List[Dict[str, Any]]
    def list_available_documents() -> List[str]
    def get_statistics() -> Dict[str, Any]
    
    # ç¼“å­˜ç®¡ç†
    def clear_file_cache()
```

### DocumentLoader ç±»å®Œæ•´ API

```python
class DocumentLoader:
    """æ–‡æ¡£åŠ è½½å™¨"""
    
    def __init__(
        input_dir: str,
        recursive: bool = True,
        clean_text: bool = True,
        preserve_formatting: bool = False
    )
    
    def load_documents() -> List[Document]
    def get_statistics() -> Dict[str, Any]
    def _clean_text(text: str) -> str
```

### WebSearchTool ç±»å®Œæ•´ API

```python
class WebSearchTool:
    """ç½‘ç»œæœç´¢å·¥å…·"""
    
    def __init__(
        max_results: int = 3,
        engine: str = "duckduckgo"
    )
    
    def search(query: str) -> List[Dict[str, str]]
```

---

## æ‰©å±•å¼€å‘

### 1. æ·»åŠ æ–°çš„æ–‡æ¡£æ ¼å¼æ”¯æŒ

#### æ­¥éª¤ 1: åˆ›å»ºè‡ªå®šä¹‰åŠ è½½å™¨

```python
# custom_loaders/epub_loader.py
from pathlib import Path
from typing import List
from llama_index.core import Document

class EPUBLoader:
    """EPUB æ–‡æ¡£åŠ è½½å™¨ç¤ºä¾‹"""
    
    def load(self, file_path: str) -> List[Document]:
        """åŠ è½½ EPUB æ–‡ä»¶"""
        try:
            import ebooklib
            from ebooklib import epub
            from bs4 import BeautifulSoup
        except ImportError:
            raise ImportError("è¯·å®‰è£…: pip install ebooklib beautifulsoup4")
        
        # è¯»å– EPUB æ–‡ä»¶
        book = epub.read_epub(file_path)
        
        # æå–æ–‡æœ¬å†…å®¹
        texts = []
        for item in book.get_items():
            if item.get_type() == ebooklib.ITEM_DOCUMENT:
                soup = BeautifulSoup(item.get_content(), 'html.parser')
                text = soup.get_text()
                texts.append(text)
        
        # åˆå¹¶æ–‡æœ¬
        full_text = '\n\n'.join(texts)
        
        # åˆ›å»º Document å¯¹è±¡
        doc = Document(
            text=full_text,
            metadata={
                'file_name': Path(file_path).name,
                'file_path': file_path,
                'format': 'epub',
                'title': book.get_metadata('DC', 'title')[0][0] if book.get_metadata('DC', 'title') else 'Unknown'
            }
        )
        
        return [doc]
```

#### æ­¥éª¤ 2: é›†æˆåˆ° DocumentLoader

```python
# ä¿®æ”¹ src/loaders/document_loader.py
from custom_loaders.epub_loader import EPUBLoader

class DocumentLoader:
    def __init__(self, ...):
        # æ·»åŠ  EPUB æ”¯æŒ
        self.supported_extensions.update({
            'epub': ['.epub']
        })
        self.epub_loader = EPUBLoader()
    
    def _load_single_file(self, file_path: str) -> List[Document]:
        # æ·»åŠ  EPUB å¤„ç†
        if file_path.endswith('.epub'):
            return self.epub_loader.load(file_path)
        # ... å…¶ä»–æ ¼å¼
```

### 2. æ·»åŠ æ–°çš„ LLM æä¾›å•†

#### ç¤ºä¾‹ï¼šé›†æˆ DeepSeek API

```python
# custom_llm/deepseek_llm.py
from typing import Optional, List, Dict, Any
from llama_index.core.llms import LLM, CompletionResponse
from llama_index.core.base.llms.types import ChatMessage
import requests

class DeepSeekLLM(LLM):
    """DeepSeek LLM å®ç°"""
    
    def __init__(
        self,
        api_key: str,
        model: str = "deepseek-chat",
        api_base: str = "https://api.deepseek.com/v1",
        **kwargs
    ):
        super().__init__(**kwargs)
        self.api_key = api_key
        self.model = model
        self.api_base = api_base
    
    def complete(self, prompt: str, **kwargs) -> CompletionResponse:
        """æ–‡æœ¬è¡¥å…¨"""
        response = requests.post(
            f"{self.api_base}/completions",
            headers={"Authorization": f"Bearer {self.api_key}"},
            json={
                "model": self.model,
                "prompt": prompt,
                **kwargs
            }
        )
        result = response.json()
        return CompletionResponse(text=result['choices'][0]['text'])
    
    def chat(self, messages: List[ChatMessage], **kwargs) -> CompletionResponse:
        """å¤šè½®å¯¹è¯"""
        response = requests.post(
            f"{self.api_base}/chat/completions",
            headers={"Authorization": f"Bearer {self.api_key}"},
            json={
                "model": self.model,
                "messages": [{"role": m.role, "content": m.content} for m in messages],
                **kwargs
            }
        )
        result = response.json()
        return CompletionResponse(text=result['choices'][0]['message']['content'])
    
    @property
    def metadata(self) -> Dict[str, Any]:
        return {
            "model": self.model,
            "provider": "deepseek"
        }
```

#### é›†æˆåˆ°é…ç½®

```python
# config/llm_config.py
def get_llm(provider: Optional[str] = None, **kwargs):
    """è·å– LLM å®ä¾‹"""
    provider = provider or os.getenv("LLM_PROVIDER", "openai")
    
    if provider == "deepseek":
        from custom_llm.deepseek_llm import DeepSeekLLM
        return DeepSeekLLM(
            api_key=os.getenv("DEEPSEEK_API_KEY"),
            model=os.getenv("DEEPSEEK_MODEL", "deepseek-chat")
        )
    elif provider == "openai":
        # ... ç°æœ‰ä»£ç 
        pass
```

### 3. è‡ªå®šä¹‰æ£€ç´¢ç­–ç•¥

#### å®ç°æ··åˆæ£€ç´¢ï¼ˆå…³é”®è¯ + è¯­ä¹‰ï¼‰

```python
# custom_retrieval/hybrid_retriever.py
from typing import List
from llama_index.core import VectorStoreIndex
from llama_index.core.schema import NodeWithScore
from rank_bm25 import BM25Okapi
import numpy as np

class HybridRetriever:
    """æ··åˆæ£€ç´¢å™¨ï¼šBM25 + å‘é‡æ£€ç´¢"""
    
    def __init__(
        self,
        index: VectorStoreIndex,
        bm25_weight: float = 0.3,
        vector_weight: float = 0.7
    ):
        self.index = index
        self.bm25_weight = bm25_weight
        self.vector_weight = vector_weight
        
        # æ„å»º BM25 ç´¢å¼•
        self._build_bm25_index()
    
    def _build_bm25_index(self):
        """æ„å»º BM25 ç´¢å¼•"""
        # è·å–æ‰€æœ‰æ–‡æ¡£
        all_nodes = list(self.index.docstore.docs.values())
        
        # åˆ†è¯ï¼ˆç®€å•æŒ‰ç©ºæ ¼åˆ†è¯ï¼Œå®é™…åº”ä½¿ç”¨åˆ†è¯å™¨ï¼‰
        corpus = [node.text.split() for node in all_nodes]
        
        # åˆ›å»º BM25 ç´¢å¼•
        self.bm25 = BM25Okapi(corpus)
        self.nodes = all_nodes
    
    def retrieve(self, query: str, top_k: int = 5) -> List[NodeWithScore]:
        """æ··åˆæ£€ç´¢"""
        # 1. BM25 æ£€ç´¢
        query_tokens = query.split()
        bm25_scores = self.bm25.get_scores(query_tokens)
        
        # 2. å‘é‡æ£€ç´¢
        retriever = self.index.as_retriever(similarity_top_k=top_k * 2)
        vector_results = retriever.retrieve(query)
        
        # 3. èåˆåˆ†æ•°
        # åˆ›å»ºèŠ‚ç‚¹IDåˆ°åˆ†æ•°çš„æ˜ å°„
        bm25_score_dict = {
            node.node_id: score 
            for node, score in zip(self.nodes, bm25_scores)
        }
        
        vector_score_dict = {
            result.node.node_id: result.score 
            for result in vector_results
        }
        
        # åˆå¹¶åˆ†æ•°ï¼ˆRRF - Reciprocal Rank Fusionï¼‰
        all_node_ids = set(bm25_score_dict.keys()) | set(vector_score_dict.keys())
        
        fused_scores = {}
        for node_id in all_node_ids:
            bm25_score = bm25_score_dict.get(node_id, 0)
            vector_score = vector_score_dict.get(node_id, 0)
            
            # å½’ä¸€åŒ–å¹¶åŠ æƒ
            fused_scores[node_id] = (
                self.bm25_weight * bm25_score + 
                self.vector_weight * vector_score
            )
        
        # 4. æ’åºå¹¶è¿”å› Top-K
        sorted_node_ids = sorted(
            fused_scores.keys(), 
            key=lambda x: fused_scores[x], 
            reverse=True
        )[:top_k]
        
        # æ„å»ºç»“æœ
        results = []
        for node_id in sorted_node_ids:
            node = next(n for n in self.nodes if n.node_id == node_id)
            results.append(NodeWithScore(
                node=node,
                score=fused_scores[node_id]
            ))
        
        return results
```

#### ä½¿ç”¨æ··åˆæ£€ç´¢

```python
from src.agent import AcademicAgent
from custom_retrieval.hybrid_retriever import HybridRetriever

# åˆ›å»º Agent
agent = AcademicAgent()

# åˆ›å»ºæ··åˆæ£€ç´¢å™¨
hybrid_retriever = HybridRetriever(
    index=agent.index,
    bm25_weight=0.3,
    vector_weight=0.7
)

# ä½¿ç”¨æ··åˆæ£€ç´¢
query = "Transformer æ³¨æ„åŠ›æœºåˆ¶"
results = hybrid_retriever.retrieve(query, top_k=5)

for i, result in enumerate(results, 1):
    print(f"\n{i}. åˆ†æ•°: {result.score:.4f}")
    print(f"   å†…å®¹: {result.node.text[:100]}...")
```

### 4. è‡ªå®šä¹‰ Prompt æ¨¡æ¿

#### åˆ›å»º Prompt ç®¡ç†å™¨

```python
# custom_prompts/prompt_manager.py
from typing import Dict
import yaml

class PromptManager:
    """Prompt æ¨¡æ¿ç®¡ç†å™¨"""
    
    def __init__(self, config_file: str = "prompts.yaml"):
        with open(config_file, 'r', encoding='utf-8') as f:
            self.prompts = yaml.safe_load(f)
    
    def get_prompt(self, name: str, **kwargs) -> str:
        """è·å–å¹¶æ ¼å¼åŒ– Prompt"""
        template = self.prompts.get(name)
        if not template:
            raise ValueError(f"Prompt '{name}' not found")
        return template.format(**kwargs)
    
    def register_prompt(self, name: str, template: str):
        """æ³¨å†Œæ–°çš„ Prompt"""
        self.prompts[name] = template
```

#### Prompt é…ç½®æ–‡ä»¶

```yaml
# prompts.yaml
rag_qa:
  system: |
    ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡åˆ†æåŠ©æ‰‹ã€‚
    è¯·åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ï¼Œç¡®ä¿ç­”æ¡ˆå‡†ç¡®ã€å®¢è§‚ã€‚
  
  user: |
    æ–‡æ¡£å†…å®¹:
    {context}
    
    é—®é¢˜: {question}
    
    è¯·æä¾›è¯¦ç»†çš„ç­”æ¡ˆï¼Œå¹¶å¼•ç”¨å…·ä½“çš„æ®µè½ã€‚

analysis:
  system: |
    ä½ æ˜¯ä¸€ä¸ªæ·±åº¦å­¦æœ¯è®ºæ–‡åˆ†æä¸“å®¶ã€‚
  
  user: |
    è¯·æ·±å…¥åˆ†æä»¥ä¸‹è®ºæ–‡:
    {paper_content}
    
    åˆ†æç»´åº¦:
    1. ç ”ç©¶é—®é¢˜å’ŒåŠ¨æœº
    2. æŠ€æœ¯æ–¹æ³•å’Œåˆ›æ–°ç‚¹
    3. å®éªŒè®¾è®¡å’Œç»“æœ
    4. ç»“è®ºå’Œæœªæ¥å·¥ä½œ
    5. ä¼˜åŠ¿å’Œå±€é™æ€§

comparison:
  system: |
    ä½ æ˜¯ä¸€ä¸ªè®ºæ–‡å¯¹æ¯”åˆ†æä¸“å®¶ã€‚
  
  user: |
    è®ºæ–‡ A:
    {paper_a}
    
    è®ºæ–‡ B:
    {paper_b}
    
    è¯·å¯¹æ¯”è¿™ä¸¤ç¯‡è®ºæ–‡çš„:
    1. ç ”ç©¶æ–¹æ³•å·®å¼‚
    2. æŠ€æœ¯åˆ›æ–°ç‚¹
    3. å®éªŒç»“æœå¯¹æ¯”
    4. å„è‡ªä¼˜åŠ£åŠ¿
```

#### ä½¿ç”¨è‡ªå®šä¹‰ Prompt

```python
from custom_prompts.prompt_manager import PromptManager

# åŠ è½½ Prompt ç®¡ç†å™¨
pm = PromptManager("prompts.yaml")

# ä½¿ç”¨é¢„å®šä¹‰ Prompt
rag_prompt = pm.get_prompt(
    "rag_qa",
    context="æ–‡æ¡£å†…å®¹...",
    question="ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æœºåˆ¶ï¼Ÿ"
)

# æ³¨å†Œæ–° Prompt
pm.register_prompt(
    "summary",
    "è¯·æ€»ç»“ä»¥ä¸‹å†…å®¹:\n{content}\n\nè¦æ±‚:\n1. ç®€æ´æ˜äº†\n2. ä¿ç•™å…³é”®ä¿¡æ¯"
)

# ä½¿ç”¨æ–° Prompt
summary_prompt = pm.get_prompt("summary", content="è¦æ€»ç»“çš„å†…å®¹...")
```

### 5. æ·»åŠ åå¤„ç†å™¨

#### ç­”æ¡ˆè´¨é‡è¯„ä¼°å™¨

```python
# custom_postprocessors/quality_evaluator.py
from typing import Dict, List
from llama_index.core.llms import LLM

class AnswerQualityEvaluator:
    """ç­”æ¡ˆè´¨é‡è¯„ä¼°å™¨"""
    
    def __init__(self, llm: LLM):
        self.llm = llm
    
    def evaluate(
        self,
        question: str,
        answer: str,
        sources: List[Dict]
    ) -> Dict[str, float]:
        """è¯„ä¼°ç­”æ¡ˆè´¨é‡"""
        
        # 1. ç›¸å…³æ€§è¯„åˆ† (0-1)
        relevance = self._evaluate_relevance(question, answer)
        
        # 2. å®Œæ•´æ€§è¯„åˆ† (0-1)
        completeness = self._evaluate_completeness(answer, sources)
        
        # 3. å‡†ç¡®æ€§è¯„åˆ† (0-1)
        accuracy = self._evaluate_accuracy(answer, sources)
        
        # 4. æ¸…æ™°åº¦è¯„åˆ† (0-1)
        clarity = self._evaluate_clarity(answer)
        
        # æ€»ä½“è¯„åˆ†
        overall = (relevance + completeness + accuracy + clarity) / 4
        
        return {
            'relevance': relevance,
            'completeness': completeness,
            'accuracy': accuracy,
            'clarity': clarity,
            'overall': overall
        }
    
    def _evaluate_relevance(self, question: str, answer: str) -> float:
        """è¯„ä¼°ç›¸å…³æ€§"""
        prompt = f"""
        é—®é¢˜: {question}
        ç­”æ¡ˆ: {answer}
        
        è¯·è¯„ä¼°ç­”æ¡ˆä¸é—®é¢˜çš„ç›¸å…³æ€§(0-1ä¹‹é—´çš„åˆ†æ•°):
        - 1.0: å®Œå…¨ç›¸å…³ï¼Œç›´æ¥å›ç­”äº†é—®é¢˜
        - 0.5: éƒ¨åˆ†ç›¸å…³ï¼Œå›ç­”äº†éƒ¨åˆ†é—®é¢˜
        - 0.0: å®Œå…¨ä¸ç›¸å…³
        
        åªè¿”å›åˆ†æ•°ï¼Œä¸è¦è§£é‡Šã€‚
        """
        
        try:
            response = self.llm.complete(prompt)
            score = float(response.text.strip())
            return max(0.0, min(1.0, score))
        except:
            return 0.5  # é»˜è®¤åˆ†æ•°
    
    def _evaluate_completeness(self, answer: str, sources: List[Dict]) -> float:
        """è¯„ä¼°å®Œæ•´æ€§"""
        # ç®€å•å®ç°ï¼šæ£€æŸ¥ç­”æ¡ˆé•¿åº¦å’Œæ¥æºè¦†ç›–åº¦
        answer_length = len(answer)
        sources_used = len(sources)
        
        length_score = min(answer_length / 500, 1.0)  # 500å­—ç¬¦ä¸ºæ»¡åˆ†
        sources_score = min(sources_used / 3, 1.0)     # ä½¿ç”¨3ä¸ªæ¥æºä¸ºæ»¡åˆ†
        
        return (length_score + sources_score) / 2
    
    def _evaluate_accuracy(self, answer: str, sources: List[Dict]) -> float:
        """è¯„ä¼°å‡†ç¡®æ€§ï¼ˆé€šè¿‡æ¥æºä¸€è‡´æ€§ï¼‰"""
        if not sources:
            return 0.5
        
        # æ£€æŸ¥ç­”æ¡ˆä¸­æ˜¯å¦å¼•ç”¨äº†æ¥æºå†…å®¹
        matches = 0
        for source in sources:
            source_text = source.get('text', '')
            # ç®€å•çš„æ–‡æœ¬åŒ¹é…
            if any(chunk in answer for chunk in source_text.split()[:10]):
                matches += 1
        
        return matches / len(sources) if sources else 0.5
    
    def _evaluate_clarity(self, answer: str) -> float:
        """è¯„ä¼°æ¸…æ™°åº¦"""
        # ç®€å•æŒ‡æ ‡ï¼šå¥å­æ•°é‡ã€å¹³å‡å¥å­é•¿åº¦
        sentences = answer.split('ã€‚')
        num_sentences = len([s for s in sentences if s.strip()])
        
        if num_sentences == 0:
            return 0.0
        
        avg_length = len(answer) / num_sentences
        
        # ç†æƒ³å¥å­é•¿åº¦ï¼š20-50å­—
        if 20 <= avg_length <= 50:
            return 1.0
        elif avg_length < 20:
            return 0.7  # å¤ªçŸ­
        else:
            return max(0.3, 1.0 - (avg_length - 50) / 100)  # å¤ªé•¿
```

#### ä½¿ç”¨è´¨é‡è¯„ä¼°å™¨

```python
from src.agent import AcademicAgent
from custom_postprocessors.quality_evaluator import AnswerQualityEvaluator
from config.llm_config import get_llm

# åˆ›å»º Agent
agent = AcademicAgent()

# åˆ›å»ºè¯„ä¼°å™¨
llm = get_llm()
evaluator = AnswerQualityEvaluator(llm)

# æ‰§è¡ŒæŸ¥è¯¢
result = agent.query("ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ")

# è¯„ä¼°ç­”æ¡ˆè´¨é‡
quality = evaluator.evaluate(
    question="ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ",
    answer=result['answer'],
    sources=result.get('source_nodes', [])
)

print(f"ç­”æ¡ˆè´¨é‡è¯„ä¼°:")
print(f"  ç›¸å…³æ€§: {quality['relevance']:.2f}")
print(f"  å®Œæ•´æ€§: {quality['completeness']:.2f}")
print(f"  å‡†ç¡®æ€§: {quality['accuracy']:.2f}")
print(f"  æ¸…æ™°åº¦: {quality['clarity']:.2f}")
print(f"  æ€»ä½“è¯„åˆ†: {quality['overall']:.2f}")
```

---
        self,
        input_dir: str,
        recursive: bool = True,
        clean_text: bool = True,
        preserve_formatting: bool = False,
        supported_formats: List[str] = None
    ):
        """
        å‚æ•°:
            input_dir: æ–‡æ¡£ç›®å½•
            recursive: æ˜¯å¦é€’å½’æ‰«æå­ç›®å½•
            clean_text: æ˜¯å¦æ¸…ç†æ–‡æœ¬
            preserve_formatting: æ˜¯å¦ä¿ç•™æ ¼å¼
            supported_formats: æ”¯æŒçš„æ–‡ä»¶æ ¼å¼åˆ—è¡¨
        """
        pass
    
    def load_documents(self) -> List[Document]:
        """åŠ è½½æ‰€æœ‰æ–‡æ¡£"""
        pass
    
    def _load_single_file(self, file_path: str) -> List[Document]:
        """åŠ è½½å•ä¸ªæ–‡ä»¶"""
        pass
```

**ä½¿ç”¨ç¤ºä¾‹ï¼š**

```python
from src.loaders import DocumentLoader

# åŸºç¡€ä½¿ç”¨
loader = DocumentLoader(input_dir="./data/documents")
documents = loader.load_documents()

# é«˜çº§é…ç½®
loader = DocumentLoader(
    input_dir="./data/documents",
    recursive=True,
    clean_text=True,
    preserve_formatting=False,
    supported_formats=[".pdf", ".docx", ".md"]
)
documents = loader.load_documents()

# ç»Ÿè®¡ä¿¡æ¯
print(f"åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£å—")
for doc in documents:
    print(f"æ–‡ä»¶: {doc.metadata['file_name']}, é•¿åº¦: {len(doc.text)}")
```

**æ‰©å±•å¼€å‘ï¼šæ·»åŠ æ–°çš„æ–‡æ¡£æ ¼å¼**

```python
from src.loaders.base import BaseLoader
from typing import List
from llama_index.core import Document

class CustomLoader(BaseLoader):
    """è‡ªå®šä¹‰åŠ è½½å™¨ç¤ºä¾‹"""
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.supported_extensions = [".custom"]
    
    def load(self, file_path: str) -> List[Document]:
        """åŠ è½½è‡ªå®šä¹‰æ ¼å¼æ–‡ä»¶"""
        # å®ç°ä½ çš„åŠ è½½é€»è¾‘
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # åˆ›å»ºæ–‡æ¡£å¯¹è±¡
        doc = Document(
            text=content,
            metadata={
                'file_name': os.path.basename(file_path),
                'file_path': file_path,
                'format': 'custom'
            }
        )
        return [doc]
```

### 2. Vector Storeï¼ˆå‘é‡å­˜å‚¨ï¼‰

**ä½ç½®**ï¼š`src/retrieval/vector_store.py`

**èŒè´£**ï¼š
- æ–‡æ¡£å‘é‡åŒ–
- å‘é‡ç´¢å¼•æ„å»º
- è¯­ä¹‰æ£€ç´¢
- ç´¢å¼•æŒä¹…åŒ–

**æ ¸å¿ƒç±»ï¼š**

```python
class VectorStore:
    """å‘é‡å­˜å‚¨ç®¡ç†å™¨"""
    
    def __init__(
        self,
        persist_dir: str,
        embedding_model: str = "BAAI/bge-small-zh-v1.5",
        collection_name: str = "documents"
    ):
        """
        å‚æ•°:
            persist_dir: æŒä¹…åŒ–ç›®å½•
            embedding_model: Embedding æ¨¡å‹åç§°
            collection_name: é›†åˆåç§°
        """
        pass
    
    def build_index(
        self,
        documents: List[Document],
        chunk_size: int = 512,
        chunk_overlap: int = 50
    ) -> VectorStoreIndex:
        """æ„å»ºå‘é‡ç´¢å¼•"""
        pass
    
    def query(
        self,
        query_text: str,
        top_k: int = 5,
        similarity_threshold: float = 0.7
    ) -> List[NodeWithScore]:
        """æŸ¥è¯¢ç›¸ä¼¼æ–‡æ¡£"""
        pass
    
    def delete_index(self):
        """åˆ é™¤ç´¢å¼•"""
        pass
```

**ä½¿ç”¨ç¤ºä¾‹ï¼š**

```python
from src.retrieval import VectorStore
from src.loaders import DocumentLoader

# 1. åŠ è½½æ–‡æ¡£
loader = DocumentLoader(input_dir="./data/documents")
documents = loader.load_documents()

# 2. æ„å»ºç´¢å¼•
vector_store = VectorStore(
    persist_dir="./data/vector_store",
    embedding_model="BAAI/bge-small-zh-v1.5"
)
index = vector_store.build_index(
    documents=documents,
    chunk_size=512,
    chunk_overlap=50
)

# 3. æŸ¥è¯¢
results = vector_store.query(
    query_text="Transformer æ¨¡å‹çš„æ ¸å¿ƒåˆ›æ–°",
    top_k=5,
    similarity_threshold=0.7
)

# 4. å¤„ç†ç»“æœ
for result in results:
    print(f"ç›¸ä¼¼åº¦: {result.score}")
    print(f"å†…å®¹: {result.node.text[:100]}...")
    print(f"æ¥æº: {result.node.metadata['file_name']}")
```

### 3. RAG Agentï¼ˆé—®ç­”å¼•æ“ï¼‰

**ä½ç½®**ï¼š`src/agent/rag_agent.py`

**èŒè´£**ï¼š
- é—®é¢˜ç†è§£
- æ–‡æ¡£æ£€ç´¢
- ä¸Šä¸‹æ–‡æ„å»º
- ç­”æ¡ˆç”Ÿæˆ
- æ¥æºè¿½æº¯

**æ ¸å¿ƒç±»ï¼š**

```python
class RAGAgent:
    """RAG é—®ç­” Agent"""
    
    def __init__(
        self,
        vector_store: VectorStore,
        llm: BaseLLM,
        prompt_template: str = None
    ):
        """
        å‚æ•°:
            vector_store: å‘é‡å­˜å‚¨
            llm: è¯­è¨€æ¨¡å‹
            prompt_template: Prompt æ¨¡æ¿
        """
        pass
    
    def query(
        self,
        question: str,
        top_k: int = 5,
        return_sources: bool = True
    ) -> Dict[str, Any]:
        """
        RAG é—®ç­”
        
        è¿”å›:
            {
                'answer': str,              # ç”Ÿæˆçš„ç­”æ¡ˆ
                'sources': List[Dict],      # æ¥æºåˆ—è¡¨
                'confidence': float,        # ç½®ä¿¡åº¦
                'retrieval_time': float,    # æ£€ç´¢è€—æ—¶
                'generation_time': float    # ç”Ÿæˆè€—æ—¶
            }
        """
        pass
    
    def _build_context(self, retrieved_nodes: List[NodeWithScore]) -> str:
        """æ„å»ºä¸Šä¸‹æ–‡"""
        pass
    
    def _format_sources(self, nodes: List[NodeWithScore]) -> List[Dict]:
        """æ ¼å¼åŒ–æ¥æºä¿¡æ¯"""
        pass
```

**ä½¿ç”¨ç¤ºä¾‹ï¼š**

```python
from src.agent import RAGAgent
from src.retrieval import VectorStore
from src.llm import MoonshotLLM

# 1. åˆå§‹åŒ–ç»„ä»¶
vector_store = VectorStore(persist_dir="./data/vector_store")
llm = MoonshotLLM()

# 2. åˆ›å»º Agent
agent = RAGAgent(
    vector_store=vector_store,
    llm=llm,
    prompt_template="åŸºäºä»¥ä¸‹å†…å®¹å›ç­”é—®é¢˜:\n{context}\n\né—®é¢˜: {question}"
)

# 3. æé—®
result = agent.query(
    question="Transformer çš„æ ¸å¿ƒåˆ›æ–°æ˜¯ä»€ä¹ˆï¼Ÿ",
    top_k=5,
    return_sources=True
)

# 4. è·å–ç»“æœ
print(f"ç­”æ¡ˆ: {result['answer']}")
print(f"ç½®ä¿¡åº¦: {result['confidence']}")
print(f"æ£€ç´¢è€—æ—¶: {result['retrieval_time']:.2f}s")
print(f"ç”Ÿæˆè€—æ—¶: {result['generation_time']:.2f}s")

for i, source in enumerate(result['sources'], 1):
    print(f"\næ¥æº {i}:")
    print(f"  æ–‡ä»¶: {source['file_name']}")
    print(f"  ç›¸ä¼¼åº¦: {source['similarity']:.2f}")
    print(f"  å†…å®¹: {source['text'][:100]}...")
```

### 4. LLM Serviceï¼ˆè¯­è¨€æ¨¡å‹æœåŠ¡ï¼‰

**ä½ç½®**ï¼š`src/llm/`

**èŒè´£**ï¼š
- ç»Ÿä¸€ LLM æ¥å£
- å¤šæä¾›å•†æ”¯æŒ
- è¯·æ±‚ç®¡ç†
- é”™è¯¯å¤„ç†

**æ ¸å¿ƒæ¥å£ï¼š**

```python
from abc import ABC, abstractmethod
from typing import List, Dict, Any

class BaseLLM(ABC):
    """LLM åŸºç±»"""
    
    @abstractmethod
    def __init__(self, config: Dict[str, Any]):
        """åˆå§‹åŒ– LLM"""
        pass
    
    @abstractmethod
    def complete(
        self,
        prompt: str,
        temperature: float = 0.7,
        max_tokens: int = 1000,
        **kwargs
    ) -> str:
        """æ–‡æœ¬è¡¥å…¨"""
        pass
    
    @abstractmethod
    def chat(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: int = 1000,
        **kwargs
    ) -> str:
        """å¤šè½®å¯¹è¯"""
        pass
```

**å®ç°ç¤ºä¾‹ï¼š**

```python
from src.llm.base import BaseLLM
import requests

class CustomLLM(BaseLLM):
    """è‡ªå®šä¹‰ LLM å®ç°"""
    
    def __init__(self, config: Dict[str, Any]):
        self.api_key = config.get('api_key')
        self.api_base = config.get('api_base')
        self.model = config.get('model')
    
    def complete(
        self,
        prompt: str,
        temperature: float = 0.7,
        max_tokens: int = 1000,
        **kwargs
    ) -> str:
        """å®ç°æ–‡æœ¬è¡¥å…¨"""
        response = requests.post(
            f"{self.api_base}/completions",
            json={
                "model": self.model,
                "prompt": prompt,
                "temperature": temperature,
                "max_tokens": max_tokens
            },
            headers={"Authorization": f"Bearer {self.api_key}"}
        )
        return response.json()['choices'][0]['text']
    
    def chat(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: int = 1000,
        **kwargs
    ) -> str:
        """å®ç°å¤šè½®å¯¹è¯"""
        response = requests.post(
            f"{self.api_base}/chat/completions",
            json={
                "model": self.model,
                "messages": messages,
                "temperature": temperature,
                "max_tokens": max_tokens
            },
            headers={"Authorization": f"Bearer {self.api_key}"}
        )
        return response.json()['choices'][0]['message']['content']
```

---

## API å‚è€ƒ

### create_agent()

åˆ›å»º RAG Agent å®ä¾‹ã€‚

```python
def create_agent(
    documents_dir: str = "./data/documents",
    vector_store_dir: str = "./data/vector_store",
    force_rebuild: bool = False,
    chunk_size: int = 512,
    chunk_overlap: int = 50,
    embedding_model: str = "BAAI/bge-small-zh-v1.5",
    llm_provider: str = "moonshot",
    **kwargs
) -> RAGAgent:
    """
    åˆ›å»º RAG Agent
    
    å‚æ•°:
        documents_dir: æ–‡æ¡£ç›®å½•
        vector_store_dir: å‘é‡å­˜å‚¨ç›®å½•
        force_rebuild: æ˜¯å¦å¼ºåˆ¶é‡å»ºç´¢å¼•
        chunk_size: æ–‡æœ¬å—å¤§å°
        chunk_overlap: æ–‡æœ¬å—é‡å 
        embedding_model: Embedding æ¨¡å‹
        llm_provider: LLM æä¾›å•†
        **kwargs: å…¶ä»–å‚æ•°
    
    è¿”å›:
        RAGAgent å®ä¾‹
    
    å¼‚å¸¸:
        FileNotFoundError: æ–‡æ¡£ç›®å½•ä¸å­˜åœ¨
        ValueError: é…ç½®å‚æ•°æ— æ•ˆ
    """
```

**ä½¿ç”¨ç¤ºä¾‹ï¼š**

```python
from src.agent import create_agent

# åŸºç¡€ä½¿ç”¨
agent = create_agent()

# è‡ªå®šä¹‰é…ç½®
agent = create_agent(
    documents_dir="./my_papers",
    force_rebuild=True,
    chunk_size=256,
    llm_provider="openai"
)
```

### query()

æ‰§è¡Œ RAG æŸ¥è¯¢ã€‚

```python
def query(
    agent: RAGAgent,
    question: str,
    mode: str = "rag",
    web_search: bool = False,
    top_k: int = 5,
    temperature: float = 0.7,
    **kwargs
) -> Dict[str, Any]:
    """
    æ‰§è¡ŒæŸ¥è¯¢
    
    å‚æ•°:
        agent: RAG Agent å®ä¾‹
        question: ç”¨æˆ·é—®é¢˜
        mode: æŸ¥è¯¢æ¨¡å¼ ('rag' æˆ– 'llm')
        web_search: æ˜¯å¦å¯ç”¨è”ç½‘æœç´¢
        top_k: æ£€ç´¢æ•°é‡
        temperature: æ¸©åº¦å‚æ•°
        **kwargs: å…¶ä»–å‚æ•°
    
    è¿”å›:
        {
            'answer': str,
            'sources': List[Dict],
            'mode': str,
            'web_results': List[Dict]  # å¦‚æœå¯ç”¨è”ç½‘æœç´¢
        }
    """
```

---

## æ‰©å±•å¼€å‘

### 1. æ·»åŠ æ–°çš„ LLM æä¾›å•†

**æ­¥éª¤ï¼š**

1. åˆ›å»ºæ–°çš„ LLM ç±»ï¼š

```python
# src/llm/custom_llm.py
from src.llm.base import BaseLLM

class CustomLLM(BaseLLM):
    def __init__(self, config):
        self.api_key = config.get('api_key')
        # åˆå§‹åŒ–ä½ çš„ LLM å®¢æˆ·ç«¯
    
    def complete(self, prompt, **kwargs):
        # å®ç°è¡¥å…¨é€»è¾‘
        pass
    
    def chat(self, messages, **kwargs):
        # å®ç°å¯¹è¯é€»è¾‘
        pass
```

2. æ³¨å†Œåˆ°é…ç½®ï¼š

```python
# src/config/llm_config.py
LLM_PROVIDERS = {
    'openai': OpenAILLM,
    'moonshot': MoonshotLLM,
    'custom': CustomLLM,  # æ·»åŠ æ–°æä¾›å•†
}
```

3. æ›´æ–° .env.exampleï¼š

```bash
# æ·»åŠ æ–°çš„é…ç½®é¡¹
LLM_PROVIDER=custom
CUSTOM_API_KEY=your-key
CUSTOM_API_BASE=https://api.custom.com/v1
```

### 2. è‡ªå®šä¹‰æ£€ç´¢ç­–ç•¥

**ç¤ºä¾‹ï¼šå®ç°æ··åˆæ£€ç´¢ï¼ˆå…³é”®è¯ + è¯­ä¹‰ï¼‰**

```python
# src/retrieval/hybrid_retriever.py
from src.retrieval.vector_store import VectorStore
from typing import List
from llama_index.core import QueryBundle
from llama_index.core.schema import NodeWithScore

class HybridRetriever:
    """æ··åˆæ£€ç´¢å™¨ï¼šç»“åˆå…³é”®è¯å’Œè¯­ä¹‰æ£€ç´¢"""
    
    def __init__(
        self,
        vector_store: VectorStore,
        keyword_weight: float = 0.3,
        semantic_weight: float = 0.7
    ):
        self.vector_store = vector_store
        self.keyword_weight = keyword_weight
        self.semantic_weight = semantic_weight
    
    def retrieve(
        self,
        query: str,
        top_k: int = 5
    ) -> List[NodeWithScore]:
        """æ··åˆæ£€ç´¢"""
        # 1. è¯­ä¹‰æ£€ç´¢
        semantic_results = self.vector_store.query(query, top_k=top_k)
        
        # 2. å…³é”®è¯æ£€ç´¢
        keyword_results = self._keyword_search(query, top_k=top_k)
        
        # 3. èåˆç»“æœ
        merged_results = self._merge_results(
            semantic_results,
            keyword_results
        )
        
        return merged_results[:top_k]
    
    def _keyword_search(self, query: str, top_k: int) -> List[NodeWithScore]:
        """å…³é”®è¯æœç´¢å®ç°"""
        # å®ç° BM25 æˆ–å…¶ä»–å…³é”®è¯æ£€ç´¢ç®—æ³•
        pass
    
    def _merge_results(
        self,
        semantic: List[NodeWithScore],
        keyword: List[NodeWithScore]
    ) -> List[NodeWithScore]:
        """èåˆæ£€ç´¢ç»“æœ"""
        # å®ç° RRF (Reciprocal Rank Fusion) æˆ–å…¶ä»–èåˆç­–ç•¥
        pass
```

### 3. è‡ªå®šä¹‰ Prompt æ¨¡æ¿

**åˆ›å»º Prompt é…ç½®æ–‡ä»¶ï¼š**

```yaml
# config/prompts.yaml
rag_prompt: |
  ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡åˆ†æåŠ©æ‰‹ã€‚
  
  åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ï¼š
  {context}
  
  é—®é¢˜: {question}
  
  è¦æ±‚:
  1. ç­”æ¡ˆå¿…é¡»åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹
  2. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜
  3. å¼•ç”¨å…·ä½“çš„æ®µè½æ”¯æŒä½ çš„ç­”æ¡ˆ
  
  ç­”æ¡ˆ:

analysis_prompt: |
  è¯·æ·±åº¦åˆ†æä»¥ä¸‹è®ºæ–‡å†…å®¹ï¼š
  {context}
  
  åˆ†æç»´åº¦ï¼š
  1. ç ”ç©¶é—®é¢˜å’ŒåŠ¨æœº
  2. æŠ€æœ¯æ–¹æ³•
  3. å®éªŒè®¾è®¡
  4. ä¸»è¦ç»“è®º
  5. åˆ›æ–°ç‚¹å’Œå±€é™æ€§
  
  åˆ†æç»“æœ:
```

**ä½¿ç”¨è‡ªå®šä¹‰ Promptï¼š**

```python
from src.utils import load_prompts

# åŠ è½½ Prompt æ¨¡æ¿
prompts = load_prompts("config/prompts.yaml")

# åˆ›å»º Agent æ—¶æŒ‡å®š
agent = RAGAgent(
    vector_store=vector_store,
    llm=llm,
    prompt_template=prompts['rag_prompt']
)
```

### 4. æ·»åŠ åå¤„ç†å™¨

**ç¤ºä¾‹ï¼šç­”æ¡ˆè´¨é‡è¯„ä¼°**

```python
# src/utils/postprocessor.py
class AnswerQualityEvaluator:
    """ç­”æ¡ˆè´¨é‡è¯„ä¼°å™¨"""
    
    def __init__(self, llm):
        self.llm = llm
    
    def evaluate(self, question: str, answer: str, sources: List[Dict]) -> Dict:
        """è¯„ä¼°ç­”æ¡ˆè´¨é‡"""
        # 1. ç›¸å…³æ€§è¯„åˆ†
        relevance_score = self._evaluate_relevance(question, answer)
        
        # 2. å®Œæ•´æ€§è¯„åˆ†
        completeness_score = self._evaluate_completeness(answer, sources)
        
        # 3. å‡†ç¡®æ€§è¯„åˆ†
        accuracy_score = self._evaluate_accuracy(answer, sources)
        
        return {
            'relevance': relevance_score,
            'completeness': completeness_score,
            'accuracy': accuracy_score,
            'overall': (relevance_score + completeness_score + accuracy_score) / 3
        }
    
    def _evaluate_relevance(self, question: str, answer: str) -> float:
        """è¯„ä¼°ç›¸å…³æ€§"""
        prompt = f"é—®é¢˜: {question}\nç­”æ¡ˆ: {answer}\n\nè¯·è¯„ä¼°ç­”æ¡ˆä¸é—®é¢˜çš„ç›¸å…³æ€§(0-1):"
        # ä½¿ç”¨ LLM è¯„ä¼°
        pass
```

---

## æµ‹è¯•æŒ‡å—

### å•å…ƒæµ‹è¯•

**ä½ç½®**ï¼š`tests/unit/`

**è¿è¡Œæµ‹è¯•ï¼š**

```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
pytest tests/

# è¿è¡Œç‰¹å®šæµ‹è¯•
pytest tests/unit/test_document_loader.py

# è¿è¡Œå¹¶æŸ¥çœ‹è¦†ç›–ç‡
pytest --cov=src tests/

# ç”ŸæˆHTMLæŠ¥å‘Š
pytest --cov=src --cov-report=html tests/
```

**æµ‹è¯•ç¤ºä¾‹ï¼š**

```python
# tests/unit/test_document_loader.py
import pytest
from src.loaders import DocumentLoader

class TestDocumentLoader:
    """DocumentLoader å•å…ƒæµ‹è¯•"""
    
    @pytest.fixture
    def loader(self):
        """æµ‹è¯•fixture"""
        return DocumentLoader(
            input_dir="tests/fixtures/documents",
            recursive=True
        )
    
    def test_load_pdf(self, loader):
        """æµ‹è¯• PDF åŠ è½½"""
        documents = loader.load_documents()
        assert len(documents) > 0
        assert any(doc.metadata['format'] == 'pdf' for doc in documents)
    
    def test_metadata(self, loader):
        """æµ‹è¯•å…ƒæ•°æ®"""
        documents = loader.load_documents()
        for doc in documents:
            assert 'file_name' in doc.metadata
            assert 'file_path' in doc.metadata
            assert 'format' in doc.metadata
```

### é›†æˆæµ‹è¯•

**ä½ç½®**ï¼š`tests/integration/`

```python
# tests/integration/test_rag_pipeline.py
import pytest
from src.agent import create_agent

class TestRAGPipeline:
    """RAG ç®¡é“é›†æˆæµ‹è¯•"""
    
    @pytest.fixture(scope="class")
    def agent(self):
        """åˆ›å»ºæµ‹è¯• Agent"""
        return create_agent(
            documents_dir="tests/fixtures/documents",
            force_rebuild=True
        )
    
    def test_end_to_end_query(self, agent):
        """ç«¯åˆ°ç«¯æŸ¥è¯¢æµ‹è¯•"""
        result = agent.query(
            question="æµ‹è¯•é—®é¢˜",
            top_k=3
        )
        
        assert 'answer' in result
        assert 'sources' in result
        assert len(result['sources']) <= 3
    
    def test_query_performance(self, agent):
        """æ€§èƒ½æµ‹è¯•"""
        import time
        
        start = time.time()
        result = agent.query("æµ‹è¯•é—®é¢˜")
        elapsed = time.time() - start
        
        assert elapsed < 10  # 10ç§’å†…å®Œæˆ
```

---

## éƒ¨ç½²æŒ‡å—

### Docker éƒ¨ç½²

**åˆ›å»º Dockerfileï¼š**

```dockerfile
FROM python:3.9-slim

WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .

# åˆ›å»ºæ•°æ®ç›®å½•
RUN mkdir -p /app/data/documents /app/data/vector_store /app/logs

# æš´éœ²ç«¯å£
EXPOSE 7860

# å¯åŠ¨åº”ç”¨
CMD ["python", "web_ui.py", "--server-port", "7860", "--server-name", "0.0.0.0"]
```

**åˆ›å»º docker-compose.ymlï¼š**

```yaml
version: '3.8'

services:
  app:
    build: .
    ports:
      - "7860:7860"
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
    environment:
      - LLM_API_KEY=${LLM_API_KEY}
      - LLM_API_BASE=${LLM_API_BASE}
      - LLM_MODEL=${LLM_MODEL}
    restart: unless-stopped
```

**éƒ¨ç½²å‘½ä»¤ï¼š**

```bash
# æ„å»ºé•œåƒ
docker-compose build

# å¯åŠ¨æœåŠ¡
docker-compose up -d

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f

# åœæ­¢æœåŠ¡
docker-compose down
```

---

## ä»£ç è§„èŒƒ

### Python ä»£ç è§„èŒƒ

**éµå¾ª PEP 8 æ ‡å‡†ï¼š**

1. **Import ç»„ç»‡**
   ```python
   # æ ‡å‡†åº“
   import os
   from pathlib import Path
   from typing import Optional, List, Dict, Any
   
   # ç¬¬ä¸‰æ–¹åº“
   from llama_index.core import VectorStoreIndex
   from loguru import logger
   
   # æœ¬åœ°æ¨¡å—
   from config import SystemConfig
   from src.loaders import DocumentLoader
   ```

2. **å‘½åè§„èŒƒ**
   - ç±»åï¼š`PascalCase` (ä¾‹å¦‚ï¼š`DocumentLoader`)
   - å‡½æ•°åï¼š`snake_case` (ä¾‹å¦‚ï¼š`load_documents`)
   - å¸¸é‡ï¼š`UPPER_CASE` (ä¾‹å¦‚ï¼š`DEFAULT_CHUNK_SIZE`)
   - ç§æœ‰æ–¹æ³•ï¼š`_private_method`

3. **ç±»å‹æ³¨è§£**
   ```python
   def query(
       self,
       question: str,
       top_k: int = 5,
       enable_web_search: bool = False
   ) -> Dict[str, Any]:
       """å¸¦å®Œæ•´ç±»å‹æ³¨è§£çš„å‡½æ•°"""
       pass
   ```

4. **æ–‡æ¡£å­—ç¬¦ä¸²ï¼ˆGoogle é£æ ¼ï¼‰**
   ```python
   def build_index(self, documents: List[Document]) -> VectorStoreIndex:
       """æ„å»ºå‘é‡ç´¢å¼•
       
       Args:
           documents: æ–‡æ¡£åˆ—è¡¨
           
       Returns:
           VectorStoreIndex: æ„å»ºçš„ç´¢å¼•å¯¹è±¡
           
       Raises:
           ValueError: å½“æ–‡æ¡£åˆ—è¡¨ä¸ºç©ºæ—¶
       """
       pass
   ```

### ä»£ç è´¨é‡æ£€æŸ¥

```bash
# æ ¼å¼åŒ–ä»£ç 
black src/ tests/

# æ’åº imports
isort src/ tests/

# ä»£ç æ£€æŸ¥
flake8 src/ tests/

# ç±»å‹æ£€æŸ¥ï¼ˆå¯é€‰ï¼‰
mypy src/
```

### Git æäº¤è§„èŒƒ

**æäº¤æ¶ˆæ¯æ ¼å¼ï¼š**
```
<type>(<scope>): <subject>

<body>

<footer>
```

**ç±»å‹ï¼ˆtypeï¼‰ï¼š**
- `feat`: æ–°åŠŸèƒ½
- `fix`: ä¿®å¤ Bug
- `docs`: æ–‡æ¡£æ›´æ–°
- `style`: ä»£ç æ ¼å¼è°ƒæ•´
- `refactor`: é‡æ„ä»£ç 
- `test`: æµ‹è¯•ç›¸å…³
- `chore`: æ„å»º/å·¥å…·ç›¸å…³

**ç¤ºä¾‹ï¼š**
```
feat(agent): æ·»åŠ å¤šè½®å¯¹è¯å†å²ç®¡ç†åŠŸèƒ½

- å®ç°å¯¹è¯å†å²å­˜å‚¨
- æ·»åŠ å†å²æ¸…ç©ºåŠŸèƒ½
- æ”¯æŒä¸Šä¸‹æ–‡è®°å¿†

Closes #123
```

---

## è´¡çŒ®æŒ‡å—

### å¼€å‘æµç¨‹

1. **Fork é¡¹ç›®**
   ```bash
   # åœ¨ GitHub ä¸Š Fork é¡¹ç›®
   git clone https://github.com/your-username/academic-paper-qa.git
   cd academic-paper-qa
   ```

2. **åˆ›å»ºåŠŸèƒ½åˆ†æ”¯**
   ```bash
   git checkout -b feature/your-feature-name
   ```

3. **å¼€å‘å’Œæµ‹è¯•**
   ```bash
   # ç¼–å†™ä»£ç 
   vim src/your_module.py
   
   # ç¼–å†™æµ‹è¯•
   vim tests/test_your_module.py
   
   # è¿è¡Œæµ‹è¯•
   pytest tests/test_your_module.py
   ```

4. **æäº¤ä»£ç **
   ```bash
   git add .
   git commit -m "feat: add your feature description"
   ```

5. **æ¨é€å¹¶åˆ›å»º PR**
   ```bash
   git push origin feature/your-feature-name
   # åœ¨ GitHub ä¸Šåˆ›å»º Pull Request
   ```

### Pull Request æ£€æŸ¥æ¸…å•

- [ ] ä»£ç éµå¾ªé¡¹ç›®è§„èŒƒ
- [ ] æ·»åŠ äº†å¿…è¦çš„æµ‹è¯•
- [ ] æµ‹è¯•å…¨éƒ¨é€šè¿‡
- [ ] æ›´æ–°äº†ç›¸å…³æ–‡æ¡£
- [ ] æäº¤æ¶ˆæ¯æ¸…æ™°æ˜ç¡®
- [ ] ä»£ç æ²¡æœ‰å¼•å…¥æ–°çš„è­¦å‘Š

---

## äºŒæ¬¡å¼€å‘å®è·µæ¡ˆä¾‹

### æ¡ˆä¾‹ 1: æ„å»ºå­¦ç§‘ä¸“ç”¨é—®ç­”ç³»ç»Ÿ

**éœ€æ±‚**: ä¸ºç‰¹å®šå­¦ç§‘ï¼ˆå¦‚ç”Ÿç‰©åŒ»å­¦ï¼‰æ„å»ºä¸“ç”¨é—®ç­”ç³»ç»Ÿ

```python
# biology_qa_system.py
from src.agent import AcademicAgent
from typing import Dict, Any

class BiologyQASystem(AcademicAgent):
    """ç”Ÿç‰©åŒ»å­¦ä¸“ç”¨é—®ç­”ç³»ç»Ÿ"""
    
    def __init__(self, **kwargs):
        super().__init__(
            documents_dir="./data/biology_papers",
            **kwargs
        )
        
        # å­¦ç§‘ä¸“ç”¨æœ¯è¯­åº“
        self.terminology = self._load_terminology()
    
    def _load_terminology(self) -> Dict[str, str]:
        """åŠ è½½å­¦ç§‘æœ¯è¯­"""
        return {
            "PCR": "Polymerase Chain Reaction (èšåˆé…¶é“¾å¼ååº”)",
            "CRISPR": "Clustered Regularly Interspaced Short Palindromic Repeats",
            # ... æ›´å¤šæœ¯è¯­
        }
    
    def query(self, question: str, **kwargs) -> Dict[str, Any]:
        """å¢å¼ºæŸ¥è¯¢ï¼šæ·»åŠ æœ¯è¯­è§£é‡Š"""
        
        # æ£€æµ‹å¹¶è§£é‡Šä¸“ä¸šæœ¯è¯­
        detected_terms = self._detect_terms(question)
        
        # æ‰§è¡Œæ ‡å‡†æŸ¥è¯¢
        result = super().query(question, **kwargs)
        
        # åœ¨ç­”æ¡ˆä¸­æ·»åŠ æœ¯è¯­è§£é‡Š
        if detected_terms:
            term_explanations = "\n\n**æœ¯è¯­è§£é‡Š:**\n"
            for term in detected_terms:
                term_explanations += f"- {term}: {self.terminology[term]}\n"
            result['answer'] += term_explanations
        
        return result
    
    def _detect_terms(self, text: str) -> list:
        """æ£€æµ‹æ–‡æœ¬ä¸­çš„ä¸“ä¸šæœ¯è¯­"""
        return [term for term in self.terminology if term in text.upper()]

# ä½¿ç”¨ç¤ºä¾‹
bio_qa = BiologyQASystem()
result = bio_qa.query("PCR æŠ€æœ¯çš„åŸç†æ˜¯ä»€ä¹ˆï¼Ÿ")
print(result['answer'])
```

### æ¡ˆä¾‹ 2: æ·»åŠ æ‰¹é‡å¤„ç†åŠŸèƒ½

**éœ€æ±‚**: æ‰¹é‡å¤„ç†å¤šä¸ªé—®é¢˜ï¼Œç”ŸæˆæŠ¥å‘Š

```python
# batch_processor.py
from src.agent import AcademicAgent
from typing import List, Dict
import pandas as pd
from datetime import datetime

class BatchProcessor:
    """æ‰¹é‡é—®ç­”å¤„ç†å™¨"""
    
    def __init__(self):
        self.agent = AcademicAgent()
    
    def process_questions(
        self,
        questions: List[str],
        output_file: str = "qa_results.xlsx"
    ) -> pd.DataFrame:
        """æ‰¹é‡å¤„ç†é—®é¢˜"""
        
        results = []
        
        for i, question in enumerate(questions, 1):
            print(f"å¤„ç†é—®é¢˜ {i}/{len(questions)}: {question[:50]}...")
            
            try:
                result = self.agent.query(question)
                
                results.append({
                    'question': question,
                    'answer': result['answer'],
                    'sources_count': len(result.get('source_nodes', [])),
                    'status': 'success',
                    'timestamp': datetime.now().isoformat()
                })
            except Exception as e:
                results.append({
                    'question': question,
                    'answer': '',
                    'sources_count': 0,
                    'status': f'error: {str(e)}',
                    'timestamp': datetime.now().isoformat()
                })
        
        # è½¬æ¢ä¸º DataFrame
        df = pd.DataFrame(results)
        
        # å¯¼å‡ºåˆ° Excel
        df.to_excel(output_file, index=False)
        print(f"\nç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        return df

# ä½¿ç”¨ç¤ºä¾‹
processor = BatchProcessor()

questions = [
    "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ",
    "Transformer çš„æ ¸å¿ƒåˆ›æ–°æ˜¯ä»€ä¹ˆï¼Ÿ",
    "å¦‚ä½•è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹ï¼Ÿ"
]

df = processor.process_questions(questions)
print(f"\nå¤„ç†å®Œæˆï¼ŒæˆåŠŸ: {len(df[df.status=='success'])} æ¡")
```

### æ¡ˆä¾‹ 3: æ·»åŠ å¼•ç”¨ç”Ÿæˆå™¨

**éœ€æ±‚**: è‡ªåŠ¨ç”Ÿæˆå­¦æœ¯å¼•ç”¨æ ¼å¼

```python
# citation_generator.py
from src.agent import AcademicAgent
from typing import Dict, List
import re

class CitationGenerator:
    """å­¦æœ¯å¼•ç”¨ç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.agent = AcademicAgent()
    
    def query_with_citations(
        self,
        question: str,
        citation_style: str = "apa"
    ) -> Dict:
        """å¸¦å¼•ç”¨çš„æŸ¥è¯¢"""
        
        # æ‰§è¡ŒæŸ¥è¯¢
        result = self.agent.query(question)
        
        # ç”Ÿæˆå¼•ç”¨
        citations = self._generate_citations(
            result.get('source_nodes', []),
            style=citation_style
        )
        
        # åœ¨ç­”æ¡ˆä¸­æ·»åŠ å¼•ç”¨æ ‡è®°
        answer_with_citations = self._add_citation_marks(
            result['answer'],
            citations
        )
        
        return {
            'answer': answer_with_citations,
            'citations': citations,
            'original_answer': result['answer']
        }
    
    def _generate_citations(
        self,
        sources: List,
        style: str = "apa"
    ) -> List[str]:
        """ç”Ÿæˆå¼•ç”¨åˆ—è¡¨"""
        citations = []
        
        for i, source in enumerate(sources, 1):
            metadata = source.node.metadata
            
            if style == "apa":
                # APA æ ¼å¼
                citation = f"[{i}] {metadata.get('file_name', 'Unknown')}. Retrieved from {metadata.get('file_path', 'Unknown path')}."
            elif style == "mla":
                # MLA æ ¼å¼
                citation = f"[{i}] {metadata.get('file_name', 'Unknown')}."
            else:
                citation = f"[{i}] {metadata.get('file_name', 'Unknown')}"
            
            citations.append(citation)
        
        return citations
    
    def _add_citation_marks(self, answer: str, citations: List[str]) -> str:
        """åœ¨ç­”æ¡ˆä¸­æ·»åŠ å¼•ç”¨æ ‡è®°"""
        # ç®€å•å®ç°ï¼šåœ¨ç­”æ¡ˆæœ«å°¾æ·»åŠ å¼•ç”¨åˆ—è¡¨
        if citations:
            answer += "\n\n**å‚è€ƒæ–‡çŒ®:**\n"
            for citation in citations:
                answer += f"{citation}\n"
        
        return answer

# ä½¿ç”¨ç¤ºä¾‹
cit_gen = CitationGenerator()
result = cit_gen.query_with_citations(
    "ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æœºåˆ¶ï¼Ÿ",
    citation_style="apa"
)
print(result['answer'])
```

### æ¡ˆä¾‹ 4: æ„å»º API æœåŠ¡

**éœ€æ±‚**: å°†ç³»ç»Ÿå°è£…ä¸º REST API

```python
# api_server.py
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import Optional, List
from src.agent import AcademicAgent
import uvicorn

app = FastAPI(title="Academic QA API")
agent = AcademicAgent()

class QueryRequest(BaseModel):
    question: str
    mode: str = "rag"
    enable_web_search: bool = False
    top_k: int = 5
    document_paths: Optional[List[str]] = None

class QueryResponse(BaseModel):
    answer: str
    sources_count: int
    mode: str

@app.post("/query", response_model=QueryResponse)
async def query(request: QueryRequest):
    """æŸ¥è¯¢æ¥å£"""
    try:
        if request.mode == "rag":
            result = agent.query(
                question=request.question,
                enable_web_search=request.enable_web_search,
                top_k=request.top_k
            )
        else:
            result = agent.query_direct(
                question=request.question,
                document_paths=request.document_paths,
                enable_web_search=request.enable_web_search
            )
        
        return QueryResponse(
            answer=result['answer'],
            sources_count=len(result.get('source_nodes', [])),
            mode=request.mode
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/stats")
async def get_statistics():
    """è·å–ç»Ÿè®¡ä¿¡æ¯"""
    return agent.get_statistics()

@app.post("/rebuild_index")
async def rebuild_index():
    """é‡å»ºç´¢å¼•"""
    try:
        agent.rebuild_index()
        return {"status": "success", "message": "ç´¢å¼•é‡å»ºå®Œæˆ"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# å¯åŠ¨æœåŠ¡
if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)

# ä½¿ç”¨ç¤ºä¾‹ï¼ˆå®¢æˆ·ç«¯ï¼‰
"""
import requests

# æŸ¥è¯¢
response = requests.post(
    "http://localhost:8000/query",
    json={
        "question": "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ",
        "mode": "rag",
        "top_k": 3
    }
)
print(response.json())

# è·å–ç»Ÿè®¡
stats = requests.get("http://localhost:8000/stats")
print(stats.json())
"""
```

### æ¡ˆä¾‹ 5: æ·»åŠ ç¼“å­˜å±‚

**éœ€æ±‚**: ä¸ºé¢‘ç¹æŸ¥è¯¢æ·»åŠ ç¼“å­˜ï¼Œæå‡æ€§èƒ½

```python
# cached_agent.py
from src.agent import AcademicAgent
from typing import Dict, Any
import hashlib
import json
import redis
from functools import wraps

class CachedAgent(AcademicAgent):
    """å¸¦ç¼“å­˜çš„ Agent"""
    
    def __init__(self, redis_host: str = "localhost", redis_port: int = 6379, **kwargs):
        super().__init__(**kwargs)
        
        # è¿æ¥ Redis
        self.cache = redis.Redis(
            host=redis_host,
            port=redis_port,
            decode_responses=True
        )
        
        # ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰
        self.cache_ttl = 3600  # 1å°æ—¶
    
    def _get_cache_key(self, question: str, **kwargs) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        # å°†é—®é¢˜å’Œå‚æ•°ç»„åˆæˆå”¯ä¸€é”®
        key_data = {
            'question': question,
            **kwargs
        }
        key_str = json.dumps(key_data, sort_keys=True)
        return hashlib.md5(key_str.encode()).hexdigest()
    
    def query(self, question: str, **kwargs) -> Dict[str, Any]:
        """å¸¦ç¼“å­˜çš„æŸ¥è¯¢"""
        
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = f"query:{self._get_cache_key(question, **kwargs)}"
        
        # å°è¯•ä»ç¼“å­˜è·å–
        cached_result = self.cache.get(cache_key)
        if cached_result:
            print(f"âœ“ ç¼“å­˜å‘½ä¸­: {cache_key}")
            return json.loads(cached_result)
        
        # ç¼“å­˜æœªå‘½ä¸­ï¼Œæ‰§è¡ŒæŸ¥è¯¢
        print(f"âœ— ç¼“å­˜æœªå‘½ä¸­ï¼Œæ‰§è¡ŒæŸ¥è¯¢...")
        result = super().query(question, **kwargs)
        
        # å­˜å…¥ç¼“å­˜ï¼ˆä¸ç¼“å­˜ source_nodesï¼Œå¤ªå¤§ï¼‰
        cacheable_result = {
            'answer': result['answer'],
            'sources_count': len(result.get('source_nodes', [])),
            'cached': False
        }
        
        self.cache.setex(
            cache_key,
            self.cache_ttl,
            json.dumps(cacheable_result)
        )
        
        return result
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        pattern = "query:*"
        keys = self.cache.keys(pattern)
        if keys:
            self.cache.delete(*keys)
            print(f"å·²æ¸…ç©º {len(keys)} ä¸ªç¼“å­˜é¡¹")

# ä½¿ç”¨ç¤ºä¾‹
agent = CachedAgent()

# ç¬¬ä¸€æ¬¡æŸ¥è¯¢ï¼ˆæ— ç¼“å­˜ï¼‰
result1 = agent.query("ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ")  # éœ€è¦å‡ ç§’

# ç¬¬äºŒæ¬¡ç›¸åŒæŸ¥è¯¢ï¼ˆæœ‰ç¼“å­˜ï¼‰
result2 = agent.query("ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ")  # æ¯«ç§’çº§å“åº”

# æ¸…ç©ºç¼“å­˜
agent.clear_cache()
```

---

## å¸¸è§å¼€å‘ä»»åŠ¡

### æ·»åŠ æ–°çš„æ–‡æ¡£æ ¼å¼æ”¯æŒ

**æ­¥éª¤**:
1. åœ¨ `src/loaders/` åˆ›å»ºæ–°çš„åŠ è½½å™¨ç±»
2. å®ç°æ–‡æ¡£è¯»å–å’Œæ–‡æœ¬æå–é€»è¾‘
3. åœ¨ `DocumentLoader` ä¸­æ³¨å†Œæ–°æ ¼å¼
4. æ·»åŠ æµ‹è¯•ç”¨ä¾‹éªŒè¯åŠŸèƒ½
5. æ›´æ–°æ–‡æ¡£è¯´æ˜æ”¯æŒçš„æ ¼å¼

**ç¤ºä¾‹**: å‚è§ [æ‰©å±•å¼€å‘ - æ·»åŠ æ–°çš„æ–‡æ¡£æ ¼å¼æ”¯æŒ](#1-æ·»åŠ æ–°çš„æ–‡æ¡£æ ¼å¼æ”¯æŒ)

### ä¼˜åŒ–æ£€ç´¢æ€§èƒ½

**ç­–ç•¥**:
1. **è°ƒæ•´åˆ†å—å‚æ•°**: 
   - å¢å¤§ `chunk_size` å¯ä»¥ä¿ç•™æ›´å¤šä¸Šä¸‹æ–‡ï¼Œä½†ä¼šé™ä½æ£€ç´¢ç²¾åº¦
   - å¢å¤§ `chunk_overlap` å¯ä»¥é¿å…å…³é”®ä¿¡æ¯è¢«åˆ‡æ–­
   
2. **å°è¯•ä¸åŒ Embedding æ¨¡å‹**:
   - `BAAI/bge-small-zh-v1.5`: è½»é‡çº§ï¼Œé€Ÿåº¦å¿«
   - `BAAI/bge-large-zh`: æ•ˆæœæ›´å¥½ï¼Œä½†æ›´æ…¢
   - `text-embedding-3-small`: OpenAI Embedding
   
3. **å®ç°æ··åˆæ£€ç´¢**:
   - ç»“åˆå…³é”®è¯æ£€ç´¢ï¼ˆBM25ï¼‰å’Œè¯­ä¹‰æ£€ç´¢
   - å‚è§ [æ‰©å±•å¼€å‘ - è‡ªå®šä¹‰æ£€ç´¢ç­–ç•¥](#3-è‡ªå®šä¹‰æ£€ç´¢ç­–ç•¥)
   
4. **æ·»åŠ ç¼“å­˜æœºåˆ¶**:
   - ç¼“å­˜é¢‘ç¹æŸ¥è¯¢çš„ç»“æœ
   - ä½¿ç”¨ Redis æˆ–å†…å­˜ç¼“å­˜
   - å‚è§ [æ¡ˆä¾‹ 5 - æ·»åŠ ç¼“å­˜å±‚](#æ¡ˆä¾‹-5-æ·»åŠ ç¼“å­˜å±‚)

### é›†æˆæ–°çš„ LLM æä¾›å•†

**æ­¥éª¤**:
1. åœ¨ `config/llm_config.py` ä¸­æ·»åŠ æ–°çš„ LLM ç±»
2. å®ç°å¿…è¦çš„æ¥å£æ–¹æ³•ï¼ˆ`complete`, `chat`ï¼‰
3. æ·»åŠ ç¯å¢ƒå˜é‡é…ç½®
4. æ›´æ–° `.env.example` ç¤ºä¾‹
5. ç¼–å†™ä½¿ç”¨æ–‡æ¡£å’Œç¤ºä¾‹

**ç¤ºä¾‹**: å‚è§ [æ‰©å±•å¼€å‘ - æ·»åŠ æ–°çš„ LLM æä¾›å•†](#2-æ·»åŠ æ–°çš„-llm-æä¾›å•†)

### æ·»åŠ æ–°çš„è¾“å‡ºæ ¼å¼

**ç¤ºä¾‹**: å¯¼å‡ºä¸º Markdown æŠ¥å‘Š

```python
def export_to_markdown(
    question: str,
    result: Dict[str, Any],
    output_file: str
):
    """å¯¼å‡ºæŸ¥è¯¢ç»“æœä¸º Markdown æŠ¥å‘Š"""
    
    with open(output_file, 'w', encoding='utf-8') as f:
        # æ ‡é¢˜
        f.write(f"# æŸ¥è¯¢æŠ¥å‘Š\n\n")
        f.write(f"**é—®é¢˜**: {question}\n\n")
        f.write(f"**æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        # ç­”æ¡ˆ
        f.write(f"## ç­”æ¡ˆ\n\n")
        f.write(f"{result['answer']}\n\n")
        
        # æ¥æº
        if result.get('source_nodes'):
            f.write(f"## å‚è€ƒæ¥æº\n\n")
            for i, node in enumerate(result['source_nodes'], 1):
                f.write(f"### æ¥æº {i}\n\n")
                f.write(f"- **æ–‡ä»¶**: {node.node.metadata.get('file_name')}\n")
                f.write(f"- **ç›¸ä¼¼åº¦**: {node.score:.2f}\n")
                f.write(f"- **å†…å®¹**:\n\n```\n{node.node.text[:200]}...\n```\n\n")
```

---

## ä¸‹ä¸€æ­¥

- ğŸ“‹ æŸ¥çœ‹ [åŠŸèƒ½ä»‹ç»](FEATURES.md) äº†è§£ç³»ç»Ÿèƒ½åŠ›
- ğŸ“– é˜…è¯» [ä½¿ç”¨æŒ‡å—](USER_GUIDE.md) å¼€å§‹ä½¿ç”¨
- ğŸš€ å¼€å§‹å¼€å‘ä½ çš„æ‰©å±•åŠŸèƒ½
- ğŸ’¬ åŠ å…¥è®¨è®ºè·å–å¸®åŠ©

---

**æ›´æ–°æ—¥æœŸ**: 2025-12-20  
**ç‰ˆæœ¬**: v2.0  
**ç»´æŠ¤è€…**: Academic QA Team
