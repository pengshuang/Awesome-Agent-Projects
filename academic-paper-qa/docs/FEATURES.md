# 📋 项目功能介绍

## 目录
- [核心功能](#核心功能)
- [问答模式](#问答模式)
- [界面选项](#界面选项)
- [多轮对话 vs 单轮问答](#多轮对话-vs-单轮问答)
- [应用场景](#应用场景)

---

## 核心功能

### 1. 💬 多轮对话系统
支持上下文记忆和连续追问，实现自然流畅的对话体验。

**功能特性：**
- 🧠 **上下文记忆**：自动记住最近对话历史
- 🔗 **指代消解**：理解"它"、"这个"等代词含义
- 💭 **连续追问**：深入讨论某个话题
- 🧹 **历史管理**：查看和清除对话历史
- 📊 **轮数控制**：灵活控制保留的历史轮数（1-50+轮可调）

#### 📊 历史轮数控制（新功能）

为避免 Token 消耗过大，系统支持灵活控制历史对话轮数：

**控制方式：**

1. **初始化设置**
```python
# 只保留最近5轮
agent = AcademicAgent(max_history_turns=5)

# 保留50轮（长对话场景）
agent = AcademicAgent(max_history_turns=50)
```

2. **动态调整**
```python
# 运行时修改
agent.set_max_history_turns(20)

# 查看当前状态
info = agent.get_chat_history_info()
print(f"当前: {info['current_turns']}/{info['max_turns']} 轮")
```

3. **环境变量**
```bash
# .env 文件
MAX_HISTORY_TURNS=50
```

4. **Web UI 控制**
- 在界面右侧「对话历史控制」区域
- 使用滑块调整轮数（1-50）
- 实时查看历史状态

**使用场景：**

| 场景 | 推荐轮数 | 说明 |
|------|----------|------|
| 快速问答 | 1-5轮 | 节省Token，响应快 |
| 一般对话 | 5-10轮 | 默认配置，平衡性能 |
| 深度讨论 | 20-30轮 | 学术讨论，保留上下文 |
| 长期对话 | 50+轮 | 完整记忆，适合特殊场景 |

**智能管理：**
- 超出限制时自动删除最早的对话
- 支持查看当前轮数和剩余空间
- 可随时清空历史重新开始

**对话示例：**
```
👤: 什么是 Transformer？
🤖: Transformer 是一种基于注意力机制的神经网络架构...

👤: 它有哪些应用？              # ← 自动理解"它"指 Transformer
🤖: Transformer 主要应用于 NLP、CV 等领域...

👤: 在 NLP 中的具体应用？       # ← 结合上下文继续深入
🤖: 在自然语言处理中，Transformer 用于机器翻译、文本生成...
```

### 2. 🧠 智能问答系统
基于 RAG（检索增强生成）技术的学术论文问答。

**能做什么：**
- 📖 **快速理解**：快速获取论文核心观点和主要贡献
- 🔍 **深度挖掘**：提取技术细节、实验设置、数据分析
- 📊 **对比分析**：跨文档比较多篇论文的方法和结论
- 💡 **灵感启发**：发现研究空白和未来工作方向

**支持的问题类型：**
```
✅ 事实性问题：    "这篇论文的主要贡献是什么？"
✅ 分析性问题：    "论文使用了哪些技术方法？"
✅ 对比性问题：    "不同论文的方法有什么区别？"
✅ 总结性问题：    "请总结这篇论文的核心思想"
✅ 批判性问题：    "这个方法有什么局限性？"
```

### 3. 🌐 联网搜索增强
集成 DuckDuckGo 搜索引擎，获取最新信息。

**应用场景：**
- 🆕 **最新进展**：查询领域最新研究动态
- 📚 **补充信息**：获取论文中提到但未详述的背景知识
- 🔗 **相关资源**：查找相关论文、代码、数据集
- ✓ **事实核查**：验证论文中的数据和引用

**使用方式：**
- **RAG 模式**：先检索文档，不足时自动联网
- **LLM 模式**：直接使用联网搜索回答问题

### 4. 📄 文档管理

#### 支持的文档格式
| 格式 | 扩展名 | 特性 |
|------|--------|------|
| PDF | `.pdf` | ✅ 学术论文标准格式 |
| Word | `.docx` | ✅ 保留格式和结构 |
| Markdown | `.md` | ✅ 纯文本，易编辑 |
| 纯文本 | `.txt` | ✅ 简单快速 |

#### 文档加载特性
- **批量处理**：一次加载整个文件夹
- **递归扫描**：自动扫描子目录
- **智能分块**：根据文档结构自动分段
- **元数据保留**：保存文件名、页码等信息

### 5. 🔍 语义检索索引

#### 索引功能
- **向量化存储**：使用 Chroma 向量数据库
- **语义理解**：基于 BAAI/bge-small-zh-v1.5 中文向量模型
- **快速检索**：毫秒级响应
- **持久化**：索引可保存和重用

#### 检索策略
- Top-K 检索：返回最相关的 K 个文本块
- 相似度评分：0-1 范围的相关性分数
- 语义检索：基于语义理解而非关键词匹配

### 6. 📊 来源追溯
每个答案都标注原文出处，确保可验证性。

**追溯信息包括：**
- 📁 **文件名**：答案来自哪个文档
- 📄 **页码**：具体在文档的哪一页（如果有）
- 📝 **原文片段**：显示相关的原始文本
- 🎯 **相似度分数**：相关性评分（0-1）

**示例输出：**
```
📌 参考来源 1:
   文件: transformer_paper.pdf
   相似度: 0.89
   内容: "我们提出了一个完全基于注意力机制的模型架构..."

📌 参考来源 2:
   文件: attention_mechanism.pdf
   相似度: 0.82
   内容: "自注意力机制允许模型在处理序列时..."
```

---

## 问答模式

### 模式 1: RAG（检索增强生成）
**工作流程：**
```
用户提问 → 语义检索文档库 → 提取相关段落 → LLM 生成答案 → 返回答案+来源
```

**特点：**
- ✅ 基于本地文档，答案准确可靠
- ✅ 提供原文出处，可验证
- ✅ 不受 LLM 知识截止日期限制
- ⚠️ 仅能回答文档中包含的内容

**适合场景：**
- 📖 深度阅读已有论文
- 🔍 查找特定技术细节
- 📊 对比分析多篇论文

### 模式 2: LLM（直接对话）
**工作流程：**
```
用户提问 → LLM 生成答案 → [可选]联网搜索 → 返回答案
```

**特点：**
- ✅ 可回答开放性问题
- ✅ 支持联网获取最新信息
- ✅ 不依赖本地文档
- ⚠️ 答案需要自行验证

**适合场景：**
- 💭 探索性提问
- 🌐 查询最新进展
- 💡 获取创意和建议

---

## 界面选项

### 1. 🌐 Web UI（推荐新手）

**两种模式：**
- **多轮对话模式**（`web_ui_multi_turn.py`）：支持上下文记忆
- **单轮问答模式**（`web_ui_single_turn.py`）：快速查询

**功能特点：**
- 🎨 **图形化界面**：美观易用，零学习成本
- 💬 **对话界面**：聊天式交互体验
- 🔧 **参数调节**：可视化调整检索参数
- 📊 **实时反馈**：进度提示、状态显示
- 📌 **来源追溯**：显示参考文档和相似度

**启动方式：**
```bash
# 多轮对话（推荐）
./start_web_multi.sh

# 单轮问答
./start_web_single.sh
```

访问地址：`http://localhost:7860`

### 2. 💻 命令行界面（开发者友好）

**两种模式：**
- **多轮对话模式**（`cli_multi_turn.py`）：交互式对话
- **单轮问答模式**（`cli_single_turn.py`）：快速查询、脚本调用

**功能特点：**
- ⚡ **快速执行**：适合脚本化和自动化
- 🔧 **灵活控制**：丰富的命令选项
- 📝 **批处理**：可编写脚本批量处理
- 🎨 **彩色输出**：美观的终端显示

**启动方式：**
```bash
# 多轮对话（推荐）
./start_cli_multi.sh

# 单轮问答
./start_cli_single.sh
```

---

## 多轮对话 vs 单轮问答

### 🎯 多轮对话模式

**适合场景：**
- 📖 **深入学习**：逐步理解复杂概念
- 🔍 **文献综述**：对比多篇论文
- 💭 **学术讨论**：头脑风暴和深入分析
- 🎓 **论文解读**：完整理解论文内容

**特性：**
- ✅ 记忆最近对话（默认 10 轮）
- ✅ 理解代词和指代关系
- ✅ 支持连续追问
- ✅ 可查看和清除历史

**对话示例：**
```
👤: 什么是 Transformer？
🤖: Transformer 是一种基于注意力机制的架构...

👤: 它有什么优点？              # ← 自动理解"它"指 Transformer
🤖: Transformer 的主要优点包括...

👤: 缺点呢？                    # ← 继续讨论
🤖: 主要缺点是计算复杂度较高...
```

### ⚡ 单轮问答模式

**适合场景：**
- 🔬 **快速查询**：独立问题，无需上下文
- 📊 **批量处理**：脚本自动化
- 💻 **API 调用**：程序集成
- 📈 **数据提取**：结构化信息提取

**特性：**
- ✅ 响应速度快（1-3秒）
- ✅ 每次查询独立
- ✅ 适合脚本调用
- ✅ 内存占用低

**查询示例：**
```bash
# 快速查询
python cli_single_turn.py query "这篇论文的主要贡献是什么？"

# 批量处理
for paper in papers; do
    python cli_single_turn.py query "总结 $paper"
done
```

### 📊 功能对比

| 特性 | 多轮对话 | 单轮问答 |
|------|---------|---------|
| 上下文记忆 | ✅ | ❌ |
| 连续追问 | ✅ | ❌ |
| 指代理解 | ✅ | ❌ |
| 响应速度 | 1.5-4秒 | 1-3秒 |
| 内存占用 | 中等 | 低 |
| 脚本调用 | ⚠️ | ✅ |
| 适合新手 | ✅ | ⚠️ |

---

## 应用场景

### 场景 1：快速理解新论文

**需求**：快速了解一篇刚下载的论文

**操作流程：**
1. 上传论文到 `data/documents/`
2. 启动 Web UI 多轮对话
3. 构建索引
4. 连续提问：
   - "这篇论文的主要贡献是什么？"
   - "它解决了什么问题？"
   - "实验结果如何？"

**推荐模式**：Web UI 多轮对话

### 场景 2：文献综述

**需求**：对比分析多篇相关论文

**操作流程：**
1. 批量上传论文到文档目录
2. 构建统一索引
3. 使用多轮对话深入分析：
   - "这些论文使用的方法有什么区别？"
   - "哪篇论文关注 Transformer 架构？"
   - "它们的实验结果如何对比？"

**推荐模式**：命令行多轮对话（便于记录）

### 场景 3：跟踪最新进展

**需求**：了解领域最新研究动态

**操作流程：**
1. 启动 Web UI
2. 开启联网搜索
3. 提问：
   - "2024年 Transformer 有哪些新进展？"
   - "最近有哪些重要的 NLP 论文？"

**推荐模式**：Web UI 多轮对话 + 联网搜索

### 场景 4：快速批量查询

**需求**：批量提取论文信息

**操作流程：**
```bash
# 批量查询脚本
for paper in *.pdf; do
    python cli_single_turn.py query "总结 $paper 的主要内容"
done
```

**推荐模式**：命令行单轮问答

---

## 优势对比

### vs 人工阅读

| 维度 | 本系统 | 人工阅读 |
|------|--------|----------|
| 速度 | ⚡ 秒级响应 | 🐌 小时级 |
| 覆盖 | 📚 可同时分析多篇 | 📖 逐篇阅读 |
| 检索 | ✅ 全文快速检索 | ⚠️ 可能遗漏细节 |
| 理解深度 | ⚠️ 依赖 LLM 理解 | ✅ 深度理解 |
| 批判性思维 | ⚠️ 需要引导 | ✅ 自然具备 |

**建议**：系统辅助快速理解，人工审核关键内容

### vs 传统关键词搜索

| 维度 | 本系统 | 传统搜索 |
|------|--------|----------|
| 理解能力 | ✅ 语义理解 | ❌ 关键词匹配 |
| 上下文 | ✅ 多轮对话 | ❌ 单次查询 |
| 答案形式 | 📝 自然语言回答 | 🔗 文档链接 |
| 本地文档 | ✅ 深度检索 | ❌ 不支持 |
| 准确性 | ✅ 提供来源 | ⚠️ 需自行验证 |

---

## 技术栈

| 组件 | 技术选型 | 说明 |
|------|---------|------|
| **RAG 框架** | LlamaIndex | 强大的 RAG 开发框架 |
| **向量数据库** | Chroma | 轻量级向量存储 |
| **Embedding** | BAAI/bge-small-zh-v1.5 | 中文语义向量模型 |
| **LLM** | Moonshot/OpenAI/DeepSeek | 支持多种 LLM API |
| **Web UI** | Gradio 4.0+ | 快速构建 ML 应用 |
| **搜索引擎** | DuckDuckGo | 免费无需 API key |
| **文档解析** | LlamaIndex SimpleDirectoryReader | 支持多种格式 |
A: 系统提供原文出处，用户可以验证。建议重要内容人工核实。

**Q: 可以离线使用吗？**
A: 文档检索部分可离线，但 LLM 推理需要 API 连接（或本地部署模型）。

**Q: 如何提高答案质量？**
A: 
1. 提供清晰具体的问题
2. 使用 RAG 模式获取准确答案
3. 调整 Top-K 参数增加检索范围
4. 必要时开启联网搜索

---

## 下一步

- 📖 阅读 [使用指南](USER_GUIDE.md) 了解详细操作
- 👨‍💻 阅读 [开发者文档](DEVELOPER_GUIDE.md) 进行二次开发
- 🚀 立即开始：`python web_ui.py`
