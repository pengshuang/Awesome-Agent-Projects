"""
å­¦æœ¯è®ºæ–‡é—®ç­” Agent æ ¸å¿ƒæ¨¡å—

æä¾›åŸºäº LlamaIndex çš„å‘é‡ç´¢å¼•ç®¡ç†å’Œæ™ºèƒ½é—®ç­”åŠŸèƒ½
"""

from pathlib import Path
from typing import Optional, List, Dict, Any
from datetime import datetime

from llama_index.core import (
    VectorStoreIndex,
    StorageContext,
    load_index_from_storage,
    Settings,
)
from llama_index.core.schema import Document
from loguru import logger

from config import SystemConfig
from src.loaders.document_loader import DocumentLoader
from src.constants import (
    LOG_SEPARATOR_FULL,
    LOG_SEPARATOR_HALF,
    DEFAULT_WEB_SEARCH_RESULTS,
    INDEX_FILE_NAMES,
    ERROR_NO_DOCUMENTS,
    ERROR_INDEX_NOT_INITIALIZED,
    SUCCESS_INDEX_LOADED,
    SUCCESS_INDEX_BUILT,
    SUCCESS_DOCUMENTS_LOADED,
    SUCCESS_QUERY_COMPLETED,
    WARNING_NO_DOCUMENTS_FOUND,
    WARNING_FORCE_REBUILD,
    INFO_LOADING_FROM_DISK,
    INFO_BUILDING_NEW_INDEX,
    INFO_WEB_SEARCH_ENABLED,
    INFO_WEB_SEARCH_RESULTS,
)


class AcademicAgent:
    """
    å­¦æœ¯è®ºæ–‡é—®ç­” Agent
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. å‘é‡ç´¢å¼•ç®¡ç†ï¼ˆæ„å»ºã€åŠ è½½ã€æŒä¹…åŒ–ï¼‰
    2. æ™ºèƒ½é—®ç­”ï¼ˆåŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰
    3. è®ºæ–‡ç®¡ç†ï¼ˆåˆ—å‡ºã€ç»Ÿè®¡ï¼‰
    
    Attributes:
        documents_dir: æ–‡æ¡£ç›®å½•è·¯å¾„
        index_dir: ç´¢å¼•å­˜å‚¨ç›®å½•è·¯å¾„
        index: å‘é‡ç´¢å¼•å®ä¾‹
        query_engine: æŸ¥è¯¢å¼•æ“å®ä¾‹
        documents: å·²åŠ è½½çš„æ–‡æ¡£åˆ—è¡¨
    """
    
    def __init__(
        self,
        documents_dir: Optional[str] = None,
        index_dir: Optional[str] = None,
        auto_load: bool = True,
    ):
        """
        åˆå§‹åŒ–å­¦æœ¯è®ºæ–‡é—®ç­” Agent
        
        Args:
            documents_dir: æ–‡æ¡£ç›®å½•è·¯å¾„ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„è·¯å¾„
            index_dir: ç´¢å¼•å­˜å‚¨ç›®å½•è·¯å¾„ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„è·¯å¾„
            auto_load: æ˜¯å¦è‡ªåŠ¨åŠ è½½æˆ–æ„å»ºç´¢å¼•
        """
        logger.info(LOG_SEPARATOR_FULL)
        logger.info("åˆå§‹åŒ–å­¦æœ¯è®ºæ–‡é—®ç­” Agent")
        logger.info(LOG_SEPARATOR_FULL)
        
        # è®¾ç½®è·¯å¾„
        self.documents_dir = Path(documents_dir or SystemConfig.DOCUMENTS_DIR)
        self.index_dir = Path(index_dir or SystemConfig.VECTOR_STORE_DIR)
        
        logger.info(f"æ–‡æ¡£ç›®å½•: {self.documents_dir}")
        logger.info(f"ç´¢å¼•ç›®å½•: {self.index_dir}")
        
        # åˆå§‹åŒ–å±æ€§
        self.index: Optional[VectorStoreIndex] = None
        self.query_engine = None
        self.documents: List[Document] = []
        
        # å¯¹è¯å†å²ç®¡ç†
        self.chat_history: List[Dict[str, str]] = []  # å­˜å‚¨å¯¹è¯å†å² [{"role": "user/assistant", "content": "..."}]
        self.max_history_turns: int = 10  # æœ€å¤§ä¿ç•™å†å²è½®æ•°
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self._ensure_directories()
        
        # è‡ªåŠ¨åŠ è½½ç´¢å¼•
        if auto_load:
            self.load_or_build_index()
        
        logger.info("Agent åˆå§‹åŒ–å®Œæˆ")
    
    def _ensure_directories(self):
        """ç¡®ä¿å¿…è¦çš„ç›®å½•å­˜åœ¨"""
        self.documents_dir.mkdir(parents=True, exist_ok=True)
        self.index_dir.mkdir(parents=True, exist_ok=True)
        logger.debug(f"âœ“ ç›®å½•æ£€æŸ¥å®Œæˆ")
    
    def _index_exists(self) -> bool:
        """
        æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
        
        Returns:
            ç´¢å¼•æ˜¯å¦å­˜åœ¨
        """
        # æ£€æŸ¥ç´¢å¼•ç›®å½•ä¸‹æ˜¯å¦æœ‰å¿…è¦çš„æ–‡ä»¶ï¼ˆLlamaIndex ç´¢å¼•éœ€è¦è¿™ä¸‰ä¸ª JSON æ–‡ä»¶ï¼‰
        required_files = INDEX_FILE_NAMES
        
        for file_name in required_files:
            file_path = self.index_dir / file_name
            if not file_path.exists():
                logger.debug(f"ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨: {file_name}")
                return False
        
        logger.debug("âœ“ ç´¢å¼•æ–‡ä»¶å®Œæ•´")
        return True
    
    def load_or_build_index(self, force_rebuild: bool = False) -> VectorStoreIndex:
        """
        åŠ è½½æˆ–æ„å»ºå‘é‡ç´¢å¼•
        
        å¦‚æœç´¢å¼•å·²å­˜åœ¨ä¸”ä¸å¼ºåˆ¶é‡å»ºï¼Œåˆ™ä»ç£ç›˜åŠ è½½ï¼›å¦åˆ™é‡æ–°æ„å»ºç´¢å¼•ã€‚
        
        Args:
            force_rebuild: æ˜¯å¦å¼ºåˆ¶é‡å»ºç´¢å¼•
            
        Returns:
            å‘é‡ç´¢å¼•å®ä¾‹
        """
        logger.info(LOG_SEPARATOR_HALF)
        logger.info("å¼€å§‹åŠ è½½æˆ–æ„å»ºç´¢å¼•...")
        logger.info(LOG_SEPARATOR_HALF)
        
        if force_rebuild:
            logger.info(WARNING_FORCE_REBUILD)
            return self.rebuild_index()
        
        # å°è¯•åŠ è½½ç°æœ‰ç´¢å¼•
        if self._index_exists():
            try:
                logger.info("æ£€æµ‹åˆ°ç°æœ‰ç´¢å¼•ï¼Œå°è¯•åŠ è½½...")
                return self._load_index()
            except Exception as e:
                logger.warning(f"åŠ è½½ç´¢å¼•å¤±è´¥: {e}")
                logger.info("å°†é‡æ–°æ„å»ºç´¢å¼•...")
                return self.rebuild_index()
        else:
            logger.info(INFO_BUILDING_NEW_INDEX)
            return self.rebuild_index()
    
    def _load_index(self) -> VectorStoreIndex:
        """
        ä»ç£ç›˜åŠ è½½ç´¢å¼•
        
        Returns:
            å‘é‡ç´¢å¼•å®ä¾‹
        """
        start_time = datetime.now()
        
        logger.info(INFO_LOADING_FROM_DISK.format(self.index_dir))
        
        try:
            # åŠ è½½å­˜å‚¨ä¸Šä¸‹æ–‡
            storage_context = StorageContext.from_defaults(
                persist_dir=str(self.index_dir)
            )
            
            # åŠ è½½ç´¢å¼•
            self.index = load_index_from_storage(storage_context)
            
            # åˆ›å»ºæŸ¥è¯¢å¼•æ“
            self._create_query_engine()
            
            # è®¡ç®—åŠ è½½æ—¶é—´
            elapsed = (datetime.now() - start_time).total_seconds()
            
            logger.success(SUCCESS_INDEX_LOADED.format(elapsed))
            
            # å°è¯•è·å–æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯
            try:
                # å°è¯•è·å–ç´¢å¼•ä¸­çš„æ–‡æ¡£å—æ•°é‡ï¼ˆéƒ¨åˆ†ç´¢å¼•ç±»å‹å¯èƒ½ä¸æ”¯æŒï¼‰
                doc_count = len(self.index.docstore.docs)
                logger.info(f"ç´¢å¼•åŒ…å« {doc_count} ä¸ªæ–‡æ¡£å—")
            except Exception:
                # æŸäº›å‘é‡å­˜å‚¨ï¼ˆå¦‚ Qdrantï¼‰å¯èƒ½æ²¡æœ‰ docstore å±æ€§ï¼Œå¿½ç•¥é”™è¯¯
                logger.debug("æ— æ³•è·å–æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯")
            
            return self.index
            
        except Exception as e:
            logger.error(f"âœ— ç´¢å¼•åŠ è½½å¤±è´¥: {e}")
            raise
    
    def rebuild_index(self) -> VectorStoreIndex:
        """
        é‡æ–°æ„å»ºå‘é‡ç´¢å¼•
        
        å®Œæ•´æµç¨‹ï¼š
        1. åŠ è½½æ–‡æ¡£
        2. æ„å»ºå‘é‡ç´¢å¼•
        3. æŒä¹…åŒ–åˆ°ç£ç›˜
        
        Returns:
            å‘é‡ç´¢å¼•å®ä¾‹
        """
        logger.info(LOG_SEPARATOR_FULL)
        logger.info("å¼€å§‹æ„å»ºå‘é‡ç´¢å¼•")
        logger.info(LOG_SEPARATOR_FULL)
        
        start_time = datetime.now()
        
        try:
            # 1. åŠ è½½æ–‡æ¡£
            logger.info("æ­¥éª¤ 1/3: åŠ è½½æ–‡æ¡£")
            self._load_documents()
            
            if not self.documents:
                raise ValueError(ERROR_NO_DOCUMENTS.format(self.documents_dir))
            
            # 2. æ„å»ºç´¢å¼•
            logger.info("æ­¥éª¤ 2/3: æ„å»ºå‘é‡ç´¢å¼•")
            self._build_index()
            
            # 3. æŒä¹…åŒ–ç´¢å¼•
            logger.info("æ­¥éª¤ 3/3: æŒä¹…åŒ–ç´¢å¼•åˆ°ç£ç›˜")
            self._persist_index()
            
            # åˆ›å»ºæŸ¥è¯¢å¼•æ“
            self._create_query_engine()
            
            # è®¡ç®—æ€»è€—æ—¶
            elapsed = (datetime.now() - start_time).total_seconds()
            
            logger.success(LOG_SEPARATOR_FULL)
            logger.success(SUCCESS_INDEX_BUILT.format(elapsed))
            logger.success(LOG_SEPARATOR_FULL)
            
            return self.index
            
        except Exception as e:
            logger.error(f"âœ— ç´¢å¼•æ„å»ºå¤±è´¥: {e}")
            raise
    
    def _load_documents(self):
        """åŠ è½½æ–‡æ¡£"""
        logger.info(f"ä»ç›®å½•åŠ è½½æ–‡æ¡£: {self.documents_dir}")
        
        # ä½¿ç”¨ DocumentLoader åŠ è½½æ–‡æ¡£
        loader = DocumentLoader(
            input_dir=self.documents_dir,
            recursive=True,
            clean_text=True,
            preserve_formatting=True,
        )
        
        # åŠ è½½æ‰€æœ‰æ”¯æŒçš„æ–‡æ¡£æ ¼å¼
        self.documents = loader.load_documents()
        
        if not self.documents:
            logger.warning(WARNING_NO_DOCUMENTS_FOUND.format(self.documents_dir))
            return
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        stats = loader.get_document_stats(self.documents)
        
        logger.success(SUCCESS_DOCUMENTS_LOADED + ":")
        logger.info(f"  - æ€»æ–‡æ¡£æ•°: {stats['total_documents']}")
        logger.info(f"  - æ€»æ–‡ä»¶æ•°: {stats['total_files']}")
        logger.info(f"  - æ–‡ä»¶ç±»å‹: {stats['file_types']}")
        logger.info(f"  - æ€»å¤§å°: {stats['total_size_mb']:.2f} MB")
        logger.info(f"  - æ€»å­—ç¬¦æ•°: {stats['total_chars']:,}")
        logger.info(f"  - æ€»å•è¯æ•°: {stats['total_words']:,}")
    
    def _build_index(self):
        """æ„å»ºå‘é‡ç´¢å¼•"""
        import os
        logger.info(f"ä½¿ç”¨ Embedding æä¾›å•†: {os.getenv('EMBEDDING_PROVIDER', 'huggingface')}")
        logger.info(f"Chunk å¤§å°: {SystemConfig.CHUNK_SIZE}, é‡å : {SystemConfig.CHUNK_OVERLAP}")
        
        # ä½¿ç”¨ LlamaIndex Settings ä¸­é…ç½®çš„ Embedding æ¨¡å‹
        # Settings å·²åœ¨ç³»ç»Ÿåˆå§‹åŒ–æ—¶é…ç½®
        
        # æ„å»ºç´¢å¼•
        self.index = VectorStoreIndex.from_documents(
            self.documents,
            show_progress=True,
        )
        
        logger.success(f"âœ“ å‘é‡ç´¢å¼•æ„å»ºå®Œæˆ")
        
        # æ˜¾ç¤ºç´¢å¼•ç»Ÿè®¡
        try:
            doc_count = len(self.index.docstore.docs)
            logger.info(f"ç´¢å¼•åŒ…å« {doc_count} ä¸ªæ–‡æ¡£å—")
        except Exception:
            logger.debug("æ— æ³•è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯")
    
    def _persist_index(self):
        """æŒä¹…åŒ–ç´¢å¼•åˆ°ç£ç›˜"""
        logger.info(f"ä¿å­˜ç´¢å¼•åˆ°: {self.index_dir}")
        
        # æŒä¹…åŒ–
        self.index.storage_context.persist(persist_dir=str(self.index_dir))
        
        logger.success(f"âœ“ ç´¢å¼•å·²ä¿å­˜åˆ°ç£ç›˜")
        
        # æ˜¾ç¤ºä¿å­˜çš„æ–‡ä»¶
        saved_files = list(self.index_dir.glob('*.json'))
        logger.debug(f"ä¿å­˜çš„æ–‡ä»¶: {[f.name for f in saved_files]}")
    
    def _create_query_engine(self):
        """åˆ›å»ºæŸ¥è¯¢å¼•æ“"""
        if not self.index:
            raise ValueError(ERROR_INDEX_NOT_INITIALIZED)
        
        # åˆ›å»ºæŸ¥è¯¢å¼•æ“ï¼Œä½¿ç”¨é…ç½®çš„å‚æ•°
        self.query_engine = self.index.as_query_engine(
            similarity_top_k=SystemConfig.RETRIEVAL_TOP_K,
            streaming=False,
        )
        
        logger.debug(f"âœ“ æŸ¥è¯¢å¼•æ“å·²åˆ›å»º (top_k={SystemConfig.RETRIEVAL_TOP_K})")
    
    def query(
        self,
        question: str,
        top_k: Optional[int] = None,
        verbose: bool = False,
        enable_web_search: bool = None,
        use_history: bool = False,
    ) -> Dict[str, Any]:
        """
        æ‰§è¡ŒæŸ¥è¯¢
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            top_k: æ£€ç´¢çš„ç›¸å…³æ–‡æ¡£æ•°é‡ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®å€¼
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            enable_web_search: æ˜¯å¦å¯ç”¨è”ç½‘æœç´¢ï¼ŒNoneæ—¶ä½¿ç”¨é…ç½®å€¼
            use_history: æ˜¯å¦ä½¿ç”¨å¯¹è¯å†å²
            
        Returns:
            åŒ…å«æŸ¥è¯¢ç»“æœçš„å­—å…¸:
            - answer: ç”Ÿæˆçš„ç­”æ¡ˆ
            - source_nodes: å‚è€ƒçš„æºæ–‡æ¡£èŠ‚ç‚¹ï¼ˆåŒ…å«æ–‡æœ¬ç‰‡æ®µï¼‰
            - web_sources: è”ç½‘æœç´¢ç»“æœï¼ˆå¦‚æœå¯ç”¨ï¼‰
            - metadata: å…ƒæ•°æ®ä¿¡æ¯
        """
        if not self.query_engine:
            raise ValueError("æŸ¥è¯¢å¼•æ“æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆåŠ è½½æˆ–æ„å»ºç´¢å¼•")
        
        logger.info(LOG_SEPARATOR_HALF)
        logger.info(f"é—®é¢˜: {question}")
        logger.info(LOG_SEPARATOR_HALF)
        
        # å¤„ç†å¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡
        enhanced_question = question
        if use_history and self.chat_history:
            # æ„å»ºå¸¦å†å²çš„æç¤ºè¯
            context_prompt = self._build_context_prompt(question)
            enhanced_question = context_prompt
            logger.debug(f"ä½¿ç”¨å¯¹è¯å†å²ï¼Œå†å²è½®æ•°: {len(self.chat_history) // 2}")
        
        start_time = datetime.now()
        web_sources = []
        
        try:
            # æ£€æŸ¥æ˜¯å¦å¯ç”¨è”ç½‘æœç´¢
            if enable_web_search is None:
                enable_web_search = SystemConfig.ENABLE_WEB_SEARCH
            
            # æ‰§è¡Œè”ç½‘æœç´¢ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if enable_web_search:
                try:
                    from src.tools.web_search import WebSearchTool
                    logger.info("ğŸŒ æ­£åœ¨è¿›è¡Œè”ç½‘æœç´¢...")
                    web_tool = WebSearchTool(max_results=3)
                    web_sources = web_tool.search(question)
                    
                    if web_sources:
                        logger.info(f"âœ“ æ‰¾åˆ° {len(web_sources)} ä¸ªç½‘ç»œèµ„æº:")
                        for i, source in enumerate(web_sources, 1):
                            logger.info(f"  [{i}] {source['url']}")
                    else:
                        logger.warning("âš  æœªæ‰¾åˆ°ç›¸å…³ç½‘ç»œèµ„æº")
                except Exception as e:
                    logger.warning(f"è”ç½‘æœç´¢å¤±è´¥: {e}")
            
            # å¦‚æœæŒ‡å®šäº† top_kï¼Œé‡æ–°åˆ›å»ºæŸ¥è¯¢å¼•æ“
            if top_k and top_k != SystemConfig.RETRIEVAL_TOP_K:
                logger.debug(f"ä½¿ç”¨è‡ªå®šä¹‰ top_k: {top_k}")
                query_engine = self.index.as_query_engine(
                    similarity_top_k=top_k,
                    streaming=False,
                )
            else:
                query_engine = self.query_engine
            
            # æ‰§è¡ŒRAGæŸ¥è¯¢
            logger.info("æ­£åœ¨æ£€ç´¢ç›¸å…³æ–‡æ¡£å¹¶ç”Ÿæˆå›ç­”...")
            response = query_engine.query(enhanced_question)
            
            # è®¡ç®—è€—æ—¶
            elapsed = (datetime.now() - start_time).total_seconds()
            
            # æå–ç­”æ¡ˆ
            answer = str(response)
            
            # æå–æºèŠ‚ç‚¹ï¼ˆåŒ…å«æ–‡æœ¬ç‰‡æ®µï¼‰
            source_nodes = []
            if hasattr(response, 'source_nodes'):
                source_nodes = response.source_nodes
            
            logger.success(f"âœ“ æŸ¥è¯¢å®Œæˆï¼è€—æ—¶: {elapsed:.2f} ç§’")
            
            if verbose:
                logger.info(f"\nå›ç­”:\n{answer}\n")
                
                if source_nodes:
                    logger.info(f"å‚è€ƒäº† {len(source_nodes)} ä¸ªæ–‡æ¡£ç‰‡æ®µ:")
                    for i, node in enumerate(source_nodes, 1):
                        score = node.score if hasattr(node, 'score') else 'N/A'
                        file_name = node.metadata.get('file_name', 'Unknown')
                        text_preview = node.text[:100].replace('\n', ' ') if hasattr(node, 'text') else 'N/A'
                        logger.info(f"  [{i}] {file_name} (ç›¸ä¼¼åº¦: {score})")
                        logger.info(f"      ç‰‡æ®µ: {text_preview}...")
            
            # æ›´æ–°å¯¹è¯å†å²
            if use_history:
                self._update_chat_history(question, answer)
            
            # æ„å»ºç»“æœ
            result = {
                'answer': answer,
                'source_nodes': source_nodes,
                'web_sources': web_sources,
                'metadata': {
                    'question': question,
                    'elapsed_time': elapsed,
                    'num_sources': len(source_nodes),
                    'num_web_sources': len(web_sources),
                    'top_k': top_k or SystemConfig.RETRIEVAL_TOP_K,
                    'web_search_enabled': enable_web_search,
                    'use_history': use_history,
                    'history_turns': len(self.chat_history) // 2,
                }
            }
            
            return result
            
        except Exception as e:
            logger.error(f"âœ— æŸ¥è¯¢å¤±è´¥: {e}")
            raise
    
    def query_direct(
        self,
        question: str,
        context: Optional[str] = None,
        enable_web_search: bool = None,
    ) -> Dict[str, Any]:
        """
        ç›´æ¥æŸ¥è¯¢LLMï¼ˆä¸ä½¿ç”¨å‘é‡åº“ï¼‰
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            context: å¯é€‰çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆç”¨æˆ·æä¾›çš„æ–‡æœ¬ï¼‰
            enable_web_search: æ˜¯å¦å¯ç”¨è”ç½‘æœç´¢
            
        Returns:
            åŒ…å«æŸ¥è¯¢ç»“æœçš„å­—å…¸:
            - answer: LLMç”Ÿæˆçš„ç­”æ¡ˆ
            - web_sources: è”ç½‘æœç´¢ç»“æœï¼ˆå¦‚æœå¯ç”¨ï¼‰
            - metadata: å…ƒæ•°æ®ä¿¡æ¯
        """
        logger.info(LOG_SEPARATOR_HALF)
        logger.info(f"é—®é¢˜ (ç›´æ¥LLMæ¨¡å¼): {question}")
        logger.info(LOG_SEPARATOR_HALF)
        
        start_time = datetime.now()
        web_sources = []
        
        try:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦è”ç½‘æœç´¢ï¼ˆNone æ—¶ä½¿ç”¨é…ç½®çš„é»˜è®¤å€¼ï¼‰
            if enable_web_search is None:
                enable_web_search = SystemConfig.ENABLE_WEB_SEARCH
            
            # æ‰§è¡Œè”ç½‘æœç´¢ä»¥è¡¥å……LLMçŸ¥è¯†
            if enable_web_search:
                try:
                    # åŠ¨æ€å¯¼å…¥é¿å…å¯åŠ¨æ—¶ä¾èµ–
                    from src.tools.web_search import WebSearchTool
                    logger.info(INFO_WEB_SEARCH_ENABLED)
                    # é™åˆ¶è¿”å›ç»“æœæ•°é¿å…ä¸Šä¸‹æ–‡è¿‡é•¿
                    web_tool = WebSearchTool(max_results=DEFAULT_WEB_SEARCH_RESULTS)
                    web_sources = web_tool.search(question)
                    
                    if web_sources:
                        logger.info(INFO_WEB_SEARCH_RESULTS.format(len(web_sources)) + ":")
                        for i, source in enumerate(web_sources, 1):
                            logger.info(f"  [{i}] {source['url']}")
                except Exception as e:
                    logger.warning(f"è”ç½‘æœç´¢å¤±è´¥: {e}")
            
            # æ„å»ºæç¤ºè¯
            prompt = question
            if context:
                prompt = f"æ ¹æ®ä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜:\n\nä¸Šä¸‹æ–‡:\n{context}\n\né—®é¢˜: {question}"
            elif web_sources:
                # å¦‚æœæœ‰ç½‘ç»œæœç´¢ç»“æœï¼Œæ·»åŠ åˆ°æç¤ºè¯
                web_context = "\n\n".join([
                    f"æ¥æº [{i+1}]: {s['title']}\n{s['snippet']}\nç½‘å€: {s['url']}"
                    for i, s in enumerate(web_sources)
                ])
                prompt = f"æ ¹æ®ä»¥ä¸‹ç½‘ç»œæœç´¢ç»“æœå›ç­”é—®é¢˜:\n\n{web_context}\n\né—®é¢˜: {question}"
            
            # ç›´æ¥è°ƒç”¨LLM
            logger.info("æ­£åœ¨è°ƒç”¨LLMç”Ÿæˆå›ç­”...")
            from llama_index.core import Settings
            llm = Settings.llm
            
            response = llm.complete(prompt)
            answer = str(response)
            
            # è®¡ç®—è€—æ—¶
            elapsed = (datetime.now() - start_time).total_seconds()
            
            logger.success(SUCCESS_QUERY_COMPLETED.format(elapsed))
            
            # æ„å»ºç»“æœ
            result = {
                'answer': answer,
                'source_nodes': [],  # ç›´æ¥æ¨¡å¼æ²¡æœ‰æºèŠ‚ç‚¹
                'web_sources': web_sources,
                'metadata': {
                    'question': question,
                    'elapsed_time': elapsed,
                    'num_sources': 0,
                    'num_web_sources': len(web_sources),
                    'mode': 'direct_llm',
                    'has_context': bool(context),
                    'web_search_enabled': enable_web_search,
                }
            }
            
            return result
            
        except Exception as e:
            logger.error(f"âœ— ç›´æ¥æŸ¥è¯¢å¤±è´¥: {e}")
            raise
    
    def list_papers(self, detailed: bool = False) -> List[Dict[str, Any]]:
        """
        åˆ—å‡ºæ‰€æœ‰å·²åŠ è½½çš„è®ºæ–‡
        
        Args:
            detailed: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            
        Returns:
            è®ºæ–‡åˆ—è¡¨ï¼Œæ¯ä¸ªè®ºæ–‡åŒ…å«å…ƒæ•°æ®ä¿¡æ¯
        """
        logger.info("-" * 70)
        logger.info("å·²åŠ è½½çš„è®ºæ–‡åˆ—è¡¨")
        logger.info("-" * 70)
        
        if not self.documents:
            logger.warning("âš  æœªåŠ è½½ä»»ä½•æ–‡æ¡£")
            return []
        
        # æŒ‰æ–‡ä»¶ååˆ†ç»„æ–‡æ¡£
        papers_dict: Dict[str, Dict[str, Any]] = {}
        
        for doc in self.documents:
            file_name = doc.metadata.get('file_name', 'Unknown')
            
            if file_name not in papers_dict:
                papers_dict[file_name] = {
                    'file_name': file_name,
                    'file_path': doc.metadata.get('file_path', 'Unknown'),
                    'file_type': doc.metadata.get('file_type', 'Unknown'),
                    'file_size_mb': doc.metadata.get('file_size_mb', 0),
                    'page_count': 1,
                    'total_chars': len(doc.text),
                    'created_time': doc.metadata.get('created_time', 'Unknown'),
                    'modified_time': doc.metadata.get('modified_time', 'Unknown'),
                }
            else:
                # å¦‚æœæ˜¯ PDF çš„å¤šé¡µï¼Œç´¯åŠ ä¿¡æ¯
                papers_dict[file_name]['page_count'] += 1
                papers_dict[file_name]['total_chars'] += len(doc.text)
        
        # è½¬æ¢ä¸ºåˆ—è¡¨
        papers = list(papers_dict.values())
        
        # æ’åº
        papers.sort(key=lambda x: x['file_name'])
        
        # æ˜¾ç¤ºåˆ—è¡¨
        logger.info(f"æ€»è®¡: {len(papers)} ä¸ªè®ºæ–‡æ–‡ä»¶\n")
        
        for i, paper in enumerate(papers, 1):
            logger.info(f"[{i}] {paper['file_name']}")
            if detailed:
                logger.info(f"    ç±»å‹: {paper['file_type']}")
                logger.info(f"    å¤§å°: {paper['file_size_mb']:.2f} MB")
                if paper['file_type'] == 'pdf':
                    logger.info(f"    é¡µæ•°: {paper['page_count']}")
                logger.info(f"    å­—ç¬¦æ•°: {paper['total_chars']:,}")
                logger.info(f"    è·¯å¾„: {paper['file_path']}")
                logger.info("")
        
        return papers
    
    def get_stats(self) -> Dict[str, Any]:
        """
        è·å– Agent ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        stats = {
            'documents_dir': str(self.documents_dir),
            'index_dir': str(self.index_dir),
            'index_exists': self._index_exists(),
            'documents_loaded': len(self.documents),
            'query_engine_ready': self.query_engine is not None,
        }
        
        # å¦‚æœæœ‰ç´¢å¼•ï¼Œæ·»åŠ ç´¢å¼•ç»Ÿè®¡
        if self.index:
            try:
                stats['index_doc_count'] = len(self.index.docstore.docs)
            except Exception:
                stats['index_doc_count'] = 'N/A'
        
        return stats
    
    def __repr__(self) -> str:
        """å­—ç¬¦ä¸²è¡¨ç¤º"""
        return (
            f"AcademicAgent(\n"
            f"  documents_dir='{self.documents_dir}',\n"
            f"  index_dir='{self.index_dir}',\n"
            f"  documents_loaded={len(self.documents)},\n"
            f"  index_ready={self.index is not None},\n"
            f"  query_engine_ready={self.query_engine is not None}\n"
            f")"
        )
    
    def _build_context_prompt(self, question: str) -> str:
        """
        æ„å»ºå¸¦å†å²ä¸Šä¸‹æ–‡çš„æç¤ºè¯
        
        Args:
            question: å½“å‰é—®é¢˜
            
        Returns:
            åŒ…å«å†å²å¯¹è¯çš„å¢å¼ºæç¤ºè¯
        """
        # è·å–æœ€è¿‘çš„å†å²ï¼ˆæŒ‰é…ç½®çš„æœ€å¤§è½®æ•°ï¼‰
        recent_history = self.chat_history[-(self.max_history_turns * 2):]
        
        # æ„å»ºå¯¹è¯å†å²å­—ç¬¦ä¸²
        history_text = ""
        for msg in recent_history:
            role_name = "ç”¨æˆ·" if msg["role"] == "user" else "åŠ©æ‰‹"
            history_text += f"\n{role_name}: {msg['content']}"
        
        # æ„å»ºæœ€ç»ˆæç¤ºè¯
        prompt = f"""æ ¹æ®ä»¥ä¸‹å¯¹è¯å†å²å’Œå½“å‰é—®é¢˜ï¼Œæä¾›å‡†ç¡®çš„å›ç­”ã€‚

å¯¹è¯å†å²:{history_text}

å½“å‰é—®é¢˜: {question}

è¯·åŸºäºä¸Šä¸‹æ–‡å›ç­”å½“å‰é—®é¢˜ï¼Œå¦‚æœé—®é¢˜ä¸ä¹‹å‰çš„å¯¹è¯ç›¸å…³ï¼Œè¯·ç»“åˆå†å²ä¿¡æ¯å›ç­”ã€‚"""
        
        return prompt
    
    def _update_chat_history(self, user_message: str, assistant_message: str):
        """
        æ›´æ–°å¯¹è¯å†å²
        
        Args:
            user_message: ç”¨æˆ·æ¶ˆæ¯
            assistant_message: åŠ©æ‰‹å›å¤
        """
        self.chat_history.append({"role": "user", "content": user_message})
        self.chat_history.append({"role": "assistant", "content": assistant_message})
        
        # é™åˆ¶å†å²é•¿åº¦
        max_messages = self.max_history_turns * 2
        if len(self.chat_history) > max_messages:
            self.chat_history = self.chat_history[-max_messages:]
        
        logger.debug(f"å¯¹è¯å†å²å·²æ›´æ–°ï¼Œå½“å‰è½®æ•°: {len(self.chat_history) // 2}")
    
    def clear_chat_history(self):
        """æ¸…é™¤å¯¹è¯å†å²"""
        self.chat_history = []
        logger.info("å¯¹è¯å†å²å·²æ¸…é™¤")
    
    def get_chat_history(self) -> List[Dict[str, str]]:
        """
        è·å–å¯¹è¯å†å²
        
        Returns:
            å¯¹è¯å†å²åˆ—è¡¨
        """
        return self.chat_history.copy()
    
    def set_max_history_turns(self, max_turns: int):
        """
        è®¾ç½®æœ€å¤§å†å²è½®æ•°
        
        Args:
            max_turns: æœ€å¤§è½®æ•°
        """
        self.max_history_turns = max_turns
        logger.info(f"æœ€å¤§å†å²è½®æ•°å·²è®¾ç½®ä¸º: {max_turns}")


# ä¾¿æ·å‡½æ•°
def create_agent(
    documents_dir: Optional[str] = None,
    index_dir: Optional[str] = None,
    force_rebuild: bool = False,
) -> AcademicAgent:
    """
    åˆ›å»ºå­¦æœ¯è®ºæ–‡é—®ç­” Agentï¼ˆä¾¿æ·å‡½æ•°ï¼‰
    
    Args:
        documents_dir: æ–‡æ¡£ç›®å½•è·¯å¾„
        index_dir: ç´¢å¼•å­˜å‚¨ç›®å½•è·¯å¾„
        force_rebuild: æ˜¯å¦å¼ºåˆ¶é‡å»ºç´¢å¼•
        
    Returns:
        AcademicAgent å®ä¾‹
    """
    agent = AcademicAgent(
        documents_dir=documents_dir,
        index_dir=index_dir,
        auto_load=False,
    )
    
    agent.load_or_build_index(force_rebuild=force_rebuild)
    
    return agent


__all__ = ['AcademicAgent', 'create_agent']
