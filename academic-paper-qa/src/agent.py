"""
å­¦æœ¯è®ºæ–‡é—®ç­” Agent æ ¸å¿ƒæ¨¡å—

æä¾›åŸºäº LlamaIndex çš„å‘é‡ç´¢å¼•ç®¡ç†å’Œæ™ºèƒ½é—®ç­”åŠŸèƒ½
"""

import os
from pathlib import Path
from typing import Optional, List, Dict, Any
from datetime import datetime

from llama_index.core import (
    VectorStoreIndex,
    StorageContext,
    load_index_from_storage,
    Settings,
)
from llama_index.core.schema import Document
from loguru import logger
from openai import OpenAI

from config import SystemConfig
from src.loaders.document_loader import DocumentLoader
from src.constants import (
    LOG_SEPARATOR_FULL,
    LOG_SEPARATOR_HALF,
    DEFAULT_WEB_SEARCH_RESULTS,
    INDEX_FILE_NAMES,
    ERROR_NO_DOCUMENTS,
    ERROR_INDEX_NOT_INITIALIZED,
    SUCCESS_INDEX_LOADED,
    SUCCESS_INDEX_BUILT,
    SUCCESS_DOCUMENTS_LOADED,
    SUCCESS_QUERY_COMPLETED,
    WARNING_NO_DOCUMENTS_FOUND,
    WARNING_FORCE_REBUILD,
    INFO_LOADING_FROM_DISK,
    INFO_BUILDING_NEW_INDEX,
    INFO_WEB_SEARCH_ENABLED,
    INFO_WEB_SEARCH_RESULTS,
)


class AcademicAgent:
    """
    å­¦æœ¯è®ºæ–‡é—®ç­” Agent
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. å‘é‡ç´¢å¼•ç®¡ç†ï¼ˆæ„å»ºã€åŠ è½½ã€æŒä¹…åŒ–ï¼‰
    2. æ™ºèƒ½é—®ç­”ï¼ˆåŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰
    3. è®ºæ–‡ç®¡ç†ï¼ˆåˆ—å‡ºã€ç»Ÿè®¡ï¼‰
    
    Attributes:
        documents_dir: æ–‡æ¡£ç›®å½•è·¯å¾„
        index_dir: ç´¢å¼•å­˜å‚¨ç›®å½•è·¯å¾„
        index: å‘é‡ç´¢å¼•å®ä¾‹
        query_engine: æŸ¥è¯¢å¼•æ“å®ä¾‹
        documents: å·²åŠ è½½çš„æ–‡æ¡£åˆ—è¡¨
    """
    
    def __init__(
        self,
        documents_dir: Optional[str] = None,
        index_dir: Optional[str] = None,
        auto_load: bool = True,
        max_history_turns: int = 10,
    ):
        """
        åˆå§‹åŒ–å­¦æœ¯è®ºæ–‡é—®ç­” Agent
        
        Args:
            documents_dir: æ–‡æ¡£ç›®å½•è·¯å¾„ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„è·¯å¾„
            index_dir: ç´¢å¼•å­˜å‚¨ç›®å½•è·¯å¾„ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„è·¯å¾„
            auto_load: æ˜¯å¦è‡ªåŠ¨åŠ è½½æˆ–æ„å»ºç´¢å¼•
            max_history_turns: å¤šè½®å¯¹è¯æœ€å¤§ä¿ç•™å†å²è½®æ•°ï¼ˆé»˜è®¤10è½®ï¼Œå³20æ¡æ¶ˆæ¯ï¼‰
        """
        logger.info(LOG_SEPARATOR_FULL)
        logger.info("åˆå§‹åŒ–å­¦æœ¯è®ºæ–‡é—®ç­” Agent")
        logger.info(LOG_SEPARATOR_FULL)
        
        # è®¾ç½®è·¯å¾„
        self.documents_dir = Path(documents_dir or SystemConfig.DOCUMENTS_DIR)
        self.index_dir = Path(index_dir or SystemConfig.VECTOR_STORE_DIR)
        
        logger.info(f"æ–‡æ¡£ç›®å½•: {self.documents_dir}")
        logger.info(f"ç´¢å¼•ç›®å½•: {self.index_dir}")
        
        # åˆå§‹åŒ–å±æ€§
        self.index: Optional[VectorStoreIndex] = None
        self.query_engine = None
        self.documents: List[Document] = []
        
        # å¯¹è¯å†å²ç®¡ç†
        self.chat_history: List[Dict[str, str]] = []  # å­˜å‚¨å¯¹è¯å†å² [{"role": "user/assistant", "content": "..."}]
        self.max_history_turns: int = max_history_turns  # æœ€å¤§ä¿ç•™å†å²è½®æ•°
        
        # æ–‡ä»¶ä¸Šä¼ ç¼“å­˜ï¼ˆç”¨äºå¤šè½®å¯¹è¯ï¼‰
        self._uploaded_files_cache: Dict[str, Dict[str, Any]] = {}  # {file_path: {"id": file_id, "content": file_content}}
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self._ensure_directories()
        
        # è‡ªåŠ¨åŠ è½½ç´¢å¼•
        if auto_load:
            self.load_or_build_index()
        
        logger.info("Agent åˆå§‹åŒ–å®Œæˆ")
    
    def _ensure_directories(self):
        """ç¡®ä¿å¿…è¦çš„ç›®å½•å­˜åœ¨"""
        self.documents_dir.mkdir(parents=True, exist_ok=True)
        self.index_dir.mkdir(parents=True, exist_ok=True)
        logger.debug(f"âœ“ ç›®å½•æ£€æŸ¥å®Œæˆ")
    
    def _index_exists(self) -> bool:
        """
        æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
        
        Returns:
            ç´¢å¼•æ˜¯å¦å­˜åœ¨
        """
        # æ£€æŸ¥ç´¢å¼•ç›®å½•ä¸‹æ˜¯å¦æœ‰å¿…è¦çš„æ–‡ä»¶ï¼ˆLlamaIndex ç´¢å¼•éœ€è¦è¿™ä¸‰ä¸ª JSON æ–‡ä»¶ï¼‰
        required_files = INDEX_FILE_NAMES
        
        for file_name in required_files:
            file_path = self.index_dir / file_name
            if not file_path.exists():
                logger.debug(f"ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨: {file_name}")
                return False
        
        logger.debug("âœ“ ç´¢å¼•æ–‡ä»¶å®Œæ•´")
        return True
    
    def load_or_build_index(self, force_rebuild: bool = False) -> VectorStoreIndex:
        """
        åŠ è½½æˆ–æ„å»ºå‘é‡ç´¢å¼•
        
        å¦‚æœç´¢å¼•å·²å­˜åœ¨ä¸”ä¸å¼ºåˆ¶é‡å»ºï¼Œåˆ™ä»ç£ç›˜åŠ è½½ï¼›å¦åˆ™é‡æ–°æ„å»ºç´¢å¼•ã€‚
        
        Args:
            force_rebuild: æ˜¯å¦å¼ºåˆ¶é‡å»ºç´¢å¼•
            
        Returns:
            å‘é‡ç´¢å¼•å®ä¾‹
        """
        logger.info(LOG_SEPARATOR_HALF)
        logger.info("å¼€å§‹åŠ è½½æˆ–æ„å»ºç´¢å¼•...")
        logger.info(LOG_SEPARATOR_HALF)
        
        if force_rebuild:
            logger.info(WARNING_FORCE_REBUILD)
            return self.rebuild_index()
        
        # å°è¯•åŠ è½½ç°æœ‰ç´¢å¼•
        if self._index_exists():
            try:
                logger.info("æ£€æµ‹åˆ°ç°æœ‰ç´¢å¼•ï¼Œå°è¯•åŠ è½½...")
                return self._load_index()
            except Exception as e:
                logger.warning(f"åŠ è½½ç´¢å¼•å¤±è´¥: {e}")
                logger.info("å°†é‡æ–°æ„å»ºç´¢å¼•...")
                return self.rebuild_index()
        else:
            logger.info(INFO_BUILDING_NEW_INDEX)
            return self.rebuild_index()
    
    def _load_index(self) -> VectorStoreIndex:
        """
        ä»ç£ç›˜åŠ è½½ç´¢å¼•
        
        Returns:
            å‘é‡ç´¢å¼•å®ä¾‹
        """
        start_time = datetime.now()
        
        logger.info(INFO_LOADING_FROM_DISK.format(self.index_dir))
        
        try:
            # åŠ è½½å­˜å‚¨ä¸Šä¸‹æ–‡
            storage_context = StorageContext.from_defaults(
                persist_dir=str(self.index_dir)
            )
            
            # åŠ è½½ç´¢å¼•
            self.index = load_index_from_storage(storage_context)
            
            # åˆ›å»ºæŸ¥è¯¢å¼•æ“
            self._create_query_engine()
            
            # è®¡ç®—åŠ è½½æ—¶é—´
            elapsed = (datetime.now() - start_time).total_seconds()
            
            logger.success(SUCCESS_INDEX_LOADED.format(elapsed))
            
            # å°è¯•è·å–æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯
            try:
                # å°è¯•è·å–ç´¢å¼•ä¸­çš„æ–‡æ¡£å—æ•°é‡ï¼ˆéƒ¨åˆ†ç´¢å¼•ç±»å‹å¯èƒ½ä¸æ”¯æŒï¼‰
                doc_count = len(self.index.docstore.docs)
                logger.info(f"ç´¢å¼•åŒ…å« {doc_count} ä¸ªæ–‡æ¡£å—")
            except Exception:
                # æŸäº›å‘é‡å­˜å‚¨ï¼ˆå¦‚ Qdrantï¼‰å¯èƒ½æ²¡æœ‰ docstore å±æ€§ï¼Œå¿½ç•¥é”™è¯¯
                logger.debug("æ— æ³•è·å–æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯")
            
            return self.index
            
        except Exception as e:
            logger.error(f"âœ— ç´¢å¼•åŠ è½½å¤±è´¥: {e}")
            raise
    
    def rebuild_index(self) -> VectorStoreIndex:
        """
        é‡æ–°æ„å»ºå‘é‡ç´¢å¼•
        
        å®Œæ•´æµç¨‹ï¼š
        1. åŠ è½½æ–‡æ¡£
        2. æ„å»ºå‘é‡ç´¢å¼•
        3. æŒä¹…åŒ–åˆ°ç£ç›˜
        
        Returns:
            å‘é‡ç´¢å¼•å®ä¾‹
        """
        logger.info(LOG_SEPARATOR_FULL)
        logger.info("å¼€å§‹æ„å»ºå‘é‡ç´¢å¼•")
        logger.info(LOG_SEPARATOR_FULL)
        
        start_time = datetime.now()
        
        try:
            # 1. åŠ è½½æ–‡æ¡£
            logger.info("æ­¥éª¤ 1/3: åŠ è½½æ–‡æ¡£")
            self._load_documents()
            
            if not self.documents:
                raise ValueError(ERROR_NO_DOCUMENTS.format(self.documents_dir))
            
            # 2. æ„å»ºç´¢å¼•
            logger.info("æ­¥éª¤ 2/3: æ„å»ºå‘é‡ç´¢å¼•")
            self._build_index()
            
            # 3. æŒä¹…åŒ–ç´¢å¼•
            logger.info("æ­¥éª¤ 3/3: æŒä¹…åŒ–ç´¢å¼•åˆ°ç£ç›˜")
            self._persist_index()
            
            # åˆ›å»ºæŸ¥è¯¢å¼•æ“
            self._create_query_engine()
            
            # è®¡ç®—æ€»è€—æ—¶
            elapsed = (datetime.now() - start_time).total_seconds()
            
            logger.success(LOG_SEPARATOR_FULL)
            logger.success(SUCCESS_INDEX_BUILT.format(elapsed))
            logger.success(LOG_SEPARATOR_FULL)
            
            return self.index
            
        except Exception as e:
            logger.error(f"âœ— ç´¢å¼•æ„å»ºå¤±è´¥: {e}")
            raise
    
    def _load_documents(self):
        """åŠ è½½æ–‡æ¡£"""
        logger.info(f"ä»ç›®å½•åŠ è½½æ–‡æ¡£: {self.documents_dir}")
        
        # ä½¿ç”¨ DocumentLoader åŠ è½½æ–‡æ¡£
        loader = DocumentLoader(
            input_dir=self.documents_dir,
            recursive=True,
            clean_text=True,
            preserve_formatting=True,
        )
        
        # åŠ è½½æ‰€æœ‰æ”¯æŒçš„æ–‡æ¡£æ ¼å¼
        self.documents = loader.load_documents()
        
        if not self.documents:
            logger.warning(WARNING_NO_DOCUMENTS_FOUND.format(self.documents_dir))
            return
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        stats = loader.get_document_stats(self.documents)
        
        logger.success(SUCCESS_DOCUMENTS_LOADED + ":")
        logger.info(f"  - æ€»æ–‡æ¡£æ•°: {stats['total_documents']}")
        logger.info(f"  - æ€»æ–‡ä»¶æ•°: {stats['total_files']}")
        logger.info(f"  - æ–‡ä»¶ç±»å‹: {stats['file_types']}")
        logger.info(f"  - æ€»å¤§å°: {stats['total_size_mb']:.2f} MB")
        logger.info(f"  - æ€»å­—ç¬¦æ•°: {stats['total_chars']:,}")
        logger.info(f"  - æ€»å•è¯æ•°: {stats['total_words']:,}")
    
    def _build_index(self):
        """æ„å»ºå‘é‡ç´¢å¼•"""
        logger.info(f"ä½¿ç”¨ Embedding æä¾›å•†: {os.getenv('EMBEDDING_PROVIDER', 'huggingface')}")
        logger.info(f"Chunk å¤§å°: {SystemConfig.CHUNK_SIZE}, é‡å : {SystemConfig.CHUNK_OVERLAP}")
        
        # ä½¿ç”¨ LlamaIndex Settings ä¸­é…ç½®çš„ Embedding æ¨¡å‹
        # Settings å·²åœ¨ç³»ç»Ÿåˆå§‹åŒ–æ—¶é…ç½®
        
        # æ„å»ºç´¢å¼•
        self.index = VectorStoreIndex.from_documents(
            self.documents,
            show_progress=True,
        )
        
        logger.success(f"âœ“ å‘é‡ç´¢å¼•æ„å»ºå®Œæˆ")
        
        # æ˜¾ç¤ºç´¢å¼•ç»Ÿè®¡
        try:
            doc_count = len(self.index.docstore.docs)
            logger.info(f"ç´¢å¼•åŒ…å« {doc_count} ä¸ªæ–‡æ¡£å—")
        except Exception:
            logger.debug("æ— æ³•è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯")
    
    def _persist_index(self):
        """æŒä¹…åŒ–ç´¢å¼•åˆ°ç£ç›˜"""
        logger.info(f"ä¿å­˜ç´¢å¼•åˆ°: {self.index_dir}")
        
        # æŒä¹…åŒ–
        self.index.storage_context.persist(persist_dir=str(self.index_dir))
        
        logger.success(f"âœ“ ç´¢å¼•å·²ä¿å­˜åˆ°ç£ç›˜")
        
        # æ˜¾ç¤ºä¿å­˜çš„æ–‡ä»¶
        saved_files = list(self.index_dir.glob('*.json'))
        logger.debug(f"ä¿å­˜çš„æ–‡ä»¶: {[f.name for f in saved_files]}")
    
    def _create_query_engine(self):
        """åˆ›å»ºæŸ¥è¯¢å¼•æ“"""
        if not self.index:
            raise ValueError(ERROR_INDEX_NOT_INITIALIZED)
        
        # åˆ›å»ºæŸ¥è¯¢å¼•æ“ï¼Œä½¿ç”¨é…ç½®çš„å‚æ•°
        self.query_engine = self.index.as_query_engine(
            similarity_top_k=SystemConfig.RETRIEVAL_TOP_K,
            streaming=False,
        )
        
        logger.debug(f"âœ“ æŸ¥è¯¢å¼•æ“å·²åˆ›å»º (top_k={SystemConfig.RETRIEVAL_TOP_K})")
    
    def query(
        self,
        question: str,
        top_k: Optional[int] = None,
        verbose: bool = False,
        enable_web_search: bool = None,
        use_history: bool = False,
    ) -> Dict[str, Any]:
        """
        æ‰§è¡ŒæŸ¥è¯¢
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            top_k: æ£€ç´¢çš„ç›¸å…³æ–‡æ¡£æ•°é‡ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®å€¼
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            enable_web_search: æ˜¯å¦å¯ç”¨è”ç½‘æœç´¢ï¼ŒNoneæ—¶ä½¿ç”¨é…ç½®å€¼
            use_history: æ˜¯å¦ä½¿ç”¨å¯¹è¯å†å²
            
        Returns:
            åŒ…å«æŸ¥è¯¢ç»“æœçš„å­—å…¸:
            - answer: ç”Ÿæˆçš„ç­”æ¡ˆ
            - source_nodes: å‚è€ƒçš„æºæ–‡æ¡£èŠ‚ç‚¹ï¼ˆåŒ…å«æ–‡æœ¬ç‰‡æ®µï¼‰
            - web_sources: è”ç½‘æœç´¢ç»“æœï¼ˆå¦‚æœå¯ç”¨ï¼‰
            - metadata: å…ƒæ•°æ®ä¿¡æ¯
        """
        if not self.query_engine:
            raise ValueError("æŸ¥è¯¢å¼•æ“æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆåŠ è½½æˆ–æ„å»ºç´¢å¼•")
        
        logger.info(LOG_SEPARATOR_HALF)
        logger.info(f"é—®é¢˜: {question}")
        logger.info(LOG_SEPARATOR_HALF)
        
        # å¤„ç†å¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡
        enhanced_question = question
        if use_history and self.chat_history:
            # æ„å»ºå¸¦å†å²çš„æç¤ºè¯
            context_prompt = self._build_context_prompt(question)
            enhanced_question = context_prompt
            logger.debug(f"ä½¿ç”¨å¯¹è¯å†å²ï¼Œå½“å‰è½®æ•°: {len(self.chat_history) // 2}ï¼Œæœ€å¤§é™åˆ¶: {self.max_history_turns} è½®")
        
        start_time = datetime.now()
        web_sources = []
        
        try:
            # æ£€æŸ¥æ˜¯å¦å¯ç”¨è”ç½‘æœç´¢
            if enable_web_search is None:
                enable_web_search = SystemConfig.ENABLE_WEB_SEARCH
            
            # æ‰§è¡Œè”ç½‘æœç´¢ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if enable_web_search:
                try:
                    from src.tools.web_search import WebSearchTool
                    logger.info("ğŸŒ æ­£åœ¨è¿›è¡Œè”ç½‘æœç´¢...")
                    web_tool = WebSearchTool(max_results=3)
                    web_sources = web_tool.search(question)
                    
                    if web_sources:
                        logger.info(f"âœ“ æ‰¾åˆ° {len(web_sources)} ä¸ªç½‘ç»œèµ„æº:")
                        for i, source in enumerate(web_sources, 1):
                            logger.info(f"  [{i}] {source['url']}")
                        
                        # å°†æœç´¢ç»“æœæ·»åŠ åˆ°æŸ¥è¯¢ä¸­
                        web_context = "\n\n".join([
                            f"æ¥æº [{i+1}]: {s['title']}\n{s['snippet']}\nç½‘å€: {s['url']}"
                            for i, s in enumerate(web_sources)
                        ])
                        enhanced_question = f"{enhanced_question}\n\nå‚è€ƒä»¥ä¸‹ç½‘ç»œæœç´¢ç»“æœ:\n{web_context}"
                        logger.debug(f"å·²å°† {len(web_sources)} ä¸ªæœç´¢ç»“æœæ·»åŠ åˆ°æŸ¥è¯¢ä¸Šä¸‹æ–‡")
                        
                        # æ‰“å°åŠ ä¸Šç½‘ç»œæœç´¢ç»“æœåè¾“å…¥ç»™æ¨¡å‹çš„å®Œæ•´å†…å®¹
                        logger.info("\n" + "="*70)
                        logger.info("ã€RAGæ¨¡å¼ã€‘è¾“å…¥ç»™æ¨¡å‹çš„å®Œæ•´æŸ¥è¯¢å†…å®¹:")
                        logger.info("="*70)
                        logger.info(enhanced_question)
                        logger.info("="*70 + "\n")
                    else:
                        logger.warning("âš  æœªæ‰¾åˆ°ç›¸å…³ç½‘ç»œèµ„æº")
                except Exception as e:
                    logger.warning(f"è”ç½‘æœç´¢å¤±è´¥: {e}")
            
            # å¦‚æœæŒ‡å®šäº† top_kï¼Œé‡æ–°åˆ›å»ºæŸ¥è¯¢å¼•æ“
            if top_k and top_k != SystemConfig.RETRIEVAL_TOP_K:
                logger.debug(f"ä½¿ç”¨è‡ªå®šä¹‰ top_k: {top_k}")
                query_engine = self.index.as_query_engine(
                    similarity_top_k=top_k,
                    streaming=False,
                )
            else:
                query_engine = self.query_engine
            
            # æ‰§è¡ŒRAGæŸ¥è¯¢
            logger.info("æ­£åœ¨æ£€ç´¢ç›¸å…³æ–‡æ¡£å¹¶ç”Ÿæˆå›ç­”...")
            response = query_engine.query(enhanced_question)
            
            # è®¡ç®—è€—æ—¶
            elapsed = (datetime.now() - start_time).total_seconds()
            
            # æå–ç­”æ¡ˆ
            answer = str(response)
            
            # æå–æºèŠ‚ç‚¹ï¼ˆåŒ…å«æ–‡æœ¬ç‰‡æ®µï¼‰
            source_nodes = []
            if hasattr(response, 'source_nodes'):
                source_nodes = response.source_nodes
            
            logger.success(f"âœ“ æŸ¥è¯¢å®Œæˆï¼è€—æ—¶: {elapsed:.2f} ç§’")
            
            if verbose:
                logger.info(f"\nå›ç­”:\n{answer}\n")
                
                if source_nodes:
                    logger.info(f"å‚è€ƒäº† {len(source_nodes)} ä¸ªæ–‡æ¡£ç‰‡æ®µ:")
                    for i, node in enumerate(source_nodes, 1):
                        score = node.score if hasattr(node, 'score') else 'N/A'
                        file_name = node.metadata.get('file_name', 'Unknown')
                        text_preview = node.text[:100].replace('\n', ' ') if hasattr(node, 'text') else 'N/A'
                        logger.info(f"  [{i}] {file_name} (ç›¸ä¼¼åº¦: {score})")
                        logger.info(f"      ç‰‡æ®µ: {text_preview}...")
            
            # æ›´æ–°å¯¹è¯å†å²
            if use_history:
                self._update_chat_history(question, answer)
            
            # æ„å»ºç»“æœ
            result = {
                'answer': answer,
                'source_nodes': source_nodes,
                'web_sources': web_sources,
                'metadata': {
                    'question': question,
                    'elapsed_time': elapsed,
                    'num_sources': len(source_nodes),
                    'num_web_sources': len(web_sources),
                    'top_k': top_k or SystemConfig.RETRIEVAL_TOP_K,
                    'web_search_enabled': enable_web_search,
                    'use_history': use_history,
                    'history_turns': len(self.chat_history) // 2,
                }
            }
            
            return result
            
        except Exception as e:
            logger.error(f"âœ— æŸ¥è¯¢å¤±è´¥: {e}")
            raise
    
    def _upload_files_to_moonshot(self, file_paths: List[str], use_cache: bool = True) -> List[Dict[str, Any]]:
        """
        ä¸Šä¼ æ–‡ä»¶åˆ° Moonshot API
        
        Args:
            file_paths: æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼ˆå¤šè½®å¯¹è¯æ—¶é¿å…é‡å¤ä¸Šä¼ ï¼‰
            
        Returns:
            ä¸Šä¼ çš„æ–‡ä»¶å¯¹è±¡åˆ—è¡¨
        """
        api_key = os.getenv("LLM_API_KEY")
        api_base = os.getenv("LLM_API_BASE", "https://api.moonshot.cn/v1")
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ Moonshot API
        if "moonshot" not in api_base.lower():
            logger.warning("å½“å‰ API ä¸æ˜¯ Moonshotï¼Œæ— æ³•ä½¿ç”¨æ–‡ä»¶ä¸Šä¼ åŠŸèƒ½")
            return []
        
        client = OpenAI(api_key=api_key, base_url=api_base)
        uploaded_files = []
        
        for file_path in file_paths:
            try:
                # å¤„ç†è·¯å¾„
                path = Path(file_path)
                if not path.is_absolute():
                    path = self.documents_dir / path
                
                path_str = str(path)
                
                if not path.exists():
                    logger.warning(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {path}")
                    continue
                
                # æ£€æŸ¥ç¼“å­˜
                if use_cache and path_str in self._uploaded_files_cache:
                    cached_file = self._uploaded_files_cache[path_str]
                    uploaded_files.append(cached_file)
                    logger.info(f"â™»ï¸ ä½¿ç”¨ç¼“å­˜çš„æ–‡ä»¶: {path.name} (ID: {cached_file['id']})")
                    continue
                
                logger.info(f"ğŸ“¤ æ­£åœ¨ä¸Šä¼ æ–‡ä»¶åˆ° Moonshot: {path.name}")
                
                # ä¸Šä¼ æ–‡ä»¶
                file_object = client.files.create(
                    file=path,
                    purpose="file-extract"
                )
                
                file_data = {
                    'id': file_object.id,
                    'filename': file_object.filename,
                    'path': path_str
                }
                
                uploaded_files.append(file_data)
                
                # ç¼“å­˜æ–‡ä»¶ä¿¡æ¯
                if use_cache:
                    self._uploaded_files_cache[path_str] = file_data
                
                logger.info(f"âœ… æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {path.name} (ID: {file_object.id})")
                
            except Exception as e:
                logger.error(f"âœ— æ–‡ä»¶ä¸Šä¼ å¤±è´¥ {file_path}: {e}")
                continue
        
        return uploaded_files
    
    def query_direct(
        self,
        question: str,
        context: Optional[str] = None,
        enable_web_search: bool = None,
        document_files: Optional[List[str]] = None,
    ) -> Dict[str, Any]:
        """
        ç›´æ¥æŸ¥è¯¢LLMï¼ˆä¸ä½¿ç”¨å‘é‡åº“ï¼‰
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            context: å¯é€‰çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆç”¨æˆ·æä¾›çš„æ–‡æœ¬ï¼‰
            enable_web_search: æ˜¯å¦å¯ç”¨è”ç½‘æœç´¢
            document_files: å¯é€‰çš„æ–‡æ¡£æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼Œä½œä¸ºé™„ä»¶å‘é€ç»™LLM
            
        Returns:
            åŒ…å«æŸ¥è¯¢ç»“æœçš„å­—å…¸:
            - answer: LLMç”Ÿæˆçš„ç­”æ¡ˆ
            - web_sources: è”ç½‘æœç´¢ç»“æœï¼ˆå¦‚æœå¯ç”¨ï¼‰
            - document_sources: ä½¿ç”¨çš„æ–‡æ¡£æ–‡ä»¶åˆ—è¡¨
            - metadata: å…ƒæ•°æ®ä¿¡æ¯
        """
        logger.info(LOG_SEPARATOR_HALF)
        logger.info(f"é—®é¢˜ (ç›´æ¥LLMæ¨¡å¼): {question}")
        logger.info(LOG_SEPARATOR_HALF)
        
        start_time = datetime.now()
        web_sources = []
        document_sources = []
        uploaded_file_ids = []
        
        try:
            # å¤„ç†æ–‡æ¡£é™„ä»¶ - ä½¿ç”¨ Moonshot æ–‡ä»¶ä¸Šä¼ 
            if document_files:
                logger.info(f"ğŸ“ å‡†å¤‡å¤„ç† {len(document_files)} ä¸ªæ–‡æ¡£é™„ä»¶...")
                
                # å°è¯•ä¸Šä¼ åˆ° Moonshot
                uploaded_files = self._upload_files_to_moonshot(document_files)
                
                if uploaded_files:
                    document_sources = document_files
                    uploaded_file_ids = [f['id'] for f in uploaded_files]
                    logger.info(f"âœ… æˆåŠŸä¸Šä¼  {len(uploaded_files)} ä¸ªæ–‡ä»¶åˆ° Moonshot")
                else:
                    logger.warning("âš ï¸ æ–‡ä»¶ä¸Šä¼ å¤±è´¥ï¼Œå°†å›é€€åˆ°æ–‡æœ¬æå–æ–¹å¼")
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦è”ç½‘æœç´¢ï¼ˆNone æ—¶ä½¿ç”¨é…ç½®çš„é»˜è®¤å€¼ï¼‰
            if enable_web_search is None:
                enable_web_search = SystemConfig.ENABLE_WEB_SEARCH
            
            # æ‰§è¡Œè”ç½‘æœç´¢ä»¥è¡¥å……LLMçŸ¥è¯†
            if enable_web_search:
                try:
                    # åŠ¨æ€å¯¼å…¥é¿å…å¯åŠ¨æ—¶ä¾èµ–
                    from src.tools.web_search import WebSearchTool
                    logger.info(INFO_WEB_SEARCH_ENABLED)
                    # é™åˆ¶è¿”å›ç»“æœæ•°é¿å…ä¸Šä¸‹æ–‡è¿‡é•¿
                    web_tool = WebSearchTool(max_results=DEFAULT_WEB_SEARCH_RESULTS)
                    web_sources = web_tool.search(question)
                    
                    if web_sources:
                        logger.info(INFO_WEB_SEARCH_RESULTS.format(len(web_sources)) + ":")
                        for i, source in enumerate(web_sources, 1):
                            logger.info(f"  [{i}] {source['url']}")
                except Exception as e:
                    logger.warning(f"è”ç½‘æœç´¢å¤±è´¥: {e}")
            
            # æ„å»ºæç¤ºè¯
            prompt_parts = []
            
            # æ·»åŠ ç”¨æˆ·æä¾›çš„ä¸Šä¸‹æ–‡
            if context:
                prompt_parts.append(f"è¡¥å……ä¸Šä¸‹æ–‡:\n{context}")
            
            # æ·»åŠ ç½‘ç»œæœç´¢ç»“æœ
            if web_sources:
                web_context = "\n\n".join([
                    f"æ¥æº [{i+1}]: {s['title']}\n{s['snippet']}\nç½‘å€: {s['url']}"
                    for i, s in enumerate(web_sources)
                ])
                prompt_parts.append(f"ç½‘ç»œæœç´¢ç»“æœ:\n{web_context}")
            
            # æ„å»ºå®Œæ•´æç¤ºè¯
            if prompt_parts:
                combined_parts = "\n\n" + ("="*50 + "\n\n").join(prompt_parts)
                prompt = combined_parts + "\n\n" + "="*50 + f"\n\né—®é¢˜: {question}"
            else:
                prompt = question
            
            # è°ƒç”¨LLMï¼ˆå¸¦æ–‡ä»¶ä¸Šä¼ ï¼‰
            logger.info("æ­£åœ¨è°ƒç”¨LLMç”Ÿæˆå›ç­”...")
            logger.debug(f"Prompt åŒ…å« {len(prompt_parts)} ä¸ªéƒ¨åˆ†ï¼Œæ€»é•¿åº¦: {len(prompt)} å­—ç¬¦")
            
            if uploaded_file_ids:
                # ä½¿ç”¨ OpenAI SDK ç›´æ¥è°ƒç”¨ï¼Œæ”¯æŒæ–‡ä»¶é™„ä»¶
                api_key = os.getenv("LLM_API_KEY")
                api_base = os.getenv("LLM_API_BASE")
                model = os.getenv("LLM_MODEL", "moonshot-v1-8k")
                
                client = OpenAI(api_key=api_key, base_url=api_base)
                
                # è·å–æ–‡ä»¶å†…å®¹ - æ¯ä¸ªæ–‡ä»¶ä½œä¸ºç‹¬ç«‹çš„ system æ¶ˆæ¯
                logger.info(f"ğŸ“¥ æ­£åœ¨è·å– {len(uploaded_file_ids)} ä¸ªæ–‡ä»¶çš„å†…å®¹...")
                file_messages = []
                for i, file_id in enumerate(uploaded_file_ids):
                    try:
                        file_content = client.files.content(file_id=file_id).text
                        file_messages.append({
                            "role": "system",
                            "content": file_content
                        })
                        logger.info(f"âœ… æˆåŠŸè·å–æ–‡ä»¶å†…å®¹ [{i+1}/{len(uploaded_file_ids)}] (ID: {file_id}ï¼Œé•¿åº¦: {len(file_content)} å­—ç¬¦)")
                    except Exception as e:
                        logger.error(f"âœ— è·å–æ–‡ä»¶å†…å®¹å¤±è´¥ (ID: {file_id}): {e}")
                        continue
                
                if not file_messages:
                    logger.warning("âš ï¸ æ— æ³•è·å–ä»»ä½•æ–‡ä»¶å†…å®¹ï¼Œå°†ä½¿ç”¨æ ‡å‡†æ–¹å¼è°ƒç”¨")
                    # å›é€€åˆ°æ ‡å‡†æ–¹å¼
                    from llama_index.core import Settings
                    llm = Settings.llm
                    response = llm.complete(prompt)
                    answer = str(response)
                else:
                    # æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼šä½¿ç”¨ * è¯­æ³•è§£æ„ file_messagesï¼Œä½¿å…¶æˆä¸º messages åˆ—è¡¨çš„å‰ N æ¡æ¶ˆæ¯
                    messages = [
                        *file_messages,  # è§£æ„æ‰€æœ‰æ–‡ä»¶å†…å®¹æ¶ˆæ¯
                        {
                            "role": "system",
                            "content": "ä½ æ˜¯ Kimiï¼Œç”± Moonshot AI æä¾›çš„äººå·¥æ™ºèƒ½åŠ©æ‰‹ã€‚ä½ ä¼šä¸ºç”¨æˆ·æä¾›å®‰å…¨ï¼Œæœ‰å¸®åŠ©ï¼Œå‡†ç¡®çš„å›ç­”ã€‚åŒæ—¶ï¼Œä½ ä¼šæ‹’ç»ä¸€åˆ‡æ¶‰åŠææ€–ä¸»ä¹‰ï¼Œç§æ—æ­§è§†ï¼Œé»„è‰²æš´åŠ›ç­‰é—®é¢˜çš„å›ç­”ã€‚"
                        },
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ]
                    
                    logger.debug(f"å‘é€æ¶ˆæ¯åˆ° Moonshot APIï¼ŒåŒ…å« {len(file_messages)} ä¸ªæ–‡ä»¶å†…å®¹")
                    
                    completion = client.chat.completions.create(
                        model=model,
                        messages=messages,
                        temperature=0.3,
                    )
                    
                    answer = completion.choices[0].message.content
            else:
                # æ²¡æœ‰æ–‡ä»¶ä¸Šä¼ ï¼Œç›´æ¥è°ƒç”¨ LLM API
                api_key = os.getenv("LLM_API_KEY")
                api_base = os.getenv("LLM_API_BASE")
                model = os.getenv("LLM_MODEL", "moonshot-v1-8k")
                
                client = OpenAI(api_key=api_key, base_url=api_base)
                
                messages = [
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯ Kimiï¼Œç”± Moonshot AI æä¾›çš„äººå·¥æ™ºèƒ½åŠ©æ‰‹ã€‚ä½ ä¼šä¸ºç”¨æˆ·æä¾›å®‰å…¨ï¼Œæœ‰å¸®åŠ©ï¼Œå‡†ç¡®çš„å›ç­”ã€‚"
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ]
                
                completion = client.chat.completions.create(
                    model=model,
                    messages=messages,
                    temperature=0.3,
                )
                
                answer = completion.choices[0].message.content
            
            # è®¡ç®—è€—æ—¶
            elapsed = (datetime.now() - start_time).total_seconds()
            
            logger.success(SUCCESS_QUERY_COMPLETED.format(elapsed))
            
            # æ„å»ºç»“æœ
            result = {
                'answer': answer,
                'source_nodes': [],  # ç›´æ¥æ¨¡å¼æ²¡æœ‰æºèŠ‚ç‚¹
                'web_sources': web_sources,
                'document_sources': document_sources,
                'metadata': {
                    'question': question,
                    'elapsed_time': elapsed,
                    'num_sources': 0,
                    'num_web_sources': len(web_sources),
                    'num_document_sources': len(document_sources),
                    'mode': 'direct_llm',
                    'has_context': bool(context),
                    'has_documents': bool(document_sources),
                    'web_search_enabled': enable_web_search,
                    'uploaded_files': len(uploaded_file_ids),
                }
            }
            
            return result
            
        except Exception as e:
            logger.error(f"âœ— ç›´æ¥æŸ¥è¯¢å¤±è´¥: {e}", exc_info=True)
            raise
    
    def list_papers(self, detailed: bool = False) -> List[Dict[str, Any]]:
        """
        åˆ—å‡ºæ‰€æœ‰å·²åŠ è½½çš„è®ºæ–‡
        
        Args:
            detailed: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            
        Returns:
            è®ºæ–‡åˆ—è¡¨ï¼Œæ¯ä¸ªè®ºæ–‡åŒ…å«å…ƒæ•°æ®ä¿¡æ¯
        """
        logger.info("-" * 70)
        logger.info("å·²åŠ è½½çš„è®ºæ–‡åˆ—è¡¨")
        logger.info("-" * 70)
        
        if not self.documents:
            logger.warning("âš  æœªåŠ è½½ä»»ä½•æ–‡æ¡£")
            return []
        
        # æŒ‰æ–‡ä»¶ååˆ†ç»„æ–‡æ¡£
        papers_dict: Dict[str, Dict[str, Any]] = {}
        
        for doc in self.documents:
            file_name = doc.metadata.get('file_name', 'Unknown')
            
            if file_name not in papers_dict:
                papers_dict[file_name] = {
                    'file_name': file_name,
                    'file_path': doc.metadata.get('file_path', 'Unknown'),
                    'file_type': doc.metadata.get('file_type', 'Unknown'),
                    'file_size_mb': doc.metadata.get('file_size_mb', 0),
                    'page_count': 1,
                    'total_chars': len(doc.text),
                    'created_time': doc.metadata.get('created_time', 'Unknown'),
                    'modified_time': doc.metadata.get('modified_time', 'Unknown'),
                }
            else:
                # å¦‚æœæ˜¯ PDF çš„å¤šé¡µï¼Œç´¯åŠ ä¿¡æ¯
                papers_dict[file_name]['page_count'] += 1
                papers_dict[file_name]['total_chars'] += len(doc.text)
        
        # è½¬æ¢ä¸ºåˆ—è¡¨
        papers = list(papers_dict.values())
        
        # æ’åº
        papers.sort(key=lambda x: x['file_name'])
        
        # æ˜¾ç¤ºåˆ—è¡¨
        logger.info(f"æ€»è®¡: {len(papers)} ä¸ªè®ºæ–‡æ–‡ä»¶\n")
        
        for i, paper in enumerate(papers, 1):
            logger.info(f"[{i}] {paper['file_name']}")
            if detailed:
                logger.info(f"    ç±»å‹: {paper['file_type']}")
                logger.info(f"    å¤§å°: {paper['file_size_mb']:.2f} MB")
                if paper['file_type'] == 'pdf':
                    logger.info(f"    é¡µæ•°: {paper['page_count']}")
                logger.info(f"    å­—ç¬¦æ•°: {paper['total_chars']:,}")
                logger.info(f"    è·¯å¾„: {paper['file_path']}")
                logger.info("")
        
        return papers
    
    def _load_document_files(self, file_paths: List[str]) -> str:
        """
        è¯»å–æ–‡æ¡£æ–‡ä»¶å†…å®¹ï¼Œç”¨äºLLMç›´æ¥æ¨¡å¼
        
        Args:
            file_paths: æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ˆç›¸å¯¹äºdocuments_diræˆ–ç»å¯¹è·¯å¾„ï¼‰
            
        Returns:
            åˆå¹¶åçš„æ–‡æ¡£å†…å®¹å­—ç¬¦ä¸²
        """
        from src.loaders.document_loader import DocumentLoader
        
        all_content = []
        
        for file_path in file_paths:
            try:
                # å¤„ç†è·¯å¾„ï¼ˆæ”¯æŒç›¸å¯¹å’Œç»å¯¹è·¯å¾„ï¼‰
                path = Path(file_path)
                if not path.is_absolute():
                    path = self.documents_dir / path
                
                if not path.exists():
                    logger.warning(f"âš  æ–‡ä»¶ä¸å­˜åœ¨: {path}")
                    continue
                
                logger.debug(f"è¯»å–æ–‡ä»¶: {path.name}")
                
                # ä½¿ç”¨ DocumentLoader è¯»å–æ–‡ä»¶
                loader = DocumentLoader(
                    input_dir=path.parent,
                    recursive=False,
                    clean_text=True,
                )
                
                # æ ¹æ®æ–‡ä»¶ç±»å‹åŠ è½½
                file_ext = path.suffix.lower()
                if file_ext == '.pdf':
                    docs = loader._load_pdf_files([path])
                elif file_ext in ['.docx', '.doc']:
                    docs = loader._load_docx_files([path])
                elif file_ext == '.md':
                    docs = loader._load_markdown_files([path])
                elif file_ext == '.txt':
                    docs = loader._load_text_files([path])
                else:
                    logger.warning(f"âš  ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}")
                    continue
                
                # åˆå¹¶æ–‡æ¡£å†…å®¹
                if docs:
                    file_content = "\n\n".join([doc.text for doc in docs])
                    # æ·»åŠ æ–‡ä»¶æ ‡è¯†
                    all_content.append(f"=== æ–‡ä»¶: {path.name} ===\n\n{file_content}")
                    logger.info(f"âœ… æˆåŠŸè¯»å– {path.name}, å­—ç¬¦æ•°: {len(file_content)}")
                else:
                    logger.warning(f"âš ï¸ æ–‡ä»¶ä¸ºç©º: {path.name}")
                
            except Exception as e:
                logger.error(f"âœ— è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}", exc_info=True)
                continue
        
        if not all_content:
            logger.warning("âš ï¸ æ²¡æœ‰æˆåŠŸè¯»å–ä»»ä½•æ–‡æ¡£")
            return ""
        
        # åˆå¹¶æ‰€æœ‰å†…å®¹
        final_content = "\n\n" + ("="*80 + "\n\n").join(all_content)
        logger.info(f"ğŸ“š æ€»å…±è¯»å– {len(all_content)} ä¸ªæ–‡æ¡£ï¼Œæ€»å­—ç¬¦æ•°: {len(final_content)}")
        return final_content
    
    def list_available_documents(self) -> List[str]:
        """
        åˆ—å‡º documents ç›®å½•ä¸‹çš„æ‰€æœ‰å¯ç”¨æ–‡æ¡£
        
        Returns:
            æ–‡æ¡£æ–‡ä»¶ååˆ—è¡¨
        """
        if not self.documents_dir.exists():
            return []
        
        supported_exts = ['.pdf', '.docx', '.doc', '.md', '.txt']
        files = []
        
        for ext in supported_exts:
            files.extend([f.name for f in self.documents_dir.rglob(f'*{ext}')])
        
        return sorted(files)
    
    def get_statistics(self) -> Dict[str, Any]:
        """
        è·å– Agent ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        stats = {
            'documents_dir': str(self.documents_dir),
            'index_dir': str(self.index_dir),
            'index_exists': self._index_exists(),
            'documents_loaded': len(self.documents),
            'query_engine_ready': self.query_engine is not None,
        }
        
        # å¦‚æœæœ‰ç´¢å¼•ï¼Œæ·»åŠ ç´¢å¼•ç»Ÿè®¡
        if self.index:
            try:
                stats['index_doc_count'] = len(self.index.docstore.docs)
            except Exception:
                stats['index_doc_count'] = 'N/A'
        
        return stats
    
    def __repr__(self) -> str:
        """å­—ç¬¦ä¸²è¡¨ç¤º"""
        return (
            f"AcademicAgent(\n"
            f"  documents_dir='{self.documents_dir}',\n"
            f"  index_dir='{self.index_dir}',\n"
            f"  documents_loaded={len(self.documents)},\n"
            f"  index_ready={self.index is not None},\n"
            f"  query_engine_ready={self.query_engine is not None}\n"
            f")"
        )
    
    def _build_context_prompt(self, question: str) -> str:
        """
        æ„å»ºå¸¦å†å²ä¸Šä¸‹æ–‡çš„æç¤ºè¯
        
        Args:
            question: å½“å‰é—®é¢˜
            
        Returns:
            åŒ…å«å†å²å¯¹è¯çš„å¢å¼ºæç¤ºè¯
        """
        # è·å–æœ€è¿‘çš„å†å²ï¼ˆæŒ‰é…ç½®çš„æœ€å¤§è½®æ•°ï¼‰
        recent_history = self.chat_history[-(self.max_history_turns * 2):]
        
        # æ„å»ºå¯¹è¯å†å²å­—ç¬¦ä¸²
        history_text = ""
        for msg in recent_history:
            role_name = "ç”¨æˆ·" if msg["role"] == "user" else "åŠ©æ‰‹"
            history_text += f"\n{role_name}: {msg['content']}"
        
        # æ„å»ºæœ€ç»ˆæç¤ºè¯
        prompt = f"""æ ¹æ®ä»¥ä¸‹å¯¹è¯å†å²å’Œå½“å‰é—®é¢˜ï¼Œæä¾›å‡†ç¡®çš„å›ç­”ã€‚

å¯¹è¯å†å²:{history_text}

å½“å‰é—®é¢˜: {question}

è¯·åŸºäºä¸Šä¸‹æ–‡å›ç­”å½“å‰é—®é¢˜ï¼Œå¦‚æœé—®é¢˜ä¸ä¹‹å‰çš„å¯¹è¯ç›¸å…³ï¼Œè¯·ç»“åˆå†å²ä¿¡æ¯å›ç­”ã€‚"""
        
        return prompt
    
    def _update_chat_history(self, user_message: str, assistant_message: str):
        """
        æ›´æ–°å¯¹è¯å†å²
        
        Args:
            user_message: ç”¨æˆ·æ¶ˆæ¯
            assistant_message: åŠ©æ‰‹å›å¤
        """
        self.chat_history.append({"role": "user", "content": user_message})
        self.chat_history.append({"role": "assistant", "content": assistant_message})
        
        # é™åˆ¶å†å²é•¿åº¦
        max_messages = self.max_history_turns * 2
        if len(self.chat_history) > max_messages:
            removed_turns = (len(self.chat_history) - max_messages) // 2
            self.chat_history = self.chat_history[-max_messages:]
            logger.debug(f"å†å²è¶…å‡ºé™åˆ¶ï¼Œå·²ç§»é™¤æœ€æ—©çš„ {removed_turns} è½®å¯¹è¯")
        
        logger.debug(f"å¯¹è¯å†å²å·²æ›´æ–°ï¼Œå½“å‰è½®æ•°: {len(self.chat_history) // 2}/{self.max_history_turns}")
    
    def clear_chat_history(self):
        """æ¸…é™¤å¯¹è¯å†å²"""
        self.chat_history = []
        logger.info("å¯¹è¯å†å²å·²æ¸…é™¤")
    
    def set_max_history_turns(self, max_turns: int):
        """
        åŠ¨æ€è®¾ç½®æœ€å¤§å†å²è½®æ•°
        
        Args:
            max_turns: æœ€å¤§ä¿ç•™å†å²è½®æ•°ï¼ˆå¿…é¡» >= 1ï¼‰
        
        Example:
            >>> agent.set_max_history_turns(50)  # ä¿ç•™æœ€è¿‘50è½®å¯¹è¯
            >>> agent.set_max_history_turns(5)   # åªä¿ç•™æœ€è¿‘5è½®
        """
        if max_turns < 1:
            raise ValueError(f"max_turns å¿…é¡» >= 1ï¼Œå½“å‰å€¼: {max_turns}")
        
        old_value = self.max_history_turns
        self.max_history_turns = max_turns
        
        # å¦‚æœæ–°é™åˆ¶æ›´å°ï¼Œç«‹å³è£å‰ªå†å²
        max_messages = max_turns * 2
        if len(self.chat_history) > max_messages:
            self.chat_history = self.chat_history[-max_messages:]
            logger.info(f"å†å²è½®æ•°é™åˆ¶å·²æ›´æ–°: {old_value} -> {max_turns}ï¼Œå†å²å·²è£å‰ªè‡³ {len(self.chat_history) // 2} è½®")
        else:
            logger.info(f"å†å²è½®æ•°é™åˆ¶å·²æ›´æ–°: {old_value} -> {max_turns}")
    
    def get_chat_history_info(self) -> Dict[str, Any]:
        """
        è·å–å¯¹è¯å†å²ä¿¡æ¯
        
        Returns:
            åŒ…å«å½“å‰è½®æ•°ã€æœ€å¤§é™åˆ¶ã€æ¶ˆæ¯æ•°çš„å­—å…¸
        
        Example:
            >>> info = agent.get_chat_history_info()
            >>> print(f"å½“å‰ {info['current_turns']}/{info['max_turns']} è½®")
        """
        return {
            'current_turns': len(self.chat_history) // 2,
            'max_turns': self.max_history_turns,
            'total_messages': len(self.chat_history),
            'is_full': len(self.chat_history) >= self.max_history_turns * 2
        }
    
    def clear_file_cache(self):
        """æ¸…é™¤æ–‡ä»¶ä¸Šä¼ ç¼“å­˜"""
        self._uploaded_files_cache = {}
        logger.info("æ–‡ä»¶ä¸Šä¼ ç¼“å­˜å·²æ¸…é™¤")
    
    def get_chat_history(self) -> List[Dict[str, str]]:
        """
        è·å–å¯¹è¯å†å²
        
        Returns:
            å¯¹è¯å†å²åˆ—è¡¨
        """
        return self.chat_history.copy()
    
    def set_max_history_turns(self, max_turns: int):
        """
        è®¾ç½®æœ€å¤§å†å²è½®æ•°
        
        Args:
            max_turns: æœ€å¤§è½®æ•°
        """
        self.max_history_turns = max_turns
        logger.info(f"æœ€å¤§å†å²è½®æ•°å·²è®¾ç½®ä¸º: {max_turns}")


# ä¾¿æ·å‡½æ•°
def create_agent(
    documents_dir: Optional[str] = None,
    index_dir: Optional[str] = None,
    force_rebuild: bool = False,
) -> AcademicAgent:
    """
    åˆ›å»ºå­¦æœ¯è®ºæ–‡é—®ç­” Agentï¼ˆä¾¿æ·å‡½æ•°ï¼‰
    
    Args:
        documents_dir: æ–‡æ¡£ç›®å½•è·¯å¾„
        index_dir: ç´¢å¼•å­˜å‚¨ç›®å½•è·¯å¾„
        force_rebuild: æ˜¯å¦å¼ºåˆ¶é‡å»ºç´¢å¼•
        
    Returns:
        AcademicAgent å®ä¾‹
    """
    agent = AcademicAgent(
        documents_dir=documents_dir,
        index_dir=index_dir,
        auto_load=False,
    )
    
    agent.load_or_build_index(force_rebuild=force_rebuild)
    
    return agent


__all__ = ['AcademicAgent', 'create_agent']
