"""
LLM å’Œ Embedding æ¨¡å‹é…ç½®æ¨¡å—
æ”¯æŒå¤šç§ä¸‰æ–¹ LLM APIï¼ˆDeepSeekã€OpenAIã€Qwen ç­‰ï¼‰åŠæœ¬åœ°æ¨¡å‹
ä½¿ç”¨ Pydantic è¿›è¡Œé…ç½®éªŒè¯
"""

import os
from typing import Optional
from llama_index.core.embeddings import BaseEmbedding
from llama_index.core.llms import LLM
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.llms.openai import OpenAI
from loguru import logger

from src.models.config import LLMConfig, EmbeddingConfig


def get_llm(
    api_key: Optional[str] = None,
    api_base: Optional[str] = None,
    model: Optional[str] = None,
    temperature: Optional[float] = None,
) -> LLM:
    """
    è·å– LLM å®ä¾‹ï¼ˆä½¿ç”¨ Pydantic éªŒè¯é…ç½®ï¼‰
    æ”¯æŒ OpenAIã€DeepSeekã€Qwen ç­‰å…¼å®¹ OpenAI API çš„æœåŠ¡
    
    Args:
        api_key: API Keyï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
        api_base: API Base URLï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
        model: æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
        temperature: æ¸©åº¦å‚æ•°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
        
    Returns:
        LLM å®ä¾‹
    """
    # ä»ç¯å¢ƒå˜é‡è·å–é…ç½®
    api_key = api_key or os.getenv("LLM_API_KEY")
    api_base = api_base or os.getenv("LLM_API_BASE", "https://api.openai.com/v1")
    model = model or os.getenv("LLM_MODEL", "gpt-3.5-turbo")
    temperature = temperature if temperature is not None else float(os.getenv("TEMPERATURE", "0.1"))
    
    if not api_key:
        raise ValueError(
            "LLM_API_KEY æœªè®¾ç½®ï¼Œè¯·åœ¨ .env æ–‡ä»¶ä¸­é…ç½®\n"
            "ç¤ºä¾‹é…ç½®ï¼š\n"
            "  # OpenAI\n"
            "  LLM_API_KEY=sk-...\n"
            "  LLM_API_BASE=https://api.openai.com/v1\n"
            "  LLM_MODEL=gpt-3.5-turbo\n\n"
            "  # DeepSeek\n"
            "  LLM_API_KEY=sk-...\n"
            "  LLM_API_BASE=https://api.deepseek.com/v1\n"
            "  LLM_MODEL=deepseek-chat\n\n"
            "  # Qwen\n"
            "  LLM_API_KEY=sk-...\n"
            "  LLM_API_BASE=https://dashscope.aliyuncs.com/compatible-mode/v1\n"
            "  LLM_MODEL=qwen-turbo\n"
        )
    
    # ä½¿ç”¨ Pydantic éªŒè¯é…ç½®
    try:
        llm_config = LLMConfig(
            api_key=api_key,
            api_base=api_base,
            model=model,
            temperature=temperature,
        )
    except Exception as e:
        raise ValueError(f"LLM é…ç½®éªŒè¯å¤±è´¥: {e}")
    
    # åˆ¤æ–­æ˜¯å¦æ˜¯ OpenAI å®˜æ–¹ API
    if "api.openai.com" in llm_config.api_base:
        logger.info(f"ğŸ¤– ä½¿ç”¨ OpenAI å®˜æ–¹ API: {llm_config.model}")
        return OpenAI(
            api_key=llm_config.api_key,
            api_base=llm_config.api_base,
            model=llm_config.model,
            temperature=llm_config.temperature,
        )
    else:
        # ä½¿ç”¨ OpenAILike é€‚é…å…¶ä»– OpenAI å…¼å®¹çš„ API
        try:
            from llama_index.llms.openai_like import OpenAILike
            
            logger.info(f"ğŸ¤– ä½¿ç”¨ OpenAI å…¼å®¹ API: {llm_config.model} (Base: {llm_config.api_base})")
            return OpenAILike(
                api_key=llm_config.api_key,
                api_base=llm_config.api_base,
                model=llm_config.model,
                temperature=llm_config.temperature,
                is_chat_model=True,
            )
        except Exception as e:
            # å¦‚æœ OpenAILike å¯¼å…¥å¤±è´¥ï¼Œå›é€€åˆ° OpenAI ç±»
            logger.warning(f"OpenAILike å¯¼å…¥å¤±è´¥: {e}")
            logger.warning(f"å›é€€ä½¿ç”¨ OpenAI ç±»ï¼ˆå¯èƒ½å­˜åœ¨å…¼å®¹æ€§é—®é¢˜ï¼‰")
            return OpenAI(
                api_key=llm_config.api_key,
                api_base=llm_config.api_base,
                model=llm_config.model,
                temperature=llm_config.temperature,
            )


def get_embedding_model(
    provider: Optional[str] = None,
    model_name: Optional[str] = None,
    api_key: Optional[str] = None,
) -> BaseEmbedding:
    """
    è·å– Embedding æ¨¡å‹å®ä¾‹ï¼ˆä½¿ç”¨ Pydantic éªŒè¯é…ç½®ï¼‰
    
    Args:
        provider: Embedding æä¾›å•†ï¼ˆopenai, huggingfaceï¼‰
        model_name: æ¨¡å‹åç§°
        api_key: API å¯†é’¥ï¼ˆOpenAI éœ€è¦ï¼‰
        
    Returns:
        Embedding æ¨¡å‹å®ä¾‹
    """
    # ä»ç¯å¢ƒå˜é‡è·å–é…ç½®
    provider = provider or os.getenv("EMBEDDING_PROVIDER", "huggingface")
    model_name = model_name or os.getenv("EMBEDDING_MODEL") or os.getenv("EMBEDDING_MODEL_NAME", "BAAI/bge-small-zh-v1.5")
    api_key = api_key or os.getenv("EMBEDDING_API_KEY") or os.getenv("LLM_API_KEY")
    
    # ä½¿ç”¨ Pydantic éªŒè¯é…ç½®
    try:
        embedding_config = EmbeddingConfig(
            provider=provider,
            model_name=model_name,
            api_key=api_key,
        )
    except Exception as e:
        raise ValueError(f"Embedding é…ç½®éªŒè¯å¤±è´¥: {e}")
    
    logger.info(f"ğŸ“š Embedding æä¾›å•†: {embedding_config.provider}")
    
    if embedding_config.provider == "openai":
        # ä½¿ç”¨ OpenAI Embedding
        if not embedding_config.api_key:
            raise ValueError("ä½¿ç”¨ OpenAI Embedding æ—¶å¿…é¡»æä¾› API Key")
        
        logger.info(f"  æ¨¡å‹: {embedding_config.model_name}")
        return OpenAIEmbedding(
            api_key=embedding_config.api_key,
            model=embedding_config.model_name
        )
    
    elif embedding_config.provider == "huggingface":
        # ä½¿ç”¨ HuggingFace Embedding
        from llama_index.embeddings.huggingface import HuggingFaceEmbedding
        
        logger.info(f"  æ¨¡å‹: {embedding_config.model_name}")
        return HuggingFaceEmbedding(
            model_name=embedding_config.model_name,
            embed_batch_size=embedding_config.embed_batch_size,
        )
    
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ Embedding æä¾›å•†: {embedding_config.provider}")
