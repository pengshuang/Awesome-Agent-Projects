"""
æ–‡ä»¶æ•°æ®æºé€‚é…å™¨ (ä½¿ç”¨ Pydantic æ¨¡å‹)
æ”¯æŒ CSV, Excel, JSON ç­‰æ ¼å¼
"""

from pathlib import Path
from typing import Any, Dict, Optional
import time
import pandas as pd
from loguru import logger

from .base import DataSource
from src.models.datasource import QueryResponse, QueryMetadata


class FileDataSource(DataSource):
    """æ–‡ä»¶æ•°æ®æº"""
    
    SUPPORTED_FORMATS = {
        '.csv': 'csv',
        '.xlsx': 'excel',
        '.xls': 'excel',
        '.json': 'json',
        '.parquet': 'parquet',
        '.txt': 'text',
    }
    
    def __init__(self, name: str, file_path: str):
        """
        åˆå§‹åŒ–æ–‡ä»¶æ•°æ®æº
        
        Args:
            name: æ•°æ®æºåç§°
            file_path: æ–‡ä»¶è·¯å¾„
        """
        super().__init__(name, "file")
        self.file_path = Path(file_path)
        self.data: Optional[pd.DataFrame] = None
        self.file_format: Optional[str] = None
        
    def connect(self) -> bool:
        """åŠ è½½æ–‡ä»¶"""
        try:
            if not self.file_path.exists():
                logger.error(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {self.file_path}")
                return False
            
            # åˆ¤æ–­æ–‡ä»¶æ ¼å¼
            suffix = self.file_path.suffix.lower()
            self.file_format = self.SUPPORTED_FORMATS.get(suffix)
            
            if not self.file_format:
                logger.error(f"âŒ ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {suffix}")
                return False
            
            # åŠ è½½æ–‡ä»¶
            logger.info(f"ğŸ“ æ­£åœ¨åŠ è½½æ–‡ä»¶: {self.file_path}")
            
            if self.file_format == 'csv':
                self.data = pd.read_csv(self.file_path)
            elif self.file_format == 'excel':
                self.data = pd.read_excel(self.file_path)
            elif self.file_format == 'json':
                self.data = pd.read_json(self.file_path)
            elif self.file_format == 'parquet':
                self.data = pd.read_parquet(self.file_path)
            elif self.file_format == 'text':
                with open(self.file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                self.data = pd.DataFrame({'content': [content]})
            
            logger.info(f"âœ… æ–‡ä»¶åŠ è½½æˆåŠŸ: {len(self.data)} è¡Œ x {len(self.data.columns)} åˆ—")
            return True
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥: {e}")
            return False
    
    def query(self, query: str, **kwargs) -> QueryResponse:
        """
        æŸ¥è¯¢æ•°æ® (è¿”å› Pydantic éªŒè¯çš„ç»“æœ)
        
        Args:
            query: æŸ¥è¯¢æè¿°ï¼ˆå¯ä»¥æ˜¯pandas queryè¯­æ³•æˆ–è‡ªç„¶è¯­è¨€æè¿°ï¼‰
            **kwargs: é¢å¤–å‚æ•°
                - limit: é™åˆ¶è¿”å›è¡Œæ•°
                - columns: æŒ‡å®šè¿”å›åˆ—
            
        Returns:
            QueryResponse: Pydantic éªŒè¯çš„æŸ¥è¯¢ç»“æœ
        """
        start_time = time.time()
        
        if self.data is None:
            return QueryResponse(
                success=False,
                data=None,
                error="æ–‡ä»¶æœªåŠ è½½",
                metadata=QueryMetadata(
                    row_count=0,
                    execution_time=0.0,
                    data_source_type="file",
                    columns=[],
                )
            )
        
        try:
            result_df = self.data.copy()
            warnings = []
            
            # å°è¯•ä½œä¸ºpandas queryæ‰§è¡Œ
            try:
                if query and query.strip():
                    result_df = self.data.query(query)
                    logger.info(f"âœ… æ‰§è¡Œpandas query: {query}")
            except Exception as e:
                # å¦‚æœä¸æ˜¯æœ‰æ•ˆçš„pandas queryï¼Œè¿”å›å…¨éƒ¨æ•°æ®
                logger.debug(f"Queryä¸æ˜¯æœ‰æ•ˆçš„pandasè¡¨è¾¾å¼ï¼Œè¿”å›å…¨éƒ¨æ•°æ®: {e}")
                warnings.append(f"Queryä¸æ˜¯æœ‰æ•ˆçš„pandasè¡¨è¾¾å¼: {str(e)}")
            
            # åº”ç”¨åˆ—ç­›é€‰
            if 'columns' in kwargs and kwargs['columns']:
                columns = kwargs['columns']
                if isinstance(columns, str):
                    columns = [columns]
                result_df = result_df[columns]
            
            # åº”ç”¨è¡Œæ•°é™åˆ¶
            limit = kwargs.get('limit', None)
            if limit:
                result_df = result_df.head(limit)
            
            # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
            data = result_df.to_dict('records')
            execution_time = time.time() - start_time
            
            logger.info(f"âœ… æŸ¥è¯¢æˆåŠŸï¼Œè¿”å› {len(data)} æ¡è®°å½•")
            
            return QueryResponse(
                success=True,
                data=data,
                error=None,
                metadata=QueryMetadata(
                    row_count=len(data),
                    total_rows=len(self.data),
                    columns=list(result_df.columns),
                    execution_time=execution_time,
                    data_source_type="file",
                    file_format=self.file_format,
                    file_size=self.file_path.stat().st_size if self.file_path.exists() else None,
                ),
                warnings=warnings,
            )
            
        except Exception as e:
            error_msg = f"æŸ¥è¯¢å¤±è´¥: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            execution_time = time.time() - start_time
            
            return QueryResponse(
                success=False,
                data=None,
                error=error_msg,
                metadata=QueryMetadata(
                    row_count=0,
                    columns=[],
                    execution_time=execution_time,
                    data_source_type="file",
                ),
            )
    
    def get_schema(self) -> Optional[str]:
        """
        è·å–æ–‡ä»¶æ•°æ®çš„schema
        
        Returns:
            Schemaæè¿°å­—ç¬¦ä¸²
        """
        if self.data is None:
            return None
        
        try:
            schema_parts = []
            
            schema_parts.append(f"æ–‡ä»¶: {self.file_path.name}")
            schema_parts.append(f"æ ¼å¼: {self.file_format}")
            schema_parts.append(f"å¤§å°: {len(self.data)} è¡Œ x {len(self.data.columns)} åˆ—")
            schema_parts.append("\nåˆ—ä¿¡æ¯:")
            schema_parts.append("-" * 50)
            
            # åˆ—ä¿¡æ¯
            for col in self.data.columns:
                dtype = self.data[col].dtype
                null_count = self.data[col].isnull().sum()
                unique_count = self.data[col].nunique()
                
                schema_parts.append(
                    f"  {col}: {dtype} "
                    f"(ç©ºå€¼: {null_count}, å”¯ä¸€å€¼: {unique_count})"
                )
            
            # å‰å‡ è¡Œæ ·ä¾‹æ•°æ®
            schema_parts.append("\næ ·ä¾‹æ•°æ® (å‰3è¡Œ):")
            schema_parts.append("-" * 50)
            sample_data = self.data.head(3).to_string(index=False)
            schema_parts.append(sample_data)
            
            return "\n".join(schema_parts)
            
        except Exception as e:
            logger.error(f"âŒ è·å–schemaå¤±è´¥: {e}")
            return None
    
    def get_dataframe(self) -> Optional[pd.DataFrame]:
        """è·å–åŸå§‹DataFrame"""
        return self.data
    
    def close(self):
        """æ¸…ç†èµ„æº"""
        self.data = None
        logger.info(f"ğŸ”’ å·²é‡Šæ”¾æ–‡ä»¶æ•°æ®æº: {self.name}")
