# ğŸ‘¨â€ğŸ’» å¼€å‘è€…æŒ‡å—

> é¢å‘éœ€è¦äºŒæ¬¡å¼€å‘å’ŒåŠŸèƒ½æ‰©å±•çš„å¼€å‘è€…

## ç›®å½•

- [é¡¹ç›®ç»“æ„](#é¡¹ç›®ç»“æ„)
- [æ ¸å¿ƒæ¨¡å—è¯¦è§£](#æ ¸å¿ƒæ¨¡å—è¯¦è§£)
- [é…ç½®ç³»ç»Ÿ](#é…ç½®ç³»ç»Ÿ)
- [æ‰©å±•å¼€å‘](#æ‰©å±•å¼€å‘)
- [æµ‹è¯•ä¸è°ƒè¯•](#æµ‹è¯•ä¸è°ƒè¯•)
- [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)

---

## é¡¹ç›®ç»“æ„

```
paper-qa-system/
â”œâ”€â”€ config/                 # é…ç½®æ¨¡å—
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ models.py          # Pydantic é…ç½®æ¨¡å‹
â”‚   â”œâ”€â”€ llm_config.py      # LLM å’Œ Embedding é…ç½®
â”‚   â”œâ”€â”€ settings.py        # å…¨å±€è®¾ç½®
â”‚   â””â”€â”€ prompts.py         # Prompt æ¨¡æ¿
â”‚
â”œâ”€â”€ src/                   # æ ¸å¿ƒä»£ç 
â”‚   â”œâ”€â”€ agent.py           # Agent æ ¸å¿ƒé€»è¾‘
â”‚   â”œâ”€â”€ models.py          # æ•°æ®æ¨¡å‹
â”‚   â”œâ”€â”€ constants.py       # å¸¸é‡å®šä¹‰
â”‚   â”‚
â”‚   â”œâ”€â”€ indexing/          # ç´¢å¼•æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ indexer.py     # ç´¢å¼•æ„å»º
â”‚   â”‚   â””â”€â”€ vector_store.py # å‘é‡å­˜å‚¨
â”‚   â”‚
â”‚   â”œâ”€â”€ query/             # æŸ¥è¯¢æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ qa_engine.py   # é—®ç­”å¼•æ“
â”‚   â”‚   â””â”€â”€ rag_pipeline.py # RAG æµç¨‹
â”‚   â”‚
â”‚   â”œâ”€â”€ loaders/           # æ–‡æ¡£åŠ è½½
â”‚   â”‚   â””â”€â”€ document_loader.py
â”‚   â”‚
â”‚   â”œâ”€â”€ tools/             # å·¥å…·æ¨¡å—
â”‚   â”‚   â””â”€â”€ web_search.py  # ç½‘ç»œæœç´¢
â”‚   â”‚
â”‚   â””â”€â”€ utils/             # å·¥å…·å‡½æ•°
â”‚       â”œâ”€â”€ helpers.py
â”‚       â””â”€â”€ logger.py
â”‚
â”œâ”€â”€ data/                  # æ•°æ®ç›®å½•
â”‚   â”œâ”€â”€ documents/         # åŸå§‹æ–‡æ¡£
â”‚   â”œâ”€â”€ vector_store/      # å‘é‡å­˜å‚¨
â”‚   â””â”€â”€ processed/         # å¤„ç†åçš„æ•°æ®
â”‚
â”œâ”€â”€ examples/              # ç¤ºä¾‹ä»£ç 
â”‚   â”œâ”€â”€ quick_start.py     # å¿«é€Ÿå¼€å§‹
â”‚   â”œâ”€â”€ agent_demo.py      # Agent ç¤ºä¾‹
â”‚   â”œâ”€â”€ build_index.py     # ç´¢å¼•æ„å»ºç¤ºä¾‹
â”‚   â””â”€â”€ history_control_demo.py # å†å²ç®¡ç†ç¤ºä¾‹
â”‚
â”œâ”€â”€ tests/                 # æµ‹è¯•ä»£ç 
â”‚   â”œâ”€â”€ test_agent_core.py
â”‚   â”œâ”€â”€ test_loader_documents.py
â”‚   â””â”€â”€ test_multi_turn_chat.py
â”‚
â”œâ”€â”€ docs/                  # æ–‡æ¡£
â”‚   â”œâ”€â”€ USER_GUIDE.md      # ç”¨æˆ·æŒ‡å—
â”‚   â”œâ”€â”€ DEVELOPER_GUIDE.md # å¼€å‘è€…æŒ‡å—
â”‚   â””â”€â”€ ARCHITECTURE.md    # æ¶æ„æ–‡æ¡£
â”‚
â”œâ”€â”€ web_ui_multi_turn.py   # Web UIï¼ˆå¤šè½®ï¼‰
â”œâ”€â”€ start_web_multi.sh     # å¯åŠ¨è„šæœ¬
â”œâ”€â”€ init_system.py         # ç³»ç»Ÿåˆå§‹åŒ–
â”œâ”€â”€ requirements.txt       # ä¾èµ–åˆ—è¡¨
â”œâ”€â”€ .env.example           # é…ç½®æ¨¡æ¿
â””â”€â”€ README.md              # é¡¹ç›®è¯´æ˜
```

---

## æ ¸å¿ƒæ¨¡å—è¯¦è§£

### 1. Agent æ¨¡å— (`src/agent.py`)

Agent æ˜¯ç³»ç»Ÿçš„æ ¸å¿ƒï¼Œè´Ÿè´£å¯¹è¯ç®¡ç†ã€æŸ¥è¯¢è·¯ç”±å’Œå·¥å…·è°ƒç”¨ã€‚

#### æ ¸å¿ƒç±»ï¼š`AcademicAgent`

```python
class AcademicAgent:
    """å­¦æœ¯è®ºæ–‡æ™ºèƒ½é—®ç­” Agent
    
    åŠŸèƒ½ï¼š
    - å¤šè½®å¯¹è¯ç®¡ç†
    - RAG æŸ¥è¯¢
    - å·¥å…·è°ƒç”¨ï¼ˆç½‘ç»œæœç´¢ï¼‰
    - æµå¼è¾“å‡º
    """
    
    def __init__(
        self,
        documents_dir: str = "data/documents",
        index_dir: str = "data/vector_store",
        auto_load: bool = True,
        max_history_turns: int = 10,
        enable_web_search: bool = True
    ):
        """åˆå§‹åŒ– Agent
        
        Args:
            documents_dir: æ–‡æ¡£ç›®å½•
            index_dir: ç´¢å¼•ç›®å½•
            auto_load: æ˜¯å¦è‡ªåŠ¨åŠ è½½ç´¢å¼•
            max_history_turns: æœ€å¤§å†å²è½®æ•°
            enable_web_search: æ˜¯å¦å¯ç”¨ç½‘ç»œæœç´¢
        """
```

#### å…³é”®æ–¹æ³•

**1. æ„å»ºç´¢å¼•**

```python
def build_index(
    self, 
    force_rebuild: bool = False
) -> Dict[str, Any]:
    """æ„å»ºæ–‡æ¡£ç´¢å¼•
    
    Args:
        force_rebuild: æ˜¯å¦å¼ºåˆ¶é‡å»ºï¼ˆåˆ é™¤å·²æœ‰ç´¢å¼•ï¼‰
        
    Returns:
        {
            "status": "success" | "error",
            "message": "æ„å»ºæˆåŠŸ" | "é”™è¯¯ä¿¡æ¯",
            "stats": {
                "documents": 10,      # æ–‡æ¡£æ•°é‡
                "chunks": 150,        # åˆ†å—æ•°é‡
                "time": 12.5         # è€—æ—¶ï¼ˆç§’ï¼‰
            }
        }
    """
```

**2. å¤šè½®å¯¹è¯**

```python
def chat(
    self,
    question: str,
    stream: bool = False,
    **kwargs
) -> Union[str, Generator[str, None, None]]:
    """å¤šè½®å¯¹è¯ï¼ˆå¸¦ä¸Šä¸‹æ–‡ï¼‰
    
    Args:
        question: ç”¨æˆ·é—®é¢˜
        stream: æ˜¯å¦æµå¼è¾“å‡º
        **kwargs: å…¶ä»–å‚æ•°ï¼ˆtop_k, threshold ç­‰ï¼‰
        
    Returns:
        - stream=False: å®Œæ•´å›ç­”å­—ç¬¦ä¸²
        - stream=True: ç”Ÿæˆå™¨ï¼Œé€å­—è¿”å›
    """
```

**3. å•æ¬¡æŸ¥è¯¢**

```python
def query(
    self,
    question: str,
    use_rag: bool = True,
    **kwargs
) -> str:
    """å•æ¬¡æŸ¥è¯¢ï¼ˆæ— ä¸Šä¸‹æ–‡ï¼‰
    
    Args:
        question: é—®é¢˜
        use_rag: æ˜¯å¦ä½¿ç”¨ RAGï¼ˆFalse åˆ™ç›´æ¥è°ƒç”¨ LLMï¼‰
        
    Returns:
        ç­”æ¡ˆå­—ç¬¦ä¸²
    """
```

**4. å†å²ç®¡ç†**

```python
def get_chat_history(self) -> List[Dict[str, str]]:
    """è·å–å¯¹è¯å†å²"""
    return self.chat_history

def clear_history(self):
    """æ¸…ç©ºå¯¹è¯å†å²"""
    self.chat_history = []
    logger.info("å¯¹è¯å†å²å·²æ¸…ç©º")

def set_max_history_turns(self, turns: int):
    """è®¾ç½®æœ€å¤§å†å²è½®æ•°"""
    self.max_history_turns = max(0, turns)
```

#### å†…éƒ¨é€»è¾‘

**å¯¹è¯å†å²ç®¡ç†**ï¼š

```python
# æ·»åŠ æ–°å¯¹è¯
self.chat_history.append({
    "role": "user",
    "content": question
})
self.chat_history.append({
    "role": "assistant",
    "content": response
})

# ä¿æŒæœ€è¿‘ N è½®
if len(self.chat_history) > self.max_history_turns * 2:
    self.chat_history = self.chat_history[-(self.max_history_turns * 2):]
```

**æŸ¥è¯¢è·¯ç”±**ï¼š

```python
# 1. åˆ¤æ–­æ˜¯å¦éœ€è¦ç½‘ç»œæœç´¢
if self._need_web_search(question):
    web_results = self.web_search_tool.search(question)
    # æ•´åˆç½‘ç»œä¿¡æ¯
    
# 2. RAG æ£€ç´¢
documents = self.query_engine.retrieve(question)

# 3. æ„å»ºä¸Šä¸‹æ–‡
context = self._build_context(documents, web_results)

# 4. LLM ç”Ÿæˆ
response = self.llm.generate(context + question)
```

---

### 2. ç´¢å¼•æ¨¡å— (`src/indexing/`)

è´Ÿè´£æ–‡æ¡£åŠ è½½ã€åˆ†å—ã€å‘é‡åŒ–å’Œå­˜å‚¨ã€‚

#### æ ¸å¿ƒç±»ï¼š`Indexer`

```python
from src.indexing import Indexer

indexer = Indexer(
    documents_dir="data/documents",
    index_dir="data/vector_store"
)

# æ„å»ºç´¢å¼•
indexer.build_index(force_rebuild=False)

# åŠ è½½ç´¢å¼•
index = indexer.load_index()
```

#### ç´¢å¼•æµç¨‹

```python
def build_index(self, force_rebuild: bool = False):
    """ç´¢å¼•æ„å»ºæµç¨‹
    
    1. åŠ è½½æ–‡æ¡£
    2. æ–‡æœ¬åˆ†å—
    3. å‘é‡åŒ–
    4. å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
    """
    
    # 1. åŠ è½½æ–‡æ¡£
    documents = self.load_documents_from_directory(self.documents_dir)
    
    # 2. åˆ›å»ºå‘é‡å­˜å‚¨
    storage_context = self.vector_store.get_storage_context()
    
    # 3. æ„å»ºç´¢å¼•
    index = VectorStoreIndex.from_documents(
        documents,
        storage_context=storage_context,
        show_progress=True
    )
    
    # 4. æŒä¹…åŒ–
    index.storage_context.persist(persist_dir=self.index_dir)
```

#### å‘é‡å­˜å‚¨é…ç½®

```python
# config/models.py
class VectorStoreConfig(BaseSettings):
    store_type: Literal["chroma", "faiss", "simple"] = "chroma"
    collection_name: str = "papers"
    persist_dir: str = "data/vector_store"
```

---

### 3. æ–‡æ¡£åŠ è½½æ¨¡å— (`src/loaders/`)

æ”¯æŒå¤šç§æ–‡æ¡£æ ¼å¼çš„åŠ è½½å’Œè§£æã€‚

#### æ ¸å¿ƒç±»ï¼š`DocumentLoader`

```python
from src.loaders import DocumentLoader

loader = DocumentLoader(
    recursive=True,           # é€’å½’åŠ è½½å­ç›®å½•
    clean_text=True,         # æ¸…æ´—æ–‡æœ¬
    preserve_formatting=True  # ä¿ç•™æ ¼å¼
)

# åŠ è½½å•ä¸ªæ–‡ä»¶
documents = loader.load_single_document("paper.pdf")

# åŠ è½½æ•´ä¸ªç›®å½•
documents = loader.load_documents("data/documents/")
```

#### æ”¯æŒçš„æ ¼å¼

| æ ¼å¼ | è¯»å–å™¨ | ä¾èµ–åº“ |
|------|--------|--------|
| **PDF** | `PDFReader` | PyMuPDF, PyPDF2 |
| **DOCX** | `DOCXReader` | python-docx |
| **TXT/MD** | `SimpleDirectoryReader` | å†…ç½® |

#### æ‰©å±•æ–°æ ¼å¼

```python
from llama_index.core.readers import BaseReader
from llama_index.core.schema import Document

class CustomReader(BaseReader):
    """è‡ªå®šä¹‰æ–‡æ¡£è¯»å–å™¨"""
    
    def load_data(self, file_path: Path) -> List[Document]:
        # å®ç°ä½ çš„åŠ è½½é€»è¾‘
        text = self._load_custom_format(file_path)
        
        return [Document(
            text=text,
            metadata={
                "file_name": file_path.name,
                "file_path": str(file_path)
            }
        )]

# æ³¨å†Œåˆ° DocumentLoader
loader.readers['.custom'] = CustomReader()
```

---

### 4. æŸ¥è¯¢æ¨¡å— (`src/query/`)

å®ç° RAG æŸ¥è¯¢å’Œç­”æ¡ˆç”Ÿæˆã€‚

#### RAG Pipeline

```python
from src.query import RAGPipeline

pipeline = RAGPipeline(
    index=index,
    llm=llm,
    top_k=5,
    similarity_threshold=0.7
)

# æŸ¥è¯¢
response = pipeline.query("Transformer æ˜¯ä»€ä¹ˆï¼Ÿ")
print(response.response)

# æŸ¥çœ‹æ£€ç´¢åˆ°çš„æ–‡æ¡£
for source in response.source_nodes:
    print(f"æ¥æºï¼š{source.metadata['file_name']} ç¬¬ {source.metadata['page']} é¡µ")
    print(f"ç›¸ä¼¼åº¦ï¼š{source.score}")
```

#### è‡ªå®šä¹‰ Prompt

```python
# config/prompts.py

CHAT_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡åˆ†æåŠ©æ‰‹ã€‚

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context}

å¯¹è¯å†å²ï¼š
{history}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·åŸºäºä¸Šä¸‹æ–‡å’Œå†å²å¯¹è¯ï¼Œç»™å‡ºå‡†ç¡®ã€ä¸“ä¸šçš„å›ç­”ã€‚å¦‚æœæ— æ³•ä»ä¸Šä¸‹æ–‡ä¸­æ‰¾åˆ°ç­”æ¡ˆï¼Œè¯·è¯šå®åœ°è¯´æ˜ã€‚
"""

# ä½¿ç”¨è‡ªå®šä¹‰ Prompt
from config.prompts import CHAT_PROMPT

agent = AcademicAgent()
agent.prompt_template = CHAT_PROMPT
```

---

## é…ç½®ç³»ç»Ÿ

æœ¬é¡¹ç›®ä½¿ç”¨ Pydantic 2.0 è¿›è¡Œé…ç½®ç®¡ç†ï¼Œæä¾›ç±»å‹å®‰å…¨å’Œè‡ªåŠ¨éªŒè¯ã€‚

### é…ç½®æ¨¡å‹

#### LLM é…ç½®

```python
# config/models.py

class LLMConfig(BaseSettings):
    """LLM é…ç½®æ¨¡å‹"""
    
    model_config = SettingsConfigDict(
        env_prefix="LLM_",           # ç¯å¢ƒå˜é‡å‰ç¼€
        env_file=".env",             # é…ç½®æ–‡ä»¶
        env_file_encoding="utf-8",
        extra="ignore"               # å¿½ç•¥é¢å¤–å­—æ®µ
    )
    
    api_key: str = Field(..., description="API Key")
    api_base: str = Field(
        default="https://api.openai.com/v1",
        description="API Base URL"
    )
    model: str = Field(
        default="gpt-3.5-turbo",
        description="æ¨¡å‹åç§°"
    )
    temperature: float = Field(
        default=0.1,
        ge=0.0,
        le=2.0,
        description="æ¸©åº¦å‚æ•°"
    )
    
    @field_validator("api_base")
    @classmethod
    def validate_api_base(cls, v: str) -> str:
        if not v.startswith(("http://", "https://")):
            raise ValueError("API Base å¿…é¡»ä»¥ http:// æˆ– https:// å¼€å¤´")
        return v.rstrip("/")
```

#### Embedding é…ç½®

```python
class EmbeddingConfig(BaseSettings):
    """Embedding é…ç½®æ¨¡å‹"""
    
    provider: Literal["openai", "huggingface", "fastembed", "qwen3"] = Field(
        default="huggingface",
        description="æä¾›å•†"
    )
    model_name: str = Field(
        default="BAAI/bge-small-zh-v1.5",
        description="æ¨¡å‹åç§°"
    )
    api_key: Optional[str] = Field(
        default=None,
        description="API Keyï¼ˆäº‘ç«¯ Embedding éœ€è¦ï¼‰"
    )
```

### è·å–é…ç½®

```python
from config.models import get_config

# è·å–é…ç½®å®ä¾‹
config = get_config()

# è®¿é—®é…ç½®
print(config.llm.api_key)
print(config.embedding.provider)
print(config.rag.chunk_size)

# é…ç½®ä¼šè‡ªåŠ¨ä» .env å’Œç¯å¢ƒå˜é‡åŠ è½½
```

### åŠ¨æ€ä¿®æ”¹é…ç½®

```python
# ä¸´æ—¶ä¿®æ”¹ï¼ˆä¸æŒä¹…åŒ–ï¼‰
config.rag.chunk_size = 1024

# é‡æ–°åŠ è½½é…ç½®
from config.models import reload_config
config = reload_config()
```

---

## æ‰©å±•å¼€å‘

### æ·»åŠ æ–°çš„ Embedding æä¾›å•†

**æ­¥éª¤ 1**ï¼šæ›´æ–°é…ç½®æ¨¡å‹

```python
# config/models.py

class EmbeddingConfig(BaseSettings):
    provider: Literal["openai", "huggingface", "fastembed", "qwen3", "custom"] = ...
```

**æ­¥éª¤ 2**ï¼šå®ç° Embedding é€»è¾‘

```python
# config/llm_config.py

def get_embedding_model(provider: Optional[str] = None) -> BaseEmbedding:
    config = get_config()
    provider = provider or config.embedding.provider
    
    # ... å…¶ä»– provider çš„ä»£ç  ...
    
    elif provider == "custom":
        from llama_index.embeddings.custom import CustomEmbedding
        
        logger.info("ä½¿ç”¨è‡ªå®šä¹‰ Embedding")
        return CustomEmbedding(
            api_key=config.embedding.api_key,
            model=config.embedding.model_name
        )
```

**æ­¥éª¤ 3**ï¼šæ›´æ–°é…ç½®æ–‡ä»¶

```bash
# .env
EMBEDDING_PROVIDER=custom
EMBEDDING_MODEL_NAME=your-model-name
EMBEDDING_API_KEY=your-api-key
```

### æ·»åŠ æ–°çš„å·¥å…·ï¼ˆToolï¼‰

**æ­¥éª¤ 1**ï¼šåˆ›å»ºå·¥å…·ç±»

```python
# src/tools/calculator.py

from llama_index.core.tools import FunctionTool

def calculator(expression: str) -> float:
    """è®¡ç®—æ•°å­¦è¡¨è¾¾å¼
    
    Args:
        expression: æ•°å­¦è¡¨è¾¾å¼ï¼Œå¦‚ "2 + 3 * 4"
        
    Returns:
        è®¡ç®—ç»“æœ
    """
    try:
        return eval(expression)
    except Exception as e:
        return f"è®¡ç®—é”™è¯¯ï¼š{e}"

# åŒ…è£…ä¸º Tool
calculator_tool = FunctionTool.from_defaults(
    fn=calculator,
    name="calculator",
    description="è®¡ç®—æ•°å­¦è¡¨è¾¾å¼"
)
```

**æ­¥éª¤ 2**ï¼šæ³¨å†Œåˆ° Agent

```python
# src/agent.py

from src.tools.calculator import calculator_tool

class AcademicAgent:
    def __init__(self, ...):
        # ... å…¶ä»–åˆå§‹åŒ– ...
        
        # æ³¨å†Œå·¥å…·
        self.tools = [
            self.web_search_tool,
            calculator_tool
        ]
        
    def _call_tool(self, tool_name: str, **kwargs):
        """è°ƒç”¨å·¥å…·"""
        for tool in self.tools:
            if tool.metadata.name == tool_name:
                return tool(**kwargs)
```

### è‡ªå®šä¹‰ RAG æµç¨‹

```python
from llama_index.core import VectorStoreIndex
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.postprocessor import SimilarityPostprocessor

def create_custom_query_engine(index, top_k=5, threshold=0.7):
    """è‡ªå®šä¹‰æŸ¥è¯¢å¼•æ“"""
    
    # 1. é…ç½®æ£€ç´¢å™¨
    retriever = VectorIndexRetriever(
        index=index,
        similarity_top_k=top_k,
    )
    
    # 2. é…ç½®åå¤„ç†å™¨
    postprocessor = SimilarityPostprocessor(
        similarity_cutoff=threshold
    )
    
    # 3. ç»„è£…æŸ¥è¯¢å¼•æ“
    query_engine = RetrieverQueryEngine(
        retriever=retriever,
        node_postprocessors=[postprocessor],
    )
    
    return query_engine
```

---

## æµ‹è¯•ä¸è°ƒè¯•

### è¿è¡Œæµ‹è¯•

```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
pytest tests/

# è¿è¡Œç‰¹å®šæµ‹è¯•æ–‡ä»¶
pytest tests/test_agent_core.py

# è¿è¡Œç‰¹å®šæµ‹è¯•å‡½æ•°
pytest tests/test_agent_core.py::test_agent_initialization

# æ˜¾ç¤ºè¯¦ç»†è¾“å‡º
pytest -v tests/

# æ˜¾ç¤º print è¾“å‡º
pytest -s tests/
```

### ç¼–å†™æµ‹è¯•

```python
# tests/test_my_feature.py

import pytest
from src.agent import AcademicAgent

@pytest.fixture
def agent():
    """åˆ›å»ºæµ‹è¯•ç”¨ Agent"""
    return AcademicAgent(
        documents_dir="tests/data/documents",
        index_dir="tests/data/index",
        auto_load=False
    )

def test_agent_initialization(agent):
    """æµ‹è¯• Agent åˆå§‹åŒ–"""
    assert agent is not None
    assert agent.documents_dir.exists()

def test_build_index(agent):
    """æµ‹è¯•ç´¢å¼•æ„å»º"""
    result = agent.build_index()
    assert result["status"] == "success"
    assert result["stats"]["documents"] > 0
```

### è°ƒè¯•æŠ€å·§

**1. å¯ç”¨è¯¦ç»†æ—¥å¿—**

```python
# config/.env
LOG_LEVEL=DEBUG

# æˆ–ä»£ç ä¸­è®¾ç½®
from src.utils.logger import logger
logger.setLevel("DEBUG")
```

**2. æŸ¥çœ‹æ£€ç´¢ç»“æœ**

```python
response = agent.query("Transformer æ˜¯ä»€ä¹ˆï¼Ÿ")

# æŸ¥çœ‹æ£€ç´¢åˆ°çš„æ–‡æ¡£
print("æ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼š")
for i, source in enumerate(response.source_nodes):
    print(f"\n--- æ–‡æ¡£ {i+1} ---")
    print(f"æ–‡ä»¶ï¼š{source.metadata['file_name']}")
    print(f"ç›¸ä¼¼åº¦ï¼š{source.score:.3f}")
    print(f"å†…å®¹ï¼š{source.text[:200]}...")
```

**3. æµ‹è¯•å•ä¸ªç»„ä»¶**

```python
# å•ç‹¬æµ‹è¯• Embedding
from config.llm_config import get_embedding_model

embed_model = get_embedding_model()
vector = embed_model.embed_query("æµ‹è¯•æ–‡æœ¬")
print(f"å‘é‡ç»´åº¦ï¼š{len(vector)}")

# å•ç‹¬æµ‹è¯• LLM
from config.llm_config import get_llm

llm = get_llm()
response = llm.complete("ä½ å¥½")
print(response.text)
```

---

## æœ€ä½³å®è·µ

### ä»£ç è§„èŒƒ

**1. ç±»å‹æ³¨è§£**

```python
from typing import List, Dict, Optional

def process_documents(
    documents: List[Document],
    max_length: int = 512
) -> List[Dict[str, Any]]:
    """å¤„ç†æ–‡æ¡£"""
    ...
```

**2. æ–‡æ¡£å­—ç¬¦ä¸²**

```python
def build_index(self, force_rebuild: bool = False) -> Dict[str, Any]:
    """æ„å»ºæ–‡æ¡£ç´¢å¼•
    
    Args:
        force_rebuild: æ˜¯å¦å¼ºåˆ¶é‡å»ºç´¢å¼•
        
    Returns:
        åŒ…å«æ„å»ºç»“æœçš„å­—å…¸ï¼Œæ ¼å¼ï¼š
        {
            "status": "success" | "error",
            "message": "æ„å»ºæˆåŠŸ" | "é”™è¯¯ä¿¡æ¯",
            "stats": {...}
        }
        
    Raises:
        RuntimeError: å½“æ–‡æ¡£ç›®å½•ä¸å­˜åœ¨æ—¶
    """
```

**3. é”™è¯¯å¤„ç†**

```python
try:
    result = self.build_index()
except FileNotFoundError as e:
    logger.error(f"æ–‡æ¡£ç›®å½•ä¸å­˜åœ¨ï¼š{e}")
    raise RuntimeError(f"è¯·ç¡®ä¿æ–‡æ¡£ç›®å½•å­˜åœ¨ï¼š{self.documents_dir}")
except Exception as e:
    logger.error(f"ç´¢å¼•æ„å»ºå¤±è´¥ï¼š{e}")
    raise
```

### æ€§èƒ½ä¼˜åŒ–

**1. æ‰¹é‡å¤„ç†**

```python
# æ‰¹é‡å‘é‡åŒ–
batch_size = 10
for i in range(0, len(texts), batch_size):
    batch = texts[i:i+batch_size]
    vectors = embed_model.embed_documents(batch)
```

**2. ç¼“å­˜ç»“æœ**

```python
from functools import lru_cache

@lru_cache(maxsize=100)
def embed_query(self, text: str) -> List[float]:
    """ç¼“å­˜å¸¸è§é—®é¢˜çš„å‘é‡"""
    return self._embed(text)
```

**3. å¼‚æ­¥å¤„ç†**

```python
import asyncio

async def process_documents_async(documents):
    tasks = [process_single(doc) for doc in documents]
    return await asyncio.gather(*tasks)
```

### å®‰å…¨å»ºè®®

**1. API Key ç®¡ç†**

```python
# âŒ ä¸è¦ç¡¬ç¼–ç 
api_key = "sk-1234567890abcdef"

# âœ… ä½¿ç”¨ç¯å¢ƒå˜é‡
import os
api_key = os.getenv("LLM_API_KEY")
```

**2. è¾“å…¥éªŒè¯**

```python
from pydantic import BaseModel, validator

class QueryRequest(BaseModel):
    question: str
    top_k: int = 5
    
    @validator("question")
    def validate_question(cls, v):
        if len(v) > 1000:
            raise ValueError("é—®é¢˜è¿‡é•¿")
        return v.strip()
```

---

## å¸¸è§å¼€å‘ä»»åŠ¡

### ä»»åŠ¡ 1ï¼šä¿®æ”¹ Prompt

```python
# config/prompts.py

# ä¿®æ”¹ç³»ç»Ÿ Prompt
SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡åˆ†æåŠ©æ‰‹ã€‚

ç‰¹ç‚¹ï¼š
- å›ç­”å‡†ç¡®ã€ä¸“ä¸š
- å¼•ç”¨æ¥æº
- æ‰¿è®¤ä¸çŸ¥é“

è¯·å§‹ç»ˆä¿æŒè¿™ä¸ªè§’è‰²ã€‚"""

# ä¿®æ”¹å¯¹è¯ Prompt
CHAT_PROMPT = """åŸºäºä»¥ä¸‹ä¿¡æ¯å›ç­”é—®é¢˜ï¼š

ä¸Šä¸‹æ–‡ï¼š{context}

å†å²å¯¹è¯ï¼š{history}

é—®é¢˜ï¼š{question}

å›ç­”ï¼š"""
```

### ä»»åŠ¡ 2ï¼šæ·»åŠ æ—¥å¿—

```python
from src.utils.logger import logger

logger.debug("è°ƒè¯•ä¿¡æ¯")
logger.info("æ™®é€šä¿¡æ¯")
logger.warning("è­¦å‘Šä¿¡æ¯")
logger.error("é”™è¯¯ä¿¡æ¯")

# å¸¦å˜é‡
logger.info(f"æ­£åœ¨å¤„ç†æ–‡æ¡£ï¼š{doc_name}")
```

### ä»»åŠ¡ 3ï¼šå¯¼å‡ºå¯¹è¯å†å²

```python
import json

def export_history(agent: AcademicAgent, output_file: str):
    """å¯¼å‡ºå¯¹è¯å†å²"""
    history = agent.get_chat_history()
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(history, f, ensure_ascii=False, indent=2)
    
    logger.info(f"å¯¹è¯å†å²å·²å¯¼å‡ºåˆ°ï¼š{output_file}")

# ä½¿ç”¨
export_history(agent, "history.json")
```

---

## å‚è€ƒèµ„æº

- **LlamaIndex å®˜æ–¹æ–‡æ¡£**ï¼šhttps://docs.llamaindex.ai/
- **Pydantic æ–‡æ¡£**ï¼šhttps://docs.pydantic.dev/
- **Gradio æ–‡æ¡£**ï¼šhttps://www.gradio.app/docs/
- **é¡¹ç›®æ¶æ„æ–‡æ¡£**ï¼š[ARCHITECTURE.md](ARCHITECTURE.md)

---

**æœ€åæ›´æ–°**: 2026-01-01
